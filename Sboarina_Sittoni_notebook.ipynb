{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pietrosittoni/Deep_Learning_Based_Gait_Classification/blob/main/Sboarina_Sittoni_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBes_Cto2S13"
      },
      "source": [
        "# Preamble\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-rweJF33Ay9"
      },
      "outputs": [],
      "source": [
        "#should add more strada facendo\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import keras\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8_UnD3G3Mi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304f7535-f153-4c12-bfc7-e49440745ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mkNMBBWiquZ"
      },
      "source": [
        "# Buildin the dataset and index\n",
        "\n",
        "We take 100 time step for each observation for the joint data:\n",
        "\n",
        "- from -110 to -10\n",
        "- We import also thepressure data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vBfr5JH28gX"
      },
      "outputs": [],
      "source": [
        "# Dirs is a list with several foilders: subject1,subject2,etc..\n",
        "path = \"/content/drive/MyDrive/Progetto_HDA/HDA_proj_A2\"\n",
        "subject = os.listdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4607CEliW9k"
      },
      "outputs": [],
      "source": [
        "response_malattie = {\"antalgic\":0,\"lurching\":1,\"normal\":2,\"steppage\":3,\"stiff-legged\":4,\"trendelenburg\":5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ciig3yPFahln"
      },
      "outputs": [],
      "source": [
        "mean_dist_before = np.zeros(1440)\n",
        "std_dist_before = np.zeros(1440)\n",
        "\n",
        "mean_dist_after = np.zeros(1440)\n",
        "std_dist_after = np.zeros(1440)\n",
        "\n",
        "# y-axis-we we will use only the y axis coordinate becasue the path is parallel to the y axis\n",
        "y_coor = [i for i in range(96) if i%3==1]\n",
        "window = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSmdRlyOzHjW"
      },
      "outputs": [],
      "source": [
        "subject = [\"subject\"+str(i) for i in range(1,13)]\n",
        "trial = [\"trial\"+str(i) for i in range(1,21)]\n",
        "control = True\n",
        "time_window = 100\n",
        "delay = 1\n",
        "up = -delay-time_window\n",
        "down = -delay\n",
        "obs = 0\n",
        "\n",
        "for i in subject:\n",
        "  pathsubject = path+\"/\"+i\n",
        "  for j in list(response_malattie.keys()):\n",
        "    pathmalattia = pathsubject+\"/\"+j\n",
        "    for l in trial:\n",
        "      pathtrial = pathmalattia+\"/\"+l\n",
        "      path_ = pathtrial+\"/pressure.csv\"\n",
        "      path__ = pathtrial+\"/skeleton.csv\"\n",
        "      if control:\n",
        "\n",
        "        control = False\n",
        "\n",
        "        full_kinect = pd.read_csv(path__,header=None).to_numpy()[:,1:-1]  # first colums composed by Nan\n",
        "        DataFramepressure = pd.read_csv(path_,header=None).to_numpy() # import our pressure\n",
        "\n",
        "        df_kinect = full_kinect[up:down] #primo giro\n",
        "\n",
        "        Y = np.array([response_malattie[j]]) #response\n",
        "\n",
        "        # mean distance traveld of the gracity center of the discard data and not\n",
        "        mean_dist_before[obs] = (abs(full_kinect[0,y_coor]-full_kinect[up,y_coor])+abs(full_kinect[down,y_coor]-full_kinect[-1,y_coor])).mean()\n",
        "\n",
        "\n",
        "        mean_dist_after[obs] = abs(full_kinect[up,y_coor]-full_kinect[down,y_coor]).mean()\n",
        "\n",
        "        window[obs] = full_kinect.shape[0]\n",
        "        obs+=1\n",
        "\n",
        "        del full_kinect\n",
        "\n",
        "      elif not control:\n",
        "        full_kinect = pd.read_csv(path__,header=None).to_numpy()[:,1:-1]\n",
        "        temp_press = pd.read_csv(path_,header=None).to_numpy()\n",
        "\n",
        "\n",
        "        DataFramepressure = np.concatenate([DataFramepressure,temp_press])\n",
        "\n",
        "        df_kinect = np.concatenate([df_kinect,full_kinect[up:down]])\n",
        "\n",
        "\n",
        "        Y = np.concatenate([Y,np.array([response_malattie[j]])])\n",
        "\n",
        "        # mean distance traveld of the gracity center of the discard data and not\n",
        "        mean_dist_before[obs] = (abs(full_kinect[0,y_coor]-full_kinect[up,y_coor])+abs(full_kinect[down,y_coor]-full_kinect[-1,y_coor])).mean()\n",
        "\n",
        "        mean_dist_after[obs] = abs(full_kinect[up,y_coor]-full_kinect[down,y_coor]).mean()\n",
        "\n",
        "        window[obs] = full_kinect.shape[0]\n",
        "        obs+=1\n",
        "\n",
        "\n",
        "        del full_kinect\n",
        "        del temp_press"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdxQm8fl30ZP"
      },
      "outputs": [],
      "source": [
        "# Save all the dataset in the drive folder\n",
        "\n",
        "savepressure = \"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/pressure.csv\"\n",
        "savekinect = \"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/kinect.csv\"\n",
        "\n",
        "\n",
        "pd.DataFrame(df_kinect).to_csv(savekinect)\n",
        "pd.DataFrame(DataFramepressure).to_csv(savepressure)\n",
        "np.savetxt(\"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/y.csv\",Y)\n",
        "\n",
        "np.savetxt(\"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/mean_dist_before.csv\",mean_dist_before)\n",
        "np.savetxt(\"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/mean_dist_after.csv\",mean_dist_after)\n",
        "pd.Series(window).to_csv(\"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/nomber_of_new_obs.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIJqHSxe4UwD"
      },
      "source": [
        "# Import prepared data and pre-select fetures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4E-6DPsku27"
      },
      "outputs": [],
      "source": [
        "columns = ['date',\n",
        "                           'pelvis_dim1','pelvis_dim2', 'pelvis_dim3',#0,1,2\n",
        "                           'spine_naval_dim1', 'spine_naval_dim2', 'spine_naval_dim3', #3,4,5\n",
        "                           'spine_chest_dim1', 'spine_chest_dim2','spine_chest_dim3',#6,7,8\n",
        "                           'neck_dim1', 'neck_dim2', 'neck_dim3',#9,10,11\n",
        "\n",
        "                           'clavicle_left_dim1', 'clavicle_left_dim2', 'clavicle_left_dim3',#12,13,14\n",
        "                           'shoulder_left_dim1', 'shoulder_left_dim2', 'shoulder_left_dim3',#15,16,17\n",
        "                           'elbow_left_dim1', 'elbow_left_dim2', 'elbow_left_dim3',#18,19,20\n",
        "                           'wrist_left_dim1', 'wrist_left_dim2', 'wrist_left_dim3',#21,22,23\n",
        "                           'hand_left_dim1', 'hand_left_dim2', 'hand_left_dim3',#24,25,26\n",
        "                           'handtip_left_dim1', 'handtip_left_dim2', 'handtip_left_dim3',#27,28,29\n",
        "                           'thumb_left_dim1', 'thumb_left_dim2', 'thumb_left_dim3',#30,31,32\n",
        "\n",
        "                           'clavicle_right_dim1', 'clavicle_right_dim2', 'clavicle_right_dim3',#33,34,35\n",
        "                           'shoulder_right_dim1', 'shoulder_right_dim2', 'shoulder_right_dim3',#36,37,38\n",
        "                           'elbow_right_dim1', 'elbow_right_dim2', 'elbow_right_dim3',#39,40,41\n",
        "                           'wrist_right_dim1', 'wrist_right_dim2', 'wrist_right_dim3',#42,43,44\n",
        "                           'hand_right_dim1', 'hand_right_dim2', 'hand_right_dim3',#45,46,47\n",
        "                           'handtip_right_dim1', 'handtip_right_dim2', 'handtip_right_dim3',#48,49,50\n",
        "                           'thumb_right_dim1', 'thumb_right_dim2', 'thumb_right_dim3',#51,52,53\n",
        "\n",
        "                           'hip_left_dim1', 'hip_left_dim2', 'hip_left_dim3',#54,55,56\n",
        "                           'knee_left_dim1', 'knee_left_dim2', 'knee_left_dim3',#57,58,59\n",
        "                           'ankle_left_dim1', 'ankle_left_dim2', 'ankle_left_dim3',#60,61,62\n",
        "                           'foot_left_dim1', 'foot_left_dim2', 'foot_left_dim3',#63,64,65\n",
        "\n",
        "                           'hip_right_dim1', 'hip_right_dim2', 'hip_right_dim3',#66,67,68\n",
        "                           'knee_right_dim1', 'knee_right_dim2', 'knee_right_dim3',#69,70,71\n",
        "                           'ankle_right_dim1', 'ankle_right_dim2', 'ankle_right_dim3',#72,73,74\n",
        "                           'foot_right_dim1', 'foot_right_dim2', 'foot_right_dim3',#75,76,77\n",
        "\n",
        "                           'head_dim1', 'head_dim2', 'head_dim3',#78,79,80\n",
        "                           'nose_dim1', 'nose_dim2', 'nose_dim3',#81,82,83\n",
        "\n",
        "                           'eye_left_dim1', 'eye_left_dim2', 'eye_left_dim3',#84,85,86\n",
        "                           'ear_left_dim1',  'ear_left_dim2',  'ear_left_dim3',#87,88,89\n",
        "\n",
        "                           'eye_right_dim1', 'eye_right_dim2', 'eye_right_dim3',#90,91,92\n",
        "                           'eye_right_dim1',  'eye_right_dim2',  'eye_right_dim3']#93,94,95\n",
        "\n",
        "left1 = [i for i in range(12,33)]\n",
        "left2 = [i for i in range(54,66)]\n",
        "left3 = [i for i in range(84,90)]\n",
        "\n",
        "mid = [i for i in range(12)]+[i for i in range(78,84)]\n",
        "\n",
        "right1 = [i for i in range(33,54)]\n",
        "right2 = [i for i in range(66,78)]\n",
        "right3 = [i for i in range(90,96)]\n",
        "\n",
        "\n",
        "joints = [0,1,2,3,4,5,6,7,8,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77]\n",
        "joints_leg = [54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77]\n",
        "joints_left_right = left1 + left2 + left3 + right1 + right2 + right3\n",
        "response_malattie = {\"antalgic\":1,\"lurching\":2,\"normal\":3,\"steppage\":4,\"stiff-legged\":5,\"trendelenburg\":6}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVx_dEZ34W45"
      },
      "outputs": [],
      "source": [
        "kinect = pd.read_csv(\"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/kinect.csv\").to_numpy()[:,1:]\n",
        "\n",
        "Y = pd.read_csv(\"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/y.csv\",header=None).to_numpy()\n",
        "Y = np.array([i.item() for i in Y])\n",
        "\n",
        "mean_dist_before = pd.read_csv(\"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/mean_dist_before.csv\",header=None).to_numpy()\n",
        "mean_dist_before = np.array([i.item() for i in mean_dist_before])\n",
        "mean_dist_after = pd.read_csv(\"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/mean_dist_after.csv\",header=None).to_numpy()\n",
        "mean_dist_after = np.array([i.item() for i in mean_dist_after])\n",
        "\n",
        "pressure = pd.read_csv(\"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/pressure.csv\").to_numpy()[:,1:].reshape((1440,128,48)) # bisogna aggiustare il codice buildn the dataset, farò più avanti\n",
        "\n",
        "obs = int(kinect.shape[0]/100)\n",
        "\n",
        "kinect = kinect.reshape((obs,100,96))\n",
        "\n",
        "windwos = dict(zip([i[0] for i in pd.read_csv(\"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/nomber_of_new_obs.csv\").values],[i[1] for i in pd.read_csv(\"/content/drive/MyDrive/Progetto_HDA/prepared_dataset2/nomber_of_new_obs.csv\").values]))\n",
        "\n",
        "Y = np.array([i.item() for i in Y])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pGngtdQh-Lq"
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "We swap the left and right in order to double the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfnvudy6Y5sA"
      },
      "outputs": [],
      "source": [
        "#defining left and right for all the dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDBoyzAMiKQR"
      },
      "outputs": [],
      "source": [
        "left1 = [i for i in range(12,33)]\n",
        "left2 = [i for i in range(54,66)]\n",
        "left3 = [i for i in range(84,90)]\n",
        "\n",
        "mid = [i for i in range(12)]+[i for i in range(78,84)]\n",
        "\n",
        "right1 = [i for i in range(33,54)]\n",
        "right2 = [i for i in range(66,78)]\n",
        "right3 = [i for i in range(90,96)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzUntTrHY8Mn"
      },
      "outputs": [],
      "source": [
        "#increasing size of arrays \"kinect_aument,\" \"pressure_aument,\" and \"Y_aument\" duplicating and reorganizing the values from the original arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3xx1qkZHRls"
      },
      "outputs": [],
      "source": [
        "kinect_aument = np.zeros((2*kinect.shape[0],100,96))\n",
        "pressure_aument = np.zeros((2*kinect.shape[0],128,48))\n",
        "\n",
        "Y_aument = np.zeros((2*kinect.shape[0]))\n",
        "\n",
        "for i in range(2*kinect.shape[0]):\n",
        "  if i%2==0:\n",
        "    kinect_aument[i] = kinect[i//2]\n",
        "    pressure_aument[i] = pressure[i//2]\n",
        "    Y_aument[i] = Y[i//2]\n",
        "\n",
        "  if i%2==1:\n",
        "    kinect_aument[i,:,left1],  kinect_aument[i,:,left2],  kinect_aument[i,:,left3],  kinect_aument[i,:,right1],  kinect_aument[i,:,right2],  kinect_aument[i,:,right3] = kinect[i//2,:,right1],  kinect[i//2,:,right2],  kinect[i//2,:,right3],  kinect[i//2,:,left1],  kinect[i//2,:,left2],  kinect[i//2,:,left3]\n",
        "    pressure_aument[i] = np.flip(pressure[i//2],axis=1)\n",
        "    Y_aument[i] = Y[i//2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JK-WKlC0-Hh"
      },
      "source": [
        "# EDA\n",
        "* Mean walk length\n",
        "* Time length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAka6no306sZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f1d105-7654-4adb-95c2-6ef8386dce1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================== Path length =====================================\n",
            "\n",
            "Min lenght: 118\n",
            "Min lenght: 509\n",
            "Mean lenght: 220.38888888888889\n",
            "======================================================================================\n",
            "\n",
            "\n",
            "======================== Distance covered (discard and keep)  ========================\n",
            "\n",
            "The mean distance of y coordinate discarded by the subjects is:\n",
            "\n",
            "How much distance is coverd: 0.45851986171875\n",
            "\n",
            "\n",
            "The mean distance of y coordinate stored by the subjects is:\n",
            "\n",
            "How much distance is coverd: 2.0926598292751737\n",
            "======================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"==================================== Path length =====================================\\n\")\n",
        "print(f\"Min lenght: {min(windwos.values())}\")\n",
        "print(f\"Min lenght: {max(windwos.values())}\")\n",
        "print(f\"Mean lenght: {sum(windwos.values())/len(windwos.values())}\")\n",
        "print(\"======================================================================================\")\n",
        "print(\"\\n\")\n",
        "\n",
        "n = len(windwos.values())\n",
        "\n",
        "print(\"======================== Distance covered (discard and keep)  ========================\\n\")\n",
        "print(\"The mean distance of y coordinate discarded by the subjects is:\\n\")\n",
        "\n",
        "print(f\"How much distance is coverd: {mean_dist_before.mean()}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"The mean distance of y coordinate stored by the subjects is:\\n\")\n",
        "\n",
        "print(f\"How much distance is coverd: {mean_dist_after.mean()}\")\n",
        "print(\"======================================================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX7NHlmd_FwG"
      },
      "source": [
        "# Train/val/test-split\n",
        "\n",
        "Subject Hold-out: randomly select 1 patient and half as test and 1 patient and half as validation.\n",
        "Standard scaler for the joints data, minmax scaler for the pressure data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSeRX0jcjdJ_"
      },
      "outputs": [],
      "source": [
        "index_patient = [[j for j in range(i*240,(i+1)*240)] for i in range(12)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkuVW_HF7Ec7",
        "outputId": "fd12e013-a3b3-43fb-cb6f-220d699096f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation patient: id 1, and even observation 3\n",
            "Test patient: id 10, and odd observation 3\n",
            "Train patients: id [ 5  9  7  6  2 11  0  8  4]\n"
          ]
        }
      ],
      "source": [
        "######## train/val/test-split\n",
        "\n",
        "np.random.seed(1635)\n",
        "id_patient = np.array([i for i in range(12)])\n",
        "np.random.shuffle(id_patient)\n",
        "print(f\"Validation patient: id {id_patient[0]}, and even observation {id_patient[1]}\")\n",
        "print(f\"Test patient: id {id_patient[2]}, and odd observation {id_patient[1]}\")\n",
        "print(f\"Train patients: id {id_patient[3:]}\")\n",
        "\n",
        "\n",
        "\n",
        "index_train = index_patient[id_patient[3]]+index_patient[id_patient[4]]+index_patient[id_patient[5]]+index_patient[id_patient[6]]+index_patient[id_patient[7]]+index_patient[id_patient[8]]+index_patient[id_patient[9]]+index_patient[id_patient[10]]+index_patient[id_patient[11]]\n",
        "index_test = index_patient[id_patient[0]]+list(np.array(index_patient[id_patient[1]])[[i for i in range(240) if i%2==0]])\n",
        "index_val = index_patient[id_patient[2]]+list(np.array(index_patient[id_patient[1]])[[i for i in range(240) if i%2==1]])\n",
        "\n",
        "\n",
        "X_train = kinect_aument[index_train]\n",
        "X_val = kinect_aument[index_val]\n",
        "X_test = kinect_aument[index_test]\n",
        "\n",
        "X_train_pres = pressure_aument[index_train]\n",
        "X_val_pres = pressure_aument[index_val]\n",
        "X_test_pres = pressure_aument[index_test]\n",
        "\n",
        "Y_train = Y_aument[index_train]\n",
        "Y_val = Y_aument[index_val]\n",
        "Y_test = Y_aument[index_test]\n",
        "\n",
        "\n",
        "Y_train = tf.one_hot(Y_train,depth=6)\n",
        "Y_val = tf.one_hot(Y_val,depth=6)\n",
        "Y_test = tf.one_hot(Y_test,depth=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls82bYkdZWyh"
      },
      "outputs": [],
      "source": [
        "# MinMaxScaler and Standard scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzuVe_QffcTW"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train_pres.reshape(X_train_pres.shape[0]*X_train_pres.shape[1],X_train_pres.shape[2]))\n",
        "\n",
        "X_train_pres = scaler.transform(X_train_pres.reshape(X_train_pres.shape[0]*X_train_pres.shape[1],X_train_pres.shape[2])).reshape(X_train_pres.shape[0],X_train_pres.shape[1],X_train_pres.shape[2])\n",
        "X_val_pres = scaler.transform(X_val_pres.reshape(X_val_pres.shape[0]*X_val_pres.shape[1],X_val_pres.shape[2])).reshape(X_val_pres.shape[0],X_val_pres.shape[1],X_val_pres.shape[2])\n",
        "X_test_pres = scaler.transform(X_test_pres.reshape(X_test_pres.shape[0]*X_test_pres.shape[1],X_test_pres.shape[2])).reshape(X_test_pres.shape[0],X_test_pres.shape[1],X_test_pres.shape[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucJzmMdOq0b5"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train.reshape(X_train.shape[0]*X_train.shape[1],X_train.shape[2]))\n",
        "\n",
        "X_train = scaler.transform(X_train.reshape(X_train.shape[0]*X_train.shape[1],X_train.shape[2])).reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2])\n",
        "X_val = scaler.transform(X_val.reshape(X_val.shape[0]*X_val.shape[1],X_val.shape[2])).reshape(X_val.shape[0],X_val.shape[1],X_val.shape[2])\n",
        "X_test = scaler.transform(X_test.reshape(X_test.shape[0]*X_test.shape[1],X_test.shape[2])).reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Tt9siXQJm9"
      },
      "outputs": [],
      "source": [
        "train_time = []\n",
        "acc_hist_val = []\n",
        "acc_hist_test = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epo2-z77JmE0"
      },
      "outputs": [],
      "source": [
        "# We save the test set!\n",
        "np.save(\"/content/drive/MyDrive/Progetto_HDA/test_set.npy\",X_test)\n",
        "np.save(\"/content/drive/MyDrive/Progetto_HDA/y_test_set.npy\",Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM67Mznil7va"
      },
      "source": [
        "# All fetures\n",
        "\n",
        "* LSTM\n",
        "* GRU\n",
        "* Hybrid pressuser+kinect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqghDYpol7va"
      },
      "source": [
        "## LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GnpwskBl7va"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(4)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1VPAu-oZdy2"
      },
      "outputs": [],
      "source": [
        "#building the LSTM RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7YF0cxel7va"
      },
      "outputs": [],
      "source": [
        "class classifier_lstm(Model):\n",
        "  def __init__(self):\n",
        "    super(classifier_lstm, self).__init__()\n",
        "    self.input_ = 96\n",
        "    self.rec =  tf.keras.Sequential([\n",
        "            layers.LSTM(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=self.input_, return_sequences=False,dropout=0.5),\n",
        "          ])\n",
        "\n",
        "    self.ffnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "  def call(self,x):\n",
        "    out = self.rec(x)\n",
        "    clas = self.ffnn(out)\n",
        "    return clas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K27-jf7nl7vb"
      },
      "outputs": [],
      "source": [
        "model_lstm_all = classifier_lstm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYvsRYRhrcNg",
        "outputId": "703708fa-c4ff-4842-d4c1-fbd18a7c0553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_64 (LSTM)              (None, None, 256)         361472    \n",
            "                                                                 \n",
            " lstm_65 (LSTM)              (None, None, 256)         525312    \n",
            "                                                                 \n",
            " lstm_66 (LSTM)              (None, None, 128)         197120    \n",
            "                                                                 \n",
            " lstm_67 (LSTM)              (None, 128)               131584    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,215,488\n",
            "Trainable params: 1,215,488\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_all.rec.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "358MXm70l7vb",
        "outputId": "20d35763-b570-40b6-fbaa-7d8fb2489f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_43\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_105 (Dense)           (2, 128)                  16384     \n",
            "                                                                 \n",
            " batch_normalization_84 (Bat  (2, 128)                 512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_84 (Activation)  (2, 128)                  0         \n",
            "                                                                 \n",
            " dense_106 (Dense)           (2, 64)                   8192      \n",
            "                                                                 \n",
            " batch_normalization_85 (Bat  (2, 64)                  256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_85 (Activation)  (2, 64)                   0         \n",
            "                                                                 \n",
            " dense_107 (Dense)           (2, 32)                   2048      \n",
            "                                                                 \n",
            " batch_normalization_86 (Bat  (2, 32)                  128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_86 (Activation)  (2, 32)                   0         \n",
            "                                                                 \n",
            " dense_108 (Dense)           (2, 16)                   512       \n",
            "                                                                 \n",
            " batch_normalization_87 (Bat  (2, 16)                  64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_87 (Activation)  (2, 16)                   0         \n",
            "                                                                 \n",
            " dense_109 (Dense)           (2, 6)                    102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,198\n",
            "Trainable params: 27,718\n",
            "Non-trainable params: 480\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_all(X_train[1:3,:,:])\n",
        "model_lstm_all.ffnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkHKzRzUZkgo"
      },
      "outputs": [],
      "source": [
        "#training the LSTM NN for all features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMV7CtrKl7vb",
        "outputId": "8932e77f-595d-45fa-d5fe-b2627bb91eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 9s 47ms/step - loss: 1.7602 - accuracy: 0.1903 - val_loss: 1.5156 - val_accuracy: 0.1667\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 1.5156 - accuracy: 0.2458 - val_loss: 1.5019 - val_accuracy: 0.1750\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.3562 - accuracy: 0.2995 - val_loss: 1.3870 - val_accuracy: 0.2389\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.2231 - accuracy: 0.3713 - val_loss: 1.2982 - val_accuracy: 0.2944\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.0961 - accuracy: 0.4495 - val_loss: 1.1138 - val_accuracy: 0.4806\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 1.0149 - accuracy: 0.5088 - val_loss: 0.9924 - val_accuracy: 0.5917\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.9319 - accuracy: 0.5880 - val_loss: 0.9457 - val_accuracy: 0.5778\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.8416 - accuracy: 0.6602 - val_loss: 0.8877 - val_accuracy: 0.6333\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.7868 - accuracy: 0.7019 - val_loss: 0.7865 - val_accuracy: 0.6694\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.7325 - accuracy: 0.7486 - val_loss: 0.7012 - val_accuracy: 0.7389\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.6650 - accuracy: 0.7796 - val_loss: 0.6954 - val_accuracy: 0.6806\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.6108 - accuracy: 0.8116 - val_loss: 0.6295 - val_accuracy: 0.7722\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5707 - accuracy: 0.8227 - val_loss: 0.5655 - val_accuracy: 0.7944\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.5305 - accuracy: 0.8644 - val_loss: 0.5597 - val_accuracy: 0.8139\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.4797 - accuracy: 0.8935 - val_loss: 0.5764 - val_accuracy: 0.8139\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.4487 - accuracy: 0.9097 - val_loss: 0.6159 - val_accuracy: 0.8000\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.4256 - accuracy: 0.9139 - val_loss: 0.5132 - val_accuracy: 0.8361\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3874 - accuracy: 0.9361 - val_loss: 0.4875 - val_accuracy: 0.8750\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.3653 - accuracy: 0.9421 - val_loss: 0.5284 - val_accuracy: 0.8333\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3524 - accuracy: 0.9444 - val_loss: 0.4387 - val_accuracy: 0.8611\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.3104 - accuracy: 0.9588 - val_loss: 0.4662 - val_accuracy: 0.8306\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2895 - accuracy: 0.9699 - val_loss: 0.4511 - val_accuracy: 0.8722\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2754 - accuracy: 0.9750 - val_loss: 0.6778 - val_accuracy: 0.7694\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2643 - accuracy: 0.9769 - val_loss: 0.5241 - val_accuracy: 0.8111\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2493 - accuracy: 0.9750 - val_loss: 0.4649 - val_accuracy: 0.8667\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2359 - accuracy: 0.9741 - val_loss: 0.4789 - val_accuracy: 0.8361\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2211 - accuracy: 0.9773 - val_loss: 0.4216 - val_accuracy: 0.8611\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2224 - accuracy: 0.9801 - val_loss: 0.6971 - val_accuracy: 0.7306\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2020 - accuracy: 0.9815 - val_loss: 0.3816 - val_accuracy: 0.8861\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1824 - accuracy: 0.9856 - val_loss: 0.3823 - val_accuracy: 0.8833\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1739 - accuracy: 0.9903 - val_loss: 0.3787 - val_accuracy: 0.8750\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1648 - accuracy: 0.9894 - val_loss: 0.4593 - val_accuracy: 0.8472\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1645 - accuracy: 0.9838 - val_loss: 0.3389 - val_accuracy: 0.8806\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1698 - accuracy: 0.9704 - val_loss: 0.6473 - val_accuracy: 0.7917\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1524 - accuracy: 0.9829 - val_loss: 0.5548 - val_accuracy: 0.8028\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1519 - accuracy: 0.9769 - val_loss: 0.5959 - val_accuracy: 0.8222\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1389 - accuracy: 0.9815 - val_loss: 0.4480 - val_accuracy: 0.8611\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1404 - accuracy: 0.9829 - val_loss: 0.4689 - val_accuracy: 0.8444\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1304 - accuracy: 0.9741 - val_loss: 0.5758 - val_accuracy: 0.8306\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1342 - accuracy: 0.9514 - val_loss: 0.7065 - val_accuracy: 0.7694\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1267 - accuracy: 0.9551 - val_loss: 0.5941 - val_accuracy: 0.8028\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1363 - accuracy: 0.9370 - val_loss: 0.5232 - val_accuracy: 0.8222\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1158 - accuracy: 0.9463 - val_loss: 0.4400 - val_accuracy: 0.8417\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1158 - accuracy: 0.9329 - val_loss: 0.4909 - val_accuracy: 0.8278\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1160 - accuracy: 0.9287 - val_loss: 0.5034 - val_accuracy: 0.8194\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1087 - accuracy: 0.9394 - val_loss: 0.5575 - val_accuracy: 0.8361\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1615 - accuracy: 0.9338 - val_loss: 0.4443 - val_accuracy: 0.8750\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1256 - accuracy: 0.9486 - val_loss: 0.5454 - val_accuracy: 0.7944\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1143 - accuracy: 0.9602 - val_loss: 0.5227 - val_accuracy: 0.8111\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1193 - accuracy: 0.9634 - val_loss: 0.5425 - val_accuracy: 0.8333\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1008 - accuracy: 0.9708 - val_loss: 0.4383 - val_accuracy: 0.8722\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0939 - accuracy: 0.9755 - val_loss: 0.4064 - val_accuracy: 0.8639\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0932 - accuracy: 0.9819 - val_loss: 0.5400 - val_accuracy: 0.8417\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0904 - accuracy: 0.9796 - val_loss: 0.4726 - val_accuracy: 0.8500\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0922 - accuracy: 0.9815 - val_loss: 0.4705 - val_accuracy: 0.8500\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1034 - accuracy: 0.9708 - val_loss: 0.6157 - val_accuracy: 0.8222\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1051 - accuracy: 0.9787 - val_loss: 0.5847 - val_accuracy: 0.8222\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1194 - accuracy: 0.9685 - val_loss: 0.4268 - val_accuracy: 0.8722\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0847 - accuracy: 0.9824 - val_loss: 0.3873 - val_accuracy: 0.8722\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0877 - accuracy: 0.9838 - val_loss: 0.5755 - val_accuracy: 0.8111\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0870 - accuracy: 0.9884 - val_loss: 0.4929 - val_accuracy: 0.8611\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0851 - accuracy: 0.9787 - val_loss: 0.7929 - val_accuracy: 0.7972\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0819 - accuracy: 0.9833 - val_loss: 0.5538 - val_accuracy: 0.8528\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0728 - accuracy: 0.9856 - val_loss: 0.6028 - val_accuracy: 0.8194\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.0670 - accuracy: 0.9884 - val_loss: 0.5689 - val_accuracy: 0.8194\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0694 - accuracy: 0.9778 - val_loss: 0.4994 - val_accuracy: 0.8528\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0868 - accuracy: 0.9727 - val_loss: 0.7530 - val_accuracy: 0.7611\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0728 - accuracy: 0.9773 - val_loss: 0.7105 - val_accuracy: 0.7972\n",
            "Epoch 69/200\n",
            "70/72 [============================>.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9643Restoring model weights from the end of the best epoch: 29.\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0808 - accuracy: 0.9653 - val_loss: 0.5501 - val_accuracy: 0.8500\n",
            "Epoch 69: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_lstm_all.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=40)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history_class_mod = model_lstm_all.fit(X_train[:,:,:], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_val[:,:,:], Y_val),callbacks=[es])\n",
        "\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB-7WolQeUu5"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH8r2ioowA6M",
        "outputId": "a4732c61-2d6e-4a07-fc67-ba470f1d29fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 2s 17ms/step\n",
            "12/12 [==============================] - 0s 14ms/step\n",
            "Accuracy validation: 0.8861111111111111\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_lstm_all.predict(X_test[:,:,:]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_lstm_all.predict(X_val[:,:,:]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA7r3Z60r1JV"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgX81-mobymQ"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(33)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMhooCahZ0CO"
      },
      "outputs": [],
      "source": [
        "#building the GRU RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7nrIMTErpSY"
      },
      "outputs": [],
      "source": [
        "class classifier_gru(Model):\n",
        "  def __init__(self):\n",
        "    super(classifier_gru, self).__init__()\n",
        "    self.input_ = 96\n",
        "    self.rec =  tf.keras.Sequential([\n",
        "            layers.GRU(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=self.input_, return_sequences=False,dropout=0.5),\n",
        "                           ])\n",
        "\n",
        "    self.ffnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "  def call(self,x):\n",
        "    out = self.rec(x)\n",
        "    clas = self.ffnn(out)\n",
        "    return clas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb4zzb3drpSZ"
      },
      "outputs": [],
      "source": [
        "model_gru_all = classifier_gru()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9hc74qsrpSZ",
        "outputId": "bf8ad3a7-c943-4929-8ce1-a8045014e974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_20 (GRU)                (None, None, 256)         271872    \n",
            "                                                                 \n",
            " gru_21 (GRU)                (None, None, 256)         394752    \n",
            "                                                                 \n",
            " gru_22 (GRU)                (None, None, 128)         148224    \n",
            "                                                                 \n",
            " gru_23 (GRU)                (None, 128)               99072     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 913,920\n",
            "Trainable params: 913,920\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_gru_all.rec.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MDWpJ0DrpSZ",
        "outputId": "f6258c7a-1954-47ad-d2ee-5725bdd3bcd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_45\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_110 (Dense)           (2, 128)                  16384     \n",
            "                                                                 \n",
            " batch_normalization_88 (Bat  (2, 128)                 512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_88 (Activation)  (2, 128)                  0         \n",
            "                                                                 \n",
            " dense_111 (Dense)           (2, 64)                   8192      \n",
            "                                                                 \n",
            " batch_normalization_89 (Bat  (2, 64)                  256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_89 (Activation)  (2, 64)                   0         \n",
            "                                                                 \n",
            " dense_112 (Dense)           (2, 32)                   2048      \n",
            "                                                                 \n",
            " batch_normalization_90 (Bat  (2, 32)                  128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_90 (Activation)  (2, 32)                   0         \n",
            "                                                                 \n",
            " dense_113 (Dense)           (2, 16)                   512       \n",
            "                                                                 \n",
            " batch_normalization_91 (Bat  (2, 16)                  64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_91 (Activation)  (2, 16)                   0         \n",
            "                                                                 \n",
            " dense_114 (Dense)           (2, 6)                    102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,198\n",
            "Trainable params: 27,718\n",
            "Non-trainable params: 480\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_gru_all(X_train[1:3,:,:])\n",
        "model_gru_all.ffnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZRBSqm-en3_"
      },
      "outputs": [],
      "source": [
        "#training the GRU RNN model for \"all features\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOmzeMq-IBSm",
        "outputId": "6955e0f0-520a-4e70-c238-c084c140be36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 9s 43ms/step - loss: 1.8916 - accuracy: 0.1718 - val_loss: 1.5017 - val_accuracy: 0.1667\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.8039 - accuracy: 0.1662 - val_loss: 1.4871 - val_accuracy: 0.2361\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.7142 - accuracy: 0.1745 - val_loss: 1.4787 - val_accuracy: 0.2583\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.6111 - accuracy: 0.2005 - val_loss: 1.4369 - val_accuracy: 0.2139\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.5451 - accuracy: 0.2287 - val_loss: 1.3178 - val_accuracy: 0.1833\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.4757 - accuracy: 0.2639 - val_loss: 1.3548 - val_accuracy: 0.2083\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.4095 - accuracy: 0.2833 - val_loss: 1.3643 - val_accuracy: 0.3111\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.3679 - accuracy: 0.3083 - val_loss: 1.2197 - val_accuracy: 0.3250\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.3129 - accuracy: 0.3454 - val_loss: 1.1362 - val_accuracy: 0.3556\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.2858 - accuracy: 0.3389 - val_loss: 1.0899 - val_accuracy: 0.3944\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.2451 - accuracy: 0.3759 - val_loss: 1.0227 - val_accuracy: 0.3722\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.2067 - accuracy: 0.4046 - val_loss: 1.0058 - val_accuracy: 0.4083\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.1741 - accuracy: 0.4301 - val_loss: 0.9900 - val_accuracy: 0.4028\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.1372 - accuracy: 0.4537 - val_loss: 1.0018 - val_accuracy: 0.4028\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.1002 - accuracy: 0.4764 - val_loss: 0.8718 - val_accuracy: 0.5528\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.0578 - accuracy: 0.5125 - val_loss: 0.8410 - val_accuracy: 0.5194\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.0031 - accuracy: 0.5375 - val_loss: 0.7757 - val_accuracy: 0.6056\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.9571 - accuracy: 0.5639 - val_loss: 0.7221 - val_accuracy: 0.6722\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.9031 - accuracy: 0.5986 - val_loss: 0.6819 - val_accuracy: 0.6889\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.8553 - accuracy: 0.6403 - val_loss: 0.6645 - val_accuracy: 0.6889\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.8033 - accuracy: 0.6694 - val_loss: 0.5953 - val_accuracy: 0.7528\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.7475 - accuracy: 0.6782 - val_loss: 0.5094 - val_accuracy: 0.7611\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.7079 - accuracy: 0.6972 - val_loss: 0.4743 - val_accuracy: 0.8000\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.6712 - accuracy: 0.7176 - val_loss: 0.4947 - val_accuracy: 0.8111\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.6313 - accuracy: 0.7347 - val_loss: 0.4503 - val_accuracy: 0.8028\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.5804 - accuracy: 0.7583 - val_loss: 0.4336 - val_accuracy: 0.7583\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.5542 - accuracy: 0.7685 - val_loss: 0.4385 - val_accuracy: 0.7722\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.5168 - accuracy: 0.7796 - val_loss: 0.4488 - val_accuracy: 0.7583\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.4857 - accuracy: 0.7921 - val_loss: 0.3893 - val_accuracy: 0.7639\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.4523 - accuracy: 0.8014 - val_loss: 0.4314 - val_accuracy: 0.7417\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.4230 - accuracy: 0.8162 - val_loss: 0.3742 - val_accuracy: 0.7917\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.3823 - accuracy: 0.8213 - val_loss: 0.3500 - val_accuracy: 0.7917\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.3641 - accuracy: 0.8301 - val_loss: 0.3921 - val_accuracy: 0.7694\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.3457 - accuracy: 0.8389 - val_loss: 0.3125 - val_accuracy: 0.8056\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.3375 - accuracy: 0.8384 - val_loss: 0.3682 - val_accuracy: 0.7750\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.3096 - accuracy: 0.8546 - val_loss: 0.3500 - val_accuracy: 0.7861\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2931 - accuracy: 0.8528 - val_loss: 0.4116 - val_accuracy: 0.7556\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2694 - accuracy: 0.8653 - val_loss: 0.3543 - val_accuracy: 0.7972\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2689 - accuracy: 0.8736 - val_loss: 0.2899 - val_accuracy: 0.8222\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2437 - accuracy: 0.8921 - val_loss: 0.3539 - val_accuracy: 0.8000\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2447 - accuracy: 0.8926 - val_loss: 0.3600 - val_accuracy: 0.8083\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2443 - accuracy: 0.9093 - val_loss: 0.3400 - val_accuracy: 0.8306\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2309 - accuracy: 0.9060 - val_loss: 0.4092 - val_accuracy: 0.8194\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2007 - accuracy: 0.9338 - val_loss: 0.3884 - val_accuracy: 0.8056\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2030 - accuracy: 0.9296 - val_loss: 0.3896 - val_accuracy: 0.8083\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1913 - accuracy: 0.9306 - val_loss: 0.3408 - val_accuracy: 0.8333\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1783 - accuracy: 0.9389 - val_loss: 0.3256 - val_accuracy: 0.8361\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1703 - accuracy: 0.9491 - val_loss: 0.2036 - val_accuracy: 0.8833\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1634 - accuracy: 0.9472 - val_loss: 0.2110 - val_accuracy: 0.8917\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1524 - accuracy: 0.9532 - val_loss: 0.2600 - val_accuracy: 0.8528\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1517 - accuracy: 0.9403 - val_loss: 0.4248 - val_accuracy: 0.7861\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1516 - accuracy: 0.9319 - val_loss: 0.3063 - val_accuracy: 0.8306\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1451 - accuracy: 0.9079 - val_loss: 0.2539 - val_accuracy: 0.8222\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1432 - accuracy: 0.8968 - val_loss: 0.3874 - val_accuracy: 0.7556\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1399 - accuracy: 0.8778 - val_loss: 0.2814 - val_accuracy: 0.7861\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1265 - accuracy: 0.8681 - val_loss: 0.2252 - val_accuracy: 0.8083\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1373 - accuracy: 0.8514 - val_loss: 0.2995 - val_accuracy: 0.7500\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1284 - accuracy: 0.8440 - val_loss: 0.2074 - val_accuracy: 0.7861\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1313 - accuracy: 0.8338 - val_loss: 0.4111 - val_accuracy: 0.7111\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1235 - accuracy: 0.8329 - val_loss: 0.4038 - val_accuracy: 0.7056\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1204 - accuracy: 0.8292 - val_loss: 0.3317 - val_accuracy: 0.7194\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1093 - accuracy: 0.8292 - val_loss: 0.1768 - val_accuracy: 0.7861\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1118 - accuracy: 0.8296 - val_loss: 0.1794 - val_accuracy: 0.7889\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1025 - accuracy: 0.8315 - val_loss: 0.2284 - val_accuracy: 0.7639\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1084 - accuracy: 0.8310 - val_loss: 0.2541 - val_accuracy: 0.7611\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1074 - accuracy: 0.8333 - val_loss: 0.2866 - val_accuracy: 0.7528\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1094 - accuracy: 0.8306 - val_loss: 0.1952 - val_accuracy: 0.7889\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1118 - accuracy: 0.8278 - val_loss: 0.1756 - val_accuracy: 0.7889\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1126 - accuracy: 0.8250 - val_loss: 0.2015 - val_accuracy: 0.7750\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0976 - accuracy: 0.8292 - val_loss: 0.1718 - val_accuracy: 0.7861\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0964 - accuracy: 0.8310 - val_loss: 0.2061 - val_accuracy: 0.7722\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0932 - accuracy: 0.8282 - val_loss: 0.2782 - val_accuracy: 0.7444\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1078 - accuracy: 0.8227 - val_loss: 0.2779 - val_accuracy: 0.7306\n",
            "Epoch 74/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0894 - accuracy: 0.8264 - val_loss: 0.2122 - val_accuracy: 0.7722\n",
            "Epoch 75/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0937 - accuracy: 0.8250 - val_loss: 0.2031 - val_accuracy: 0.7750\n",
            "Epoch 76/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0922 - accuracy: 0.8273 - val_loss: 0.4747 - val_accuracy: 0.6694\n",
            "Epoch 77/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0946 - accuracy: 0.8250 - val_loss: 0.2945 - val_accuracy: 0.7389\n",
            "Epoch 78/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1022 - accuracy: 0.8204 - val_loss: 0.2178 - val_accuracy: 0.7639\n",
            "Epoch 79/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0918 - accuracy: 0.8259 - val_loss: 0.2386 - val_accuracy: 0.7556\n",
            "Epoch 80/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0828 - accuracy: 0.8269 - val_loss: 0.1899 - val_accuracy: 0.7722\n",
            "Epoch 81/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0885 - accuracy: 0.8250 - val_loss: 0.3207 - val_accuracy: 0.7250\n",
            "Epoch 82/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0777 - accuracy: 0.8278 - val_loss: 0.2439 - val_accuracy: 0.7500\n",
            "Epoch 83/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0816 - accuracy: 0.8236 - val_loss: 0.1838 - val_accuracy: 0.7778\n",
            "Epoch 84/200\n",
            "70/72 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.8267Restoring model weights from the end of the best epoch: 49.\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0783 - accuracy: 0.8264 - val_loss: 0.3024 - val_accuracy: 0.7250\n",
            "Epoch 84: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_gru_all.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "history_class_mod = model_gru_all.fit(X_train[:,:,:], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_val[:,:,:], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j_uywbffhTe"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trcqz4N8IBSn",
        "outputId": "30de5742-9660-48e2-ac2e-e8fbd01a860e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 1s 15ms/step\n",
            "12/12 [==============================] - 0s 12ms/step\n",
            "Accuracy validation: 0.8916666666666667\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_gru_all.predict(X_test[:,:,:]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_gru_all.predict(X_val[:,:,:]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoFD4BH2l7vb"
      },
      "source": [
        "## Hybrid-LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncD0dhyWa4Ft"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxMOkm47f2Ck"
      },
      "outputs": [],
      "source": [
        "#building the Hybrid-LSTM CNN RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYudBaJql7vb"
      },
      "outputs": [],
      "source": [
        "class fuse(Model):\n",
        "  def __init__(self):\n",
        "    super(fuse, self).__init__()\n",
        "    self.cnn = tf.keras.Sequential([\n",
        "      layers.Input(shape=(128, 48,1)),\n",
        "      layers.Conv2D(128, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,1)),\n",
        "      layers.Conv2D(64, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Conv2D(32, (3, 3), activation=None, padding='same', strides=1),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Flatten()])\n",
        "\n",
        "\n",
        "\n",
        "    self.rnn =  tf.keras.Sequential([\n",
        "            layers.LSTM(256,input_dim=96, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(256,input_dim=96, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=96, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=96, return_sequences=False,dropout=0.5),\n",
        "    ])\n",
        "\n",
        "    self.calssifier = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "\n",
        "    self.fc_cnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "    self.fc_rnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "\n",
        "  def call(self,x):\n",
        "    out0 = self.cnn(x[0])\n",
        "    out0 = self.fc_cnn(out0)\n",
        "    out1 = self.rnn(x[1])\n",
        "    out1 = self.fc_rnn(out1)\n",
        "\n",
        "    out = self.calssifier(layers.concatenate([out0,out1]))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRcXwsdml7vb"
      },
      "outputs": [],
      "source": [
        "model_hy_lstm_all=fuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2UyKQlRl7vb",
        "outputId": "c99e8fe9-8f67-4937-d026-6dca7e9c6056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_46\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 64, 24, 128)       1280      \n",
            "                                                                 \n",
            " activation_92 (Activation)  (None, 64, 24, 128)       0         \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 32, 24, 128)      0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 12, 64)        73792     \n",
            "                                                                 \n",
            " activation_93 (Activation)  (None, 16, 12, 64)        0         \n",
            "                                                                 \n",
            " average_pooling2d_1 (Averag  (None, 8, 6, 64)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 6, 32)          18464     \n",
            "                                                                 \n",
            " activation_94 (Activation)  (None, 8, 6, 32)          0         \n",
            "                                                                 \n",
            " average_pooling2d_2 (Averag  (None, 4, 3, 32)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 384)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,536\n",
            "Trainable params: 93,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_hy_lstm_all.cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gkX5jG9f_hM"
      },
      "outputs": [],
      "source": [
        "#training the Hybrid-LSTM CNN RNN model for \"all features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np-6VKCfIdch",
        "outputId": "6bda853f-0325-4e8c-e226-c9f34d35de7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 16s 67ms/step - loss: 1.6863 - accuracy: 0.1833 - val_loss: 1.5475 - val_accuracy: 0.1556\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 1.4830 - accuracy: 0.2694 - val_loss: 1.6158 - val_accuracy: 0.1667\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 1.3008 - accuracy: 0.3690 - val_loss: 1.6383 - val_accuracy: 0.1778\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 1.1847 - accuracy: 0.4477 - val_loss: 1.5714 - val_accuracy: 0.3250\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.0761 - accuracy: 0.5042 - val_loss: 1.4136 - val_accuracy: 0.3056\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.9938 - accuracy: 0.5727 - val_loss: 1.2035 - val_accuracy: 0.4944\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.9200 - accuracy: 0.6074 - val_loss: 1.1135 - val_accuracy: 0.4778\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.8420 - accuracy: 0.6407 - val_loss: 0.9945 - val_accuracy: 0.5139\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.7955 - accuracy: 0.6523 - val_loss: 0.9636 - val_accuracy: 0.5278\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.7253 - accuracy: 0.6801 - val_loss: 0.9224 - val_accuracy: 0.5444\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.6906 - accuracy: 0.6940 - val_loss: 0.8423 - val_accuracy: 0.5778\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.6429 - accuracy: 0.6995 - val_loss: 0.6863 - val_accuracy: 0.6583\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.5872 - accuracy: 0.7301 - val_loss: 0.8071 - val_accuracy: 0.5694\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.5562 - accuracy: 0.7375 - val_loss: 0.6121 - val_accuracy: 0.6972\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.5125 - accuracy: 0.7694 - val_loss: 0.6281 - val_accuracy: 0.6806\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4886 - accuracy: 0.7694 - val_loss: 0.5032 - val_accuracy: 0.7250\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4476 - accuracy: 0.7838 - val_loss: 0.5108 - val_accuracy: 0.7222\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4295 - accuracy: 0.7870 - val_loss: 0.6141 - val_accuracy: 0.6556\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4196 - accuracy: 0.7912 - val_loss: 0.4091 - val_accuracy: 0.7750\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3919 - accuracy: 0.7981 - val_loss: 0.5375 - val_accuracy: 0.7000\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3732 - accuracy: 0.8028 - val_loss: 0.5395 - val_accuracy: 0.7278\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3605 - accuracy: 0.8046 - val_loss: 0.4712 - val_accuracy: 0.7389\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3407 - accuracy: 0.8148 - val_loss: 0.4798 - val_accuracy: 0.7528\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3255 - accuracy: 0.8139 - val_loss: 0.4205 - val_accuracy: 0.7361\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3343 - accuracy: 0.8093 - val_loss: 0.4883 - val_accuracy: 0.7167\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3202 - accuracy: 0.8083 - val_loss: 0.5424 - val_accuracy: 0.7111\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3032 - accuracy: 0.8148 - val_loss: 0.4322 - val_accuracy: 0.7417\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2874 - accuracy: 0.8204 - val_loss: 0.4424 - val_accuracy: 0.7417\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2854 - accuracy: 0.8171 - val_loss: 0.4605 - val_accuracy: 0.7333\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2633 - accuracy: 0.8227 - val_loss: 0.4956 - val_accuracy: 0.7222\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2720 - accuracy: 0.8167 - val_loss: 0.3726 - val_accuracy: 0.7667\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2604 - accuracy: 0.8190 - val_loss: 0.4234 - val_accuracy: 0.7583\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2443 - accuracy: 0.8231 - val_loss: 0.6894 - val_accuracy: 0.6667\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2491 - accuracy: 0.8199 - val_loss: 0.3390 - val_accuracy: 0.7583\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2356 - accuracy: 0.8245 - val_loss: 0.4404 - val_accuracy: 0.7417\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2305 - accuracy: 0.8236 - val_loss: 0.5058 - val_accuracy: 0.7139\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2449 - accuracy: 0.8139 - val_loss: 0.5745 - val_accuracy: 0.7028\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2285 - accuracy: 0.8199 - val_loss: 0.6258 - val_accuracy: 0.6833\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2368 - accuracy: 0.8167 - val_loss: 0.5017 - val_accuracy: 0.7250\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2148 - accuracy: 0.8241 - val_loss: 0.5369 - val_accuracy: 0.7056\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2071 - accuracy: 0.8231 - val_loss: 0.3821 - val_accuracy: 0.7639\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1945 - accuracy: 0.8273 - val_loss: 0.5038 - val_accuracy: 0.7056\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1849 - accuracy: 0.8296 - val_loss: 0.4525 - val_accuracy: 0.7222\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1959 - accuracy: 0.8250 - val_loss: 0.5677 - val_accuracy: 0.6667\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.2022 - accuracy: 0.8218 - val_loss: 0.4891 - val_accuracy: 0.7194\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1815 - accuracy: 0.8278 - val_loss: 0.4667 - val_accuracy: 0.7333\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1753 - accuracy: 0.8292 - val_loss: 0.3885 - val_accuracy: 0.7444\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1730 - accuracy: 0.8301 - val_loss: 0.3660 - val_accuracy: 0.7361\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1693 - accuracy: 0.8296 - val_loss: 0.3942 - val_accuracy: 0.7444\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1810 - accuracy: 0.8250 - val_loss: 0.4291 - val_accuracy: 0.7167\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.1774 - accuracy: 0.8292 - val_loss: 0.5722 - val_accuracy: 0.7000\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1772 - accuracy: 0.8296 - val_loss: 0.5415 - val_accuracy: 0.6861\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2100 - accuracy: 0.8120 - val_loss: 0.6532 - val_accuracy: 0.6917\n",
            "Epoch 54/200\n",
            "71/72 [============================>.] - ETA: 0s - loss: 0.1970 - accuracy: 0.8216Restoring model weights from the end of the best epoch: 19.\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1970 - accuracy: 0.8227 - val_loss: 0.5298 - val_accuracy: 0.7056\n",
            "Epoch 54: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_hy_lstm_all.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history_class_mod = model_hy_lstm_all.fit([X_train_pres,X_train], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=([X_val_pres,X_val], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QehYfhV3gGwX"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYjzF6y-Idcv",
        "outputId": "1ea676b7-997b-4b3d-d809-2cda66da0f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 2s 20ms/step\n",
            "12/12 [==============================] - 0s 10ms/step\n",
            "Accuracy validation: 0.775\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_hy_lstm_all.predict([X_test_pres,X_test]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_hy_lstm_all.predict([X_val_pres,X_val]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4FHiYVaH5sO"
      },
      "source": [
        "## Hybrid-GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2FkcQUbH5sb"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OupDrcbgRVm"
      },
      "outputs": [],
      "source": [
        "#building the Hybrid-GRU CNN RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3DCkvxRH5sc"
      },
      "outputs": [],
      "source": [
        "class fuse(Model):\n",
        "  def __init__(self):\n",
        "    super(fuse, self).__init__()\n",
        "    self.cnn = tf.keras.Sequential([\n",
        "      layers.Input(shape=(128, 48,1)),\n",
        "      layers.Conv2D(128, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,1)),\n",
        "      layers.Conv2D(64, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Conv2D(32, (3, 3), activation=None, padding='same', strides=1),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Flatten()])\n",
        "\n",
        "\n",
        "\n",
        "    self.rnn =  tf.keras.Sequential([\n",
        "            layers.GRU(256,input_dim=96, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(256,input_dim=96, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=96, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=96, return_sequences=False,dropout=0.5),\n",
        "    ])\n",
        "\n",
        "    self.calssifier = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "\n",
        "    self.fc_cnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "    self.fc_rnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "\n",
        "  def call(self,x):\n",
        "    out0 = self.cnn(x[0])\n",
        "    out0 = self.fc_cnn(out0)\n",
        "    out1 = self.rnn(x[1])\n",
        "    out1 = self.fc_rnn(out1)\n",
        "\n",
        "    out = self.calssifier(layers.concatenate([out0,out1]))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz1WtITfH5sc"
      },
      "outputs": [],
      "source": [
        "model_hy_gru_all=fuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzUpVBngH5sc",
        "outputId": "84323f7d-cc14-4dde-8579-cd330bec4b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 64, 24, 128)       1280      \n",
            "                                                                 \n",
            " activation_103 (Activation)  (None, 64, 24, 128)      0         \n",
            "                                                                 \n",
            " average_pooling2d_3 (Averag  (None, 32, 24, 128)      0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 12, 64)        73792     \n",
            "                                                                 \n",
            " activation_104 (Activation)  (None, 16, 12, 64)       0         \n",
            "                                                                 \n",
            " average_pooling2d_4 (Averag  (None, 8, 6, 64)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 6, 32)          18464     \n",
            "                                                                 \n",
            " activation_105 (Activation)  (None, 8, 6, 32)         0         \n",
            "                                                                 \n",
            " average_pooling2d_5 (Averag  (None, 4, 3, 32)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 384)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,536\n",
            "Trainable params: 93,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_hy_gru_all.cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSicD6EmgZYj"
      },
      "outputs": [],
      "source": [
        "#Training the Hybrid-GRU CNN RNN model on \"all features\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JWXEY0NH5sc",
        "outputId": "8163c657-2de5-4837-c336-a28e8b587da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 12s 53ms/step - loss: 1.7060 - accuracy: 0.1778 - val_loss: 1.5340 - val_accuracy: 0.1361\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.5880 - accuracy: 0.2194 - val_loss: 1.5517 - val_accuracy: 0.1639\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.4908 - accuracy: 0.2528 - val_loss: 1.5113 - val_accuracy: 0.1833\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.4060 - accuracy: 0.2931 - val_loss: 1.4851 - val_accuracy: 0.1861\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.3225 - accuracy: 0.3602 - val_loss: 1.4522 - val_accuracy: 0.1944\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.2465 - accuracy: 0.4065 - val_loss: 1.4657 - val_accuracy: 0.1833\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 1.1733 - accuracy: 0.4491 - val_loss: 1.3277 - val_accuracy: 0.3472\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.1154 - accuracy: 0.4824 - val_loss: 1.2192 - val_accuracy: 0.3778\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.0393 - accuracy: 0.5301 - val_loss: 1.1372 - val_accuracy: 0.4500\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.9822 - accuracy: 0.5667 - val_loss: 1.0678 - val_accuracy: 0.5083\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.9240 - accuracy: 0.5921 - val_loss: 0.9543 - val_accuracy: 0.5278\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.8887 - accuracy: 0.5977 - val_loss: 0.9331 - val_accuracy: 0.5472\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.8375 - accuracy: 0.6181 - val_loss: 0.9032 - val_accuracy: 0.5528\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.7944 - accuracy: 0.6306 - val_loss: 0.8661 - val_accuracy: 0.5806\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.7534 - accuracy: 0.6477 - val_loss: 0.8868 - val_accuracy: 0.5639\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.7155 - accuracy: 0.6565 - val_loss: 0.8057 - val_accuracy: 0.5889\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6780 - accuracy: 0.6690 - val_loss: 0.8324 - val_accuracy: 0.5861\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6696 - accuracy: 0.6704 - val_loss: 0.7645 - val_accuracy: 0.6278\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6353 - accuracy: 0.6870 - val_loss: 0.7624 - val_accuracy: 0.5944\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.5965 - accuracy: 0.7102 - val_loss: 0.7042 - val_accuracy: 0.6000\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.5684 - accuracy: 0.7148 - val_loss: 0.7297 - val_accuracy: 0.6194\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5598 - accuracy: 0.7194 - val_loss: 0.6597 - val_accuracy: 0.6194\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5340 - accuracy: 0.7194 - val_loss: 0.6706 - val_accuracy: 0.6250\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5157 - accuracy: 0.7278 - val_loss: 0.6851 - val_accuracy: 0.6111\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5129 - accuracy: 0.7255 - val_loss: 0.6639 - val_accuracy: 0.6250\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4871 - accuracy: 0.7403 - val_loss: 0.6364 - val_accuracy: 0.6333\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4683 - accuracy: 0.7380 - val_loss: 0.6283 - val_accuracy: 0.6639\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4578 - accuracy: 0.7472 - val_loss: 0.6160 - val_accuracy: 0.6611\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4425 - accuracy: 0.7477 - val_loss: 0.5780 - val_accuracy: 0.6833\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4224 - accuracy: 0.7616 - val_loss: 0.6287 - val_accuracy: 0.6417\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4195 - accuracy: 0.7551 - val_loss: 0.6362 - val_accuracy: 0.6278\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.4212 - accuracy: 0.7519 - val_loss: 0.5989 - val_accuracy: 0.6333\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3910 - accuracy: 0.7662 - val_loss: 0.6402 - val_accuracy: 0.6500\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3902 - accuracy: 0.7667 - val_loss: 0.6209 - val_accuracy: 0.6361\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3871 - accuracy: 0.7676 - val_loss: 0.6037 - val_accuracy: 0.6500\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3807 - accuracy: 0.7699 - val_loss: 0.6264 - val_accuracy: 0.6250\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3771 - accuracy: 0.7690 - val_loss: 0.6295 - val_accuracy: 0.6167\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3686 - accuracy: 0.7764 - val_loss: 0.6146 - val_accuracy: 0.6139\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3538 - accuracy: 0.7810 - val_loss: 0.5980 - val_accuracy: 0.6417\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3616 - accuracy: 0.7736 - val_loss: 0.5516 - val_accuracy: 0.6833\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3515 - accuracy: 0.7838 - val_loss: 0.5661 - val_accuracy: 0.6611\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3503 - accuracy: 0.7843 - val_loss: 0.5684 - val_accuracy: 0.6556\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3379 - accuracy: 0.7875 - val_loss: 0.5839 - val_accuracy: 0.6500\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3401 - accuracy: 0.7880 - val_loss: 0.5810 - val_accuracy: 0.6500\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3255 - accuracy: 0.7968 - val_loss: 0.6593 - val_accuracy: 0.6167\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3147 - accuracy: 0.7963 - val_loss: 0.5456 - val_accuracy: 0.6917\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2998 - accuracy: 0.8042 - val_loss: 0.5271 - val_accuracy: 0.6917\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3119 - accuracy: 0.7926 - val_loss: 0.5245 - val_accuracy: 0.6806\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2977 - accuracy: 0.8037 - val_loss: 0.5803 - val_accuracy: 0.6472\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2900 - accuracy: 0.8069 - val_loss: 0.5651 - val_accuracy: 0.6889\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2845 - accuracy: 0.8093 - val_loss: 0.5498 - val_accuracy: 0.6639\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2672 - accuracy: 0.8148 - val_loss: 0.5351 - val_accuracy: 0.6861\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2720 - accuracy: 0.8083 - val_loss: 0.5483 - val_accuracy: 0.6750\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2818 - accuracy: 0.8051 - val_loss: 0.4783 - val_accuracy: 0.7139\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2689 - accuracy: 0.8116 - val_loss: 0.4946 - val_accuracy: 0.7250\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2543 - accuracy: 0.8153 - val_loss: 0.5188 - val_accuracy: 0.6722\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2571 - accuracy: 0.8130 - val_loss: 0.5147 - val_accuracy: 0.6806\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2456 - accuracy: 0.8106 - val_loss: 0.4998 - val_accuracy: 0.6944\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2494 - accuracy: 0.8171 - val_loss: 0.5230 - val_accuracy: 0.6750\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2408 - accuracy: 0.8167 - val_loss: 0.5152 - val_accuracy: 0.6722\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2381 - accuracy: 0.8162 - val_loss: 0.4265 - val_accuracy: 0.7000\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2325 - accuracy: 0.8185 - val_loss: 0.4377 - val_accuracy: 0.7139\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2384 - accuracy: 0.8171 - val_loss: 0.4485 - val_accuracy: 0.6889\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2287 - accuracy: 0.8171 - val_loss: 0.4700 - val_accuracy: 0.6889\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2179 - accuracy: 0.8194 - val_loss: 0.4486 - val_accuracy: 0.7333\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2201 - accuracy: 0.8185 - val_loss: 0.5134 - val_accuracy: 0.6889\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2122 - accuracy: 0.8185 - val_loss: 0.4720 - val_accuracy: 0.6667\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2223 - accuracy: 0.8144 - val_loss: 0.4777 - val_accuracy: 0.7250\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2112 - accuracy: 0.8208 - val_loss: 0.3997 - val_accuracy: 0.7361\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2147 - accuracy: 0.8162 - val_loss: 0.4343 - val_accuracy: 0.7361\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2098 - accuracy: 0.8176 - val_loss: 0.4598 - val_accuracy: 0.6639\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2029 - accuracy: 0.8222 - val_loss: 0.5011 - val_accuracy: 0.6806\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2009 - accuracy: 0.8222 - val_loss: 0.4065 - val_accuracy: 0.7194\n",
            "Epoch 74/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1957 - accuracy: 0.8269 - val_loss: 0.5171 - val_accuracy: 0.6556\n",
            "Epoch 75/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1946 - accuracy: 0.8208 - val_loss: 0.3706 - val_accuracy: 0.7417\n",
            "Epoch 76/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.1936 - accuracy: 0.8245 - val_loss: 0.3711 - val_accuracy: 0.7333\n",
            "Epoch 77/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1863 - accuracy: 0.8250 - val_loss: 0.4666 - val_accuracy: 0.6861\n",
            "Epoch 78/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1914 - accuracy: 0.8236 - val_loss: 0.4634 - val_accuracy: 0.7111\n",
            "Epoch 79/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1823 - accuracy: 0.8269 - val_loss: 0.4818 - val_accuracy: 0.6861\n",
            "Epoch 80/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1847 - accuracy: 0.8241 - val_loss: 0.3942 - val_accuracy: 0.7306\n",
            "Epoch 81/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1845 - accuracy: 0.8218 - val_loss: 0.3762 - val_accuracy: 0.7444\n",
            "Epoch 82/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1750 - accuracy: 0.8269 - val_loss: 0.3896 - val_accuracy: 0.7361\n",
            "Epoch 83/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.1759 - accuracy: 0.8287 - val_loss: 0.4140 - val_accuracy: 0.7194\n",
            "Epoch 84/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1809 - accuracy: 0.8245 - val_loss: 0.4219 - val_accuracy: 0.7111\n",
            "Epoch 85/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1723 - accuracy: 0.8259 - val_loss: 0.4181 - val_accuracy: 0.7222\n",
            "Epoch 86/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1822 - accuracy: 0.8255 - val_loss: 0.3910 - val_accuracy: 0.7389\n",
            "Epoch 87/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1841 - accuracy: 0.8250 - val_loss: 0.3714 - val_accuracy: 0.7472\n",
            "Epoch 88/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1819 - accuracy: 0.8255 - val_loss: 0.4959 - val_accuracy: 0.6806\n",
            "Epoch 89/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1747 - accuracy: 0.8287 - val_loss: 0.3379 - val_accuracy: 0.7667\n",
            "Epoch 90/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1803 - accuracy: 0.8250 - val_loss: 0.4127 - val_accuracy: 0.7250\n",
            "Epoch 91/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1781 - accuracy: 0.8273 - val_loss: 0.3724 - val_accuracy: 0.7417\n",
            "Epoch 92/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1705 - accuracy: 0.8296 - val_loss: 0.3813 - val_accuracy: 0.7472\n",
            "Epoch 93/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1850 - accuracy: 0.8259 - val_loss: 0.3991 - val_accuracy: 0.7278\n",
            "Epoch 94/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1883 - accuracy: 0.8250 - val_loss: 0.4727 - val_accuracy: 0.7139\n",
            "Epoch 95/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1812 - accuracy: 0.8292 - val_loss: 0.4415 - val_accuracy: 0.7306\n",
            "Epoch 96/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1806 - accuracy: 0.8306 - val_loss: 0.4275 - val_accuracy: 0.7111\n",
            "Epoch 97/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1833 - accuracy: 0.8296 - val_loss: 0.4392 - val_accuracy: 0.7194\n",
            "Epoch 98/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2008 - accuracy: 0.8255 - val_loss: 0.3283 - val_accuracy: 0.7556\n",
            "Epoch 99/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1961 - accuracy: 0.8259 - val_loss: 0.4225 - val_accuracy: 0.7278\n",
            "Epoch 100/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1927 - accuracy: 0.8273 - val_loss: 0.3742 - val_accuracy: 0.7472\n",
            "Epoch 101/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1869 - accuracy: 0.8301 - val_loss: 0.3621 - val_accuracy: 0.7528\n",
            "Epoch 102/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1867 - accuracy: 0.8296 - val_loss: 0.4150 - val_accuracy: 0.7528\n",
            "Epoch 103/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1943 - accuracy: 0.8287 - val_loss: 0.4413 - val_accuracy: 0.7306\n",
            "Epoch 104/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1972 - accuracy: 0.8259 - val_loss: 0.4012 - val_accuracy: 0.7417\n",
            "Epoch 105/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1895 - accuracy: 0.8296 - val_loss: 0.3502 - val_accuracy: 0.7583\n",
            "Epoch 106/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1935 - accuracy: 0.8287 - val_loss: 0.4131 - val_accuracy: 0.7583\n",
            "Epoch 107/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1961 - accuracy: 0.8282 - val_loss: 0.3894 - val_accuracy: 0.7556\n",
            "Epoch 108/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1976 - accuracy: 0.8292 - val_loss: 0.3906 - val_accuracy: 0.7389\n",
            "Epoch 109/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1994 - accuracy: 0.8292 - val_loss: 0.3453 - val_accuracy: 0.7778\n",
            "Epoch 110/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1967 - accuracy: 0.8282 - val_loss: 0.4634 - val_accuracy: 0.7278\n",
            "Epoch 111/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1968 - accuracy: 0.8278 - val_loss: 0.3409 - val_accuracy: 0.7639\n",
            "Epoch 112/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1907 - accuracy: 0.8310 - val_loss: 0.3661 - val_accuracy: 0.7750\n",
            "Epoch 113/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1961 - accuracy: 0.8273 - val_loss: 0.3267 - val_accuracy: 0.7722\n",
            "Epoch 114/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1880 - accuracy: 0.8324 - val_loss: 0.3487 - val_accuracy: 0.7639\n",
            "Epoch 115/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1883 - accuracy: 0.8301 - val_loss: 0.3404 - val_accuracy: 0.7667\n",
            "Epoch 116/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1889 - accuracy: 0.8315 - val_loss: 0.3469 - val_accuracy: 0.7500\n",
            "Epoch 117/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1936 - accuracy: 0.8301 - val_loss: 0.3813 - val_accuracy: 0.7583\n",
            "Epoch 118/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1925 - accuracy: 0.8324 - val_loss: 0.3721 - val_accuracy: 0.7556\n",
            "Epoch 119/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1996 - accuracy: 0.8273 - val_loss: 0.3420 - val_accuracy: 0.7667\n",
            "Epoch 120/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1948 - accuracy: 0.8287 - val_loss: 0.3716 - val_accuracy: 0.7639\n",
            "Epoch 121/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1916 - accuracy: 0.8319 - val_loss: 0.3854 - val_accuracy: 0.7528\n",
            "Epoch 122/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2053 - accuracy: 0.8259 - val_loss: 0.3636 - val_accuracy: 0.7667\n",
            "Epoch 123/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1968 - accuracy: 0.8310 - val_loss: 0.3967 - val_accuracy: 0.7556\n",
            "Epoch 124/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1960 - accuracy: 0.8306 - val_loss: 0.3726 - val_accuracy: 0.7444\n",
            "Epoch 125/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1957 - accuracy: 0.8282 - val_loss: 0.3320 - val_accuracy: 0.7694\n",
            "Epoch 126/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1947 - accuracy: 0.8287 - val_loss: 0.4427 - val_accuracy: 0.7083\n",
            "Epoch 127/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1986 - accuracy: 0.8287 - val_loss: 0.3877 - val_accuracy: 0.7417\n",
            "Epoch 128/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1957 - accuracy: 0.8306 - val_loss: 0.4050 - val_accuracy: 0.7500\n",
            "Epoch 129/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1921 - accuracy: 0.8315 - val_loss: 0.3771 - val_accuracy: 0.7361\n",
            "Epoch 130/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1866 - accuracy: 0.8324 - val_loss: 0.3533 - val_accuracy: 0.7722\n",
            "Epoch 131/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1953 - accuracy: 0.8292 - val_loss: 0.3901 - val_accuracy: 0.7556\n",
            "Epoch 132/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1938 - accuracy: 0.8282 - val_loss: 0.3356 - val_accuracy: 0.7694\n",
            "Epoch 133/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1946 - accuracy: 0.8319 - val_loss: 0.3578 - val_accuracy: 0.7694\n",
            "Epoch 134/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1956 - accuracy: 0.8296 - val_loss: 0.4065 - val_accuracy: 0.7361\n",
            "Epoch 135/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1931 - accuracy: 0.8310 - val_loss: 0.3357 - val_accuracy: 0.7583\n",
            "Epoch 136/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1841 - accuracy: 0.8319 - val_loss: 0.3282 - val_accuracy: 0.7611\n",
            "Epoch 137/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1980 - accuracy: 0.8310 - val_loss: 0.3453 - val_accuracy: 0.7722\n",
            "Epoch 138/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1973 - accuracy: 0.8315 - val_loss: 0.3424 - val_accuracy: 0.7722\n",
            "Epoch 139/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1951 - accuracy: 0.8310 - val_loss: 0.3593 - val_accuracy: 0.7583\n",
            "Epoch 140/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1982 - accuracy: 0.8287 - val_loss: 0.3041 - val_accuracy: 0.7861\n",
            "Epoch 141/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1931 - accuracy: 0.8310 - val_loss: 0.3209 - val_accuracy: 0.7694\n",
            "Epoch 142/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1922 - accuracy: 0.8310 - val_loss: 0.4181 - val_accuracy: 0.7444\n",
            "Epoch 143/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1932 - accuracy: 0.8319 - val_loss: 0.3258 - val_accuracy: 0.7833\n",
            "Epoch 144/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1973 - accuracy: 0.8273 - val_loss: 0.3697 - val_accuracy: 0.7444\n",
            "Epoch 145/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1905 - accuracy: 0.8319 - val_loss: 0.3911 - val_accuracy: 0.7250\n",
            "Epoch 146/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1907 - accuracy: 0.8310 - val_loss: 0.3847 - val_accuracy: 0.7556\n",
            "Epoch 147/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1879 - accuracy: 0.8319 - val_loss: 0.3555 - val_accuracy: 0.7556\n",
            "Epoch 148/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1884 - accuracy: 0.8315 - val_loss: 0.2982 - val_accuracy: 0.7861\n",
            "Epoch 149/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1922 - accuracy: 0.8315 - val_loss: 0.3177 - val_accuracy: 0.7750\n",
            "Epoch 150/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1940 - accuracy: 0.8292 - val_loss: 0.3591 - val_accuracy: 0.7694\n",
            "Epoch 151/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1918 - accuracy: 0.8310 - val_loss: 0.3883 - val_accuracy: 0.7528\n",
            "Epoch 152/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1925 - accuracy: 0.8306 - val_loss: 0.3615 - val_accuracy: 0.7639\n",
            "Epoch 153/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1872 - accuracy: 0.8319 - val_loss: 0.3138 - val_accuracy: 0.7750\n",
            "Epoch 154/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1970 - accuracy: 0.8301 - val_loss: 0.3182 - val_accuracy: 0.7694\n",
            "Epoch 155/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1930 - accuracy: 0.8306 - val_loss: 0.3543 - val_accuracy: 0.7556\n",
            "Epoch 156/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1874 - accuracy: 0.8310 - val_loss: 0.3628 - val_accuracy: 0.7694\n",
            "Epoch 157/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1851 - accuracy: 0.8324 - val_loss: 0.3358 - val_accuracy: 0.7694\n",
            "Epoch 158/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1890 - accuracy: 0.8324 - val_loss: 0.3482 - val_accuracy: 0.7583\n",
            "Epoch 159/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1808 - accuracy: 0.8329 - val_loss: 0.3159 - val_accuracy: 0.7778\n",
            "Epoch 160/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1904 - accuracy: 0.8315 - val_loss: 0.3514 - val_accuracy: 0.7722\n",
            "Epoch 161/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1929 - accuracy: 0.8310 - val_loss: 0.3265 - val_accuracy: 0.7639\n",
            "Epoch 162/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1899 - accuracy: 0.8329 - val_loss: 0.3897 - val_accuracy: 0.7500\n",
            "Epoch 163/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1962 - accuracy: 0.8306 - val_loss: 0.3996 - val_accuracy: 0.7389\n",
            "Epoch 164/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1908 - accuracy: 0.8296 - val_loss: 0.3768 - val_accuracy: 0.7611\n",
            "Epoch 165/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1918 - accuracy: 0.8310 - val_loss: 0.3555 - val_accuracy: 0.7694\n",
            "Epoch 166/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1866 - accuracy: 0.8324 - val_loss: 0.3286 - val_accuracy: 0.7611\n",
            "Epoch 167/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1851 - accuracy: 0.8324 - val_loss: 0.3088 - val_accuracy: 0.7750\n",
            "Epoch 168/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1876 - accuracy: 0.8324 - val_loss: 0.3227 - val_accuracy: 0.7750\n",
            "Epoch 169/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1858 - accuracy: 0.8319 - val_loss: 0.3185 - val_accuracy: 0.7778\n",
            "Epoch 170/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1863 - accuracy: 0.8329 - val_loss: 0.3505 - val_accuracy: 0.7778\n",
            "Epoch 171/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1906 - accuracy: 0.8306 - val_loss: 0.3478 - val_accuracy: 0.7611\n",
            "Epoch 172/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1856 - accuracy: 0.8310 - val_loss: 0.3330 - val_accuracy: 0.7750\n",
            "Epoch 173/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1924 - accuracy: 0.8301 - val_loss: 0.3635 - val_accuracy: 0.7611\n",
            "Epoch 174/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1991 - accuracy: 0.8301 - val_loss: 0.4307 - val_accuracy: 0.7417\n",
            "Epoch 175/200\n",
            "71/72 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.8315Restoring model weights from the end of the best epoch: 140.\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.1950 - accuracy: 0.8319 - val_loss: 0.3841 - val_accuracy: 0.7528\n",
            "Epoch 175: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_hy_gru_all.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "history_class_mod = model_hy_gru_all.fit([X_train_pres,X_train], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=([X_val_pres,X_val], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY-Ecb7zge_C"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n57bYb6DH5sc",
        "outputId": "824d2fd2-410a-4087-97e0-9d409ced7d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 2s 16ms/step\n",
            "12/12 [==============================] - 0s 11ms/step\n",
            "Accuracy validation: 0.7861111111111111\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_hy_gru_all.predict([X_test_pres,X_test]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_hy_gru_all.predict([X_val_pres,X_val]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVrNHMNzKfPp"
      },
      "source": [
        "# Sub1\n",
        "\n",
        "* LSTM\n",
        "* GRU\n",
        "* Hybrid pressuser+kinect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojn9LZRjZmbf"
      },
      "outputs": [],
      "source": [
        "features = joints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAh4PQTUKfP2"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFXlwp6KKfP2"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_3jMn9lg5-3"
      },
      "outputs": [],
      "source": [
        "#building LST RNN model for Sub1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JkTEAzZKfP3"
      },
      "outputs": [],
      "source": [
        "class classifier_lstm(Model):\n",
        "  def __init__(self):\n",
        "    super(classifier_lstm, self).__init__()\n",
        "    self.input_ = len(features)\n",
        "    self.rec =  tf.keras.Sequential([\n",
        "            layers.LSTM(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=self.input_, return_sequences=False,dropout=0.5),\n",
        "          ])\n",
        "\n",
        "    self.ffnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "  def call(self,x):\n",
        "    out = self.rec(x)\n",
        "    clas = self.ffnn(out)\n",
        "    return clas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSN6df5YKfP3"
      },
      "outputs": [],
      "source": [
        "model_lstm_sub1 = classifier_lstm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1Gef0xXKfP3",
        "outputId": "31cb2f8f-03a9-4eec-93b0-e9b91926651b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_56\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_72 (LSTM)              (None, None, 256)         296960    \n",
            "                                                                 \n",
            " lstm_73 (LSTM)              (None, None, 256)         525312    \n",
            "                                                                 \n",
            " lstm_74 (LSTM)              (None, None, 128)         197120    \n",
            "                                                                 \n",
            " lstm_75 (LSTM)              (None, 128)               131584    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,150,976\n",
            "Trainable params: 1,150,976\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_sub1.rec.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvNurhYoKfP3",
        "outputId": "3c5beb58-7523-4a8d-820e-fa975fd642d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_57\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_133 (Dense)           (2, 128)                  16384     \n",
            "                                                                 \n",
            " batch_normalization_108 (Ba  (2, 128)                 512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_114 (Activation)  (2, 128)                 0         \n",
            "                                                                 \n",
            " dense_134 (Dense)           (2, 64)                   8192      \n",
            "                                                                 \n",
            " batch_normalization_109 (Ba  (2, 64)                  256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_115 (Activation)  (2, 64)                  0         \n",
            "                                                                 \n",
            " dense_135 (Dense)           (2, 32)                   2048      \n",
            "                                                                 \n",
            " batch_normalization_110 (Ba  (2, 32)                  128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_116 (Activation)  (2, 32)                  0         \n",
            "                                                                 \n",
            " dense_136 (Dense)           (2, 16)                   512       \n",
            "                                                                 \n",
            " batch_normalization_111 (Ba  (2, 16)                  64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_117 (Activation)  (2, 16)                  0         \n",
            "                                                                 \n",
            " dense_137 (Dense)           (2, 6)                    102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,198\n",
            "Trainable params: 27,718\n",
            "Non-trainable params: 480\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_sub1(X_train[1:3,:,features])\n",
        "model_lstm_sub1.ffnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPzuQ280hAnF"
      },
      "outputs": [],
      "source": [
        "#training LST RNN model on Sub1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvEoNyalKfP3",
        "outputId": "ea371c6c-99c7-4f3b-d734-4f49039c629d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 9s 46ms/step - loss: 1.7417 - accuracy: 0.1861 - val_loss: 1.5050 - val_accuracy: 0.1667\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.4677 - accuracy: 0.2523 - val_loss: 1.4813 - val_accuracy: 0.1667\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.2817 - accuracy: 0.3222 - val_loss: 1.3777 - val_accuracy: 0.2222\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.1538 - accuracy: 0.3843 - val_loss: 1.2040 - val_accuracy: 0.4222\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.0641 - accuracy: 0.4630 - val_loss: 1.0943 - val_accuracy: 0.4889\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.9938 - accuracy: 0.5102 - val_loss: 0.9196 - val_accuracy: 0.6278\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.9217 - accuracy: 0.5569 - val_loss: 0.8220 - val_accuracy: 0.6333\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.8565 - accuracy: 0.6301 - val_loss: 0.8196 - val_accuracy: 0.6167\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.7940 - accuracy: 0.6894 - val_loss: 0.7879 - val_accuracy: 0.6583\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.7363 - accuracy: 0.7097 - val_loss: 0.7136 - val_accuracy: 0.6778\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.6935 - accuracy: 0.7264 - val_loss: 0.7127 - val_accuracy: 0.7361\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.6619 - accuracy: 0.7421 - val_loss: 0.7755 - val_accuracy: 0.6667\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.6159 - accuracy: 0.7815 - val_loss: 0.6489 - val_accuracy: 0.7028\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.5851 - accuracy: 0.8009 - val_loss: 0.6146 - val_accuracy: 0.7778\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5434 - accuracy: 0.8319 - val_loss: 0.6668 - val_accuracy: 0.7250\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5166 - accuracy: 0.8394 - val_loss: 0.8003 - val_accuracy: 0.6250\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.4880 - accuracy: 0.8611 - val_loss: 0.6889 - val_accuracy: 0.7111\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.4556 - accuracy: 0.8704 - val_loss: 0.7371 - val_accuracy: 0.7222\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.4270 - accuracy: 0.8912 - val_loss: 0.6183 - val_accuracy: 0.7389\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.3918 - accuracy: 0.9069 - val_loss: 0.5495 - val_accuracy: 0.7667\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.3889 - accuracy: 0.8954 - val_loss: 0.5978 - val_accuracy: 0.7194\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.3366 - accuracy: 0.9389 - val_loss: 0.6489 - val_accuracy: 0.7056\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3371 - accuracy: 0.9292 - val_loss: 0.5889 - val_accuracy: 0.7389\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2974 - accuracy: 0.9440 - val_loss: 0.5979 - val_accuracy: 0.7611\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2938 - accuracy: 0.9435 - val_loss: 0.7913 - val_accuracy: 0.6722\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2826 - accuracy: 0.9440 - val_loss: 0.6001 - val_accuracy: 0.7556\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2547 - accuracy: 0.9597 - val_loss: 0.6132 - val_accuracy: 0.7389\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2451 - accuracy: 0.9556 - val_loss: 0.6102 - val_accuracy: 0.7500\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2479 - accuracy: 0.9505 - val_loss: 0.6271 - val_accuracy: 0.7361\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2117 - accuracy: 0.9667 - val_loss: 0.6940 - val_accuracy: 0.6972\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2079 - accuracy: 0.9523 - val_loss: 0.6064 - val_accuracy: 0.7361\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1946 - accuracy: 0.9620 - val_loss: 0.6722 - val_accuracy: 0.7056\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1880 - accuracy: 0.9625 - val_loss: 0.6625 - val_accuracy: 0.7111\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1904 - accuracy: 0.9653 - val_loss: 0.7022 - val_accuracy: 0.7389\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1871 - accuracy: 0.9616 - val_loss: 0.6289 - val_accuracy: 0.7389\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1868 - accuracy: 0.9606 - val_loss: 0.6464 - val_accuracy: 0.7306\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1640 - accuracy: 0.9759 - val_loss: 0.6091 - val_accuracy: 0.7667\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1641 - accuracy: 0.9722 - val_loss: 0.7109 - val_accuracy: 0.7194\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1541 - accuracy: 0.9611 - val_loss: 0.6041 - val_accuracy: 0.7889\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1650 - accuracy: 0.9495 - val_loss: 0.6294 - val_accuracy: 0.7528\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1405 - accuracy: 0.9694 - val_loss: 0.7610 - val_accuracy: 0.7194\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1446 - accuracy: 0.9593 - val_loss: 0.6832 - val_accuracy: 0.7361\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1274 - accuracy: 0.9713 - val_loss: 0.6112 - val_accuracy: 0.7333\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1348 - accuracy: 0.9630 - val_loss: 0.7803 - val_accuracy: 0.6944\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1207 - accuracy: 0.9681 - val_loss: 0.6673 - val_accuracy: 0.7056\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1256 - accuracy: 0.9699 - val_loss: 0.6715 - val_accuracy: 0.7417\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1147 - accuracy: 0.9630 - val_loss: 0.7238 - val_accuracy: 0.7278\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1246 - accuracy: 0.9657 - val_loss: 0.6193 - val_accuracy: 0.7556\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1133 - accuracy: 0.9731 - val_loss: 0.7245 - val_accuracy: 0.7083\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1078 - accuracy: 0.9602 - val_loss: 0.7263 - val_accuracy: 0.7333\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0997 - accuracy: 0.9681 - val_loss: 0.6276 - val_accuracy: 0.7528\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0975 - accuracy: 0.9639 - val_loss: 0.6427 - val_accuracy: 0.7278\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1003 - accuracy: 0.9481 - val_loss: 0.6904 - val_accuracy: 0.7583\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1100 - accuracy: 0.9454 - val_loss: 0.6728 - val_accuracy: 0.7472\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0882 - accuracy: 0.9519 - val_loss: 0.8132 - val_accuracy: 0.6944\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0864 - accuracy: 0.9546 - val_loss: 0.8695 - val_accuracy: 0.6556\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0972 - accuracy: 0.9454 - val_loss: 0.6480 - val_accuracy: 0.6972\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0974 - accuracy: 0.9366 - val_loss: 0.8128 - val_accuracy: 0.6972\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1021 - accuracy: 0.9343 - val_loss: 0.7974 - val_accuracy: 0.7139\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0847 - accuracy: 0.9449 - val_loss: 0.7947 - val_accuracy: 0.6889\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0884 - accuracy: 0.9315 - val_loss: 0.6087 - val_accuracy: 0.7250\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0733 - accuracy: 0.9352 - val_loss: 0.6972 - val_accuracy: 0.6722\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.0699 - accuracy: 0.9356 - val_loss: 0.7332 - val_accuracy: 0.6861\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0821 - accuracy: 0.9296 - val_loss: 0.8611 - val_accuracy: 0.6611\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0770 - accuracy: 0.9282 - val_loss: 0.7390 - val_accuracy: 0.6750\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0692 - accuracy: 0.9324 - val_loss: 0.8037 - val_accuracy: 0.6778\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0865 - accuracy: 0.9194 - val_loss: 0.9211 - val_accuracy: 0.6389\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0971 - accuracy: 0.9060 - val_loss: 0.7171 - val_accuracy: 0.6778\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0746 - accuracy: 0.9236 - val_loss: 0.7717 - val_accuracy: 0.6944\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0725 - accuracy: 0.9190 - val_loss: 0.5393 - val_accuracy: 0.7222\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.0704 - accuracy: 0.9042 - val_loss: 0.7335 - val_accuracy: 0.6861\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0654 - accuracy: 0.9056 - val_loss: 0.7060 - val_accuracy: 0.7389\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0847 - accuracy: 0.8889 - val_loss: 0.8319 - val_accuracy: 0.7194\n",
            "Epoch 74/200\n",
            "70/72 [============================>.] - ETA: 0s - loss: 0.1007 - accuracy: 0.8671Restoring model weights from the end of the best epoch: 39.\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0999 - accuracy: 0.8708 - val_loss: 0.6932 - val_accuracy: 0.7306\n",
            "Epoch 74: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_lstm_sub1.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "history_class_mod = model_lstm_sub1.fit(X_train[:,:,features], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_val[:,:,features], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3WG8eumhE1w"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5M2jAIyKfP3",
        "outputId": "0de74065-66c1-409f-dc3d-4afacfd1037d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 1s 15ms/step\n",
            "12/12 [==============================] - 0s 12ms/step\n",
            "Accuracy validation: 0.7888888888888889\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_lstm_sub1.predict(X_test[:,:,features]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_lstm_sub1.predict(X_val[:,:,features]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6psAvIYKfP4"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1vMCGEhbxMr"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWz1986ni4kA"
      },
      "outputs": [],
      "source": [
        "#building GRU RNN for \"sub1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJJzUBxRKfP4"
      },
      "outputs": [],
      "source": [
        "class classifier_gru(Model):\n",
        "  def __init__(self):\n",
        "    super(classifier_gru, self).__init__()\n",
        "    self.input_ = len(features)\n",
        "    self.rec =  tf.keras.Sequential([\n",
        "            layers.GRU(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=self.input_, return_sequences=False,dropout=0.5),\n",
        "            ])\n",
        "\n",
        "\n",
        "    self.ffnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "  def call(self,x):\n",
        "    out = self.rec(x)\n",
        "    clas = self.ffnn(out)\n",
        "    return clas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkBzUE3tKfP4"
      },
      "outputs": [],
      "source": [
        "model_gru_sub1 = classifier_gru()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPq96-26KfP4",
        "outputId": "738d7a81-ebcc-4f87-bd57-9661619b23a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_58\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_28 (GRU)                (None, None, 256)         223488    \n",
            "                                                                 \n",
            " gru_29 (GRU)                (None, None, 256)         394752    \n",
            "                                                                 \n",
            " gru_30 (GRU)                (None, None, 128)         148224    \n",
            "                                                                 \n",
            " gru_31 (GRU)                (None, 128)               99072     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 865,536\n",
            "Trainable params: 865,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_gru_sub1.rec.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFQr3sm8KfP4",
        "outputId": "161ceb1d-574b-4775-982e-77d9f757b291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_59\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_138 (Dense)           (2, 128)                  16384     \n",
            "                                                                 \n",
            " batch_normalization_112 (Ba  (2, 128)                 512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_118 (Activation)  (2, 128)                 0         \n",
            "                                                                 \n",
            " dense_139 (Dense)           (2, 64)                   8192      \n",
            "                                                                 \n",
            " batch_normalization_113 (Ba  (2, 64)                  256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_119 (Activation)  (2, 64)                  0         \n",
            "                                                                 \n",
            " dense_140 (Dense)           (2, 32)                   2048      \n",
            "                                                                 \n",
            " batch_normalization_114 (Ba  (2, 32)                  128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_120 (Activation)  (2, 32)                  0         \n",
            "                                                                 \n",
            " dense_141 (Dense)           (2, 16)                   512       \n",
            "                                                                 \n",
            " batch_normalization_115 (Ba  (2, 16)                  64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_121 (Activation)  (2, 16)                  0         \n",
            "                                                                 \n",
            " dense_142 (Dense)           (2, 6)                    102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,198\n",
            "Trainable params: 27,718\n",
            "Non-trainable params: 480\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_gru_sub1(X_train[1:3,:,features])\n",
        "model_gru_sub1.ffnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTjeSBTwlSvc"
      },
      "outputs": [],
      "source": [
        "#Training the GRU RNN on \"sub1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6G4QPe4KfP4",
        "outputId": "5f18c9d2-d3f3-4539-e45e-ed587bcebec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 10s 55ms/step - loss: 1.9154 - accuracy: 0.1676 - val_loss: 1.4896 - val_accuracy: 0.1694\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.7938 - accuracy: 0.1630 - val_loss: 1.4531 - val_accuracy: 0.2111\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 1.7056 - accuracy: 0.1759 - val_loss: 1.3889 - val_accuracy: 0.2306\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.5926 - accuracy: 0.2287 - val_loss: 1.3571 - val_accuracy: 0.3583\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.5048 - accuracy: 0.2458 - val_loss: 1.2708 - val_accuracy: 0.2944\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 1.4506 - accuracy: 0.2565 - val_loss: 1.3281 - val_accuracy: 0.3194\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.4081 - accuracy: 0.2912 - val_loss: 1.1521 - val_accuracy: 0.3278\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 1s 20ms/step - loss: 1.3568 - accuracy: 0.3069 - val_loss: 1.1095 - val_accuracy: 0.3389\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 1s 20ms/step - loss: 1.3065 - accuracy: 0.3292 - val_loss: 1.0867 - val_accuracy: 0.3417\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 1.2768 - accuracy: 0.3449 - val_loss: 1.0116 - val_accuracy: 0.4056\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 1.2525 - accuracy: 0.3657 - val_loss: 1.0289 - val_accuracy: 0.3500\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.2258 - accuracy: 0.3921 - val_loss: 1.0194 - val_accuracy: 0.4194\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.1867 - accuracy: 0.4241 - val_loss: 0.9398 - val_accuracy: 0.4611\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 1.1654 - accuracy: 0.4241 - val_loss: 0.9762 - val_accuracy: 0.4278\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 1.1380 - accuracy: 0.4468 - val_loss: 0.9901 - val_accuracy: 0.4333\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.0999 - accuracy: 0.4583 - val_loss: 0.8711 - val_accuracy: 0.5028\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.0657 - accuracy: 0.4986 - val_loss: 0.8506 - val_accuracy: 0.5722\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.0258 - accuracy: 0.5130 - val_loss: 0.7542 - val_accuracy: 0.5861\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.9853 - accuracy: 0.5403 - val_loss: 0.6945 - val_accuracy: 0.6417\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.9313 - accuracy: 0.5560 - val_loss: 0.6372 - val_accuracy: 0.6333\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.8649 - accuracy: 0.6032 - val_loss: 0.6605 - val_accuracy: 0.6111\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.8227 - accuracy: 0.6250 - val_loss: 0.6663 - val_accuracy: 0.6167\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.7615 - accuracy: 0.6644 - val_loss: 0.7050 - val_accuracy: 0.6278\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.7087 - accuracy: 0.6704 - val_loss: 0.7258 - val_accuracy: 0.5889\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.6792 - accuracy: 0.6852 - val_loss: 0.6112 - val_accuracy: 0.6750\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.6531 - accuracy: 0.6981 - val_loss: 0.6433 - val_accuracy: 0.6361\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.6196 - accuracy: 0.7074 - val_loss: 0.6251 - val_accuracy: 0.6306\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.5948 - accuracy: 0.7245 - val_loss: 0.5666 - val_accuracy: 0.6750\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.5736 - accuracy: 0.7181 - val_loss: 0.5929 - val_accuracy: 0.6472\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.5470 - accuracy: 0.7306 - val_loss: 0.6198 - val_accuracy: 0.6417\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5208 - accuracy: 0.7468 - val_loss: 0.5525 - val_accuracy: 0.6556\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.5065 - accuracy: 0.7407 - val_loss: 0.6223 - val_accuracy: 0.6250\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.4871 - accuracy: 0.7514 - val_loss: 0.5979 - val_accuracy: 0.6361\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.4755 - accuracy: 0.7537 - val_loss: 0.6067 - val_accuracy: 0.6278\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.4501 - accuracy: 0.7648 - val_loss: 0.5675 - val_accuracy: 0.6417\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.4463 - accuracy: 0.7653 - val_loss: 0.6057 - val_accuracy: 0.6333\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.4295 - accuracy: 0.7759 - val_loss: 0.5390 - val_accuracy: 0.6583\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.4237 - accuracy: 0.7741 - val_loss: 0.5858 - val_accuracy: 0.6444\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.3919 - accuracy: 0.7787 - val_loss: 0.6016 - val_accuracy: 0.6222\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.3828 - accuracy: 0.7852 - val_loss: 0.5857 - val_accuracy: 0.6417\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.3720 - accuracy: 0.7912 - val_loss: 0.5147 - val_accuracy: 0.6722\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.3551 - accuracy: 0.8069 - val_loss: 0.5683 - val_accuracy: 0.6417\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.3534 - accuracy: 0.7954 - val_loss: 0.5281 - val_accuracy: 0.6667\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.3457 - accuracy: 0.8032 - val_loss: 0.5593 - val_accuracy: 0.6500\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.3322 - accuracy: 0.8185 - val_loss: 0.5219 - val_accuracy: 0.6722\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.3147 - accuracy: 0.8273 - val_loss: 0.5376 - val_accuracy: 0.6500\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.3089 - accuracy: 0.8343 - val_loss: 0.4741 - val_accuracy: 0.7111\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.3114 - accuracy: 0.8273 - val_loss: 0.6532 - val_accuracy: 0.6194\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2818 - accuracy: 0.8449 - val_loss: 0.5617 - val_accuracy: 0.6333\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2848 - accuracy: 0.8343 - val_loss: 0.5226 - val_accuracy: 0.6611\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.2868 - accuracy: 0.8296 - val_loss: 0.5359 - val_accuracy: 0.6972\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.2620 - accuracy: 0.8403 - val_loss: 0.5189 - val_accuracy: 0.7000\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.2521 - accuracy: 0.8394 - val_loss: 0.5871 - val_accuracy: 0.6472\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.2596 - accuracy: 0.8306 - val_loss: 0.5654 - val_accuracy: 0.6861\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.2402 - accuracy: 0.8361 - val_loss: 0.4946 - val_accuracy: 0.7000\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2321 - accuracy: 0.8333 - val_loss: 0.5264 - val_accuracy: 0.6889\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2360 - accuracy: 0.8231 - val_loss: 0.5363 - val_accuracy: 0.6806\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2315 - accuracy: 0.8236 - val_loss: 0.5159 - val_accuracy: 0.7139\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.2315 - accuracy: 0.8347 - val_loss: 0.4992 - val_accuracy: 0.6667\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.2201 - accuracy: 0.8417 - val_loss: 0.5611 - val_accuracy: 0.6583\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.2148 - accuracy: 0.8343 - val_loss: 0.5860 - val_accuracy: 0.6528\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.2236 - accuracy: 0.8287 - val_loss: 0.6061 - val_accuracy: 0.6417\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.2106 - accuracy: 0.8319 - val_loss: 0.6483 - val_accuracy: 0.6500\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.2087 - accuracy: 0.8296 - val_loss: 0.5233 - val_accuracy: 0.6639\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1927 - accuracy: 0.8352 - val_loss: 0.4404 - val_accuracy: 0.7167\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1973 - accuracy: 0.8222 - val_loss: 0.5360 - val_accuracy: 0.6694\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1842 - accuracy: 0.8227 - val_loss: 0.4255 - val_accuracy: 0.7111\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1979 - accuracy: 0.8222 - val_loss: 0.5377 - val_accuracy: 0.6722\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1818 - accuracy: 0.8148 - val_loss: 0.5488 - val_accuracy: 0.6667\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1848 - accuracy: 0.8190 - val_loss: 0.5593 - val_accuracy: 0.6556\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1765 - accuracy: 0.8208 - val_loss: 0.5169 - val_accuracy: 0.6667\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1617 - accuracy: 0.8241 - val_loss: 0.5798 - val_accuracy: 0.6611\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1727 - accuracy: 0.8134 - val_loss: 0.5447 - val_accuracy: 0.6750\n",
            "Epoch 74/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1616 - accuracy: 0.8227 - val_loss: 0.5058 - val_accuracy: 0.6889\n",
            "Epoch 75/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1792 - accuracy: 0.8106 - val_loss: 0.5820 - val_accuracy: 0.6472\n",
            "Epoch 76/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1648 - accuracy: 0.8130 - val_loss: 0.5168 - val_accuracy: 0.6694\n",
            "Epoch 77/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1580 - accuracy: 0.8199 - val_loss: 0.5753 - val_accuracy: 0.6750\n",
            "Epoch 78/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1473 - accuracy: 0.8273 - val_loss: 0.5698 - val_accuracy: 0.6750\n",
            "Epoch 79/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1495 - accuracy: 0.8250 - val_loss: 0.4576 - val_accuracy: 0.6917\n",
            "Epoch 80/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1549 - accuracy: 0.8222 - val_loss: 0.4794 - val_accuracy: 0.6944\n",
            "Epoch 81/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1656 - accuracy: 0.8167 - val_loss: 0.6009 - val_accuracy: 0.6583\n",
            "Epoch 82/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1440 - accuracy: 0.8296 - val_loss: 0.4262 - val_accuracy: 0.7111\n",
            "Epoch 83/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1375 - accuracy: 0.8333 - val_loss: 0.4916 - val_accuracy: 0.6944\n",
            "Epoch 84/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1465 - accuracy: 0.8343 - val_loss: 0.5490 - val_accuracy: 0.6778\n",
            "Epoch 85/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1444 - accuracy: 0.8236 - val_loss: 0.6209 - val_accuracy: 0.6667\n",
            "Epoch 86/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1442 - accuracy: 0.8231 - val_loss: 0.4804 - val_accuracy: 0.7028\n",
            "Epoch 87/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1481 - accuracy: 0.8227 - val_loss: 0.5825 - val_accuracy: 0.6778\n",
            "Epoch 88/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1446 - accuracy: 0.8222 - val_loss: 0.5723 - val_accuracy: 0.6611\n",
            "Epoch 89/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1357 - accuracy: 0.8319 - val_loss: 0.5475 - val_accuracy: 0.6806\n",
            "Epoch 90/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1373 - accuracy: 0.8356 - val_loss: 0.5280 - val_accuracy: 0.6889\n",
            "Epoch 91/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1317 - accuracy: 0.8343 - val_loss: 0.6189 - val_accuracy: 0.6611\n",
            "Epoch 92/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1452 - accuracy: 0.8333 - val_loss: 0.4837 - val_accuracy: 0.7111\n",
            "Epoch 93/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1375 - accuracy: 0.8329 - val_loss: 0.5948 - val_accuracy: 0.7056\n",
            "Epoch 94/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1463 - accuracy: 0.8273 - val_loss: 0.4613 - val_accuracy: 0.7333\n",
            "Epoch 95/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1361 - accuracy: 0.8306 - val_loss: 0.5332 - val_accuracy: 0.6889\n",
            "Epoch 96/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1298 - accuracy: 0.8282 - val_loss: 0.5450 - val_accuracy: 0.6806\n",
            "Epoch 97/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1415 - accuracy: 0.8231 - val_loss: 0.5580 - val_accuracy: 0.6833\n",
            "Epoch 98/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1437 - accuracy: 0.8213 - val_loss: 0.4147 - val_accuracy: 0.7139\n",
            "Epoch 99/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1290 - accuracy: 0.8278 - val_loss: 0.5874 - val_accuracy: 0.6778\n",
            "Epoch 100/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1323 - accuracy: 0.8236 - val_loss: 0.5470 - val_accuracy: 0.6778\n",
            "Epoch 101/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1266 - accuracy: 0.8231 - val_loss: 0.6463 - val_accuracy: 0.6556\n",
            "Epoch 102/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1245 - accuracy: 0.8222 - val_loss: 0.5508 - val_accuracy: 0.6861\n",
            "Epoch 103/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1297 - accuracy: 0.8213 - val_loss: 0.4935 - val_accuracy: 0.6944\n",
            "Epoch 104/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1279 - accuracy: 0.8185 - val_loss: 0.5060 - val_accuracy: 0.6944\n",
            "Epoch 105/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1176 - accuracy: 0.8208 - val_loss: 0.5359 - val_accuracy: 0.6806\n",
            "Epoch 106/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1187 - accuracy: 0.8236 - val_loss: 0.5232 - val_accuracy: 0.6806\n",
            "Epoch 107/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1175 - accuracy: 0.8250 - val_loss: 0.5949 - val_accuracy: 0.6611\n",
            "Epoch 108/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1168 - accuracy: 0.8231 - val_loss: 0.5451 - val_accuracy: 0.6778\n",
            "Epoch 109/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1209 - accuracy: 0.8199 - val_loss: 0.5124 - val_accuracy: 0.6972\n",
            "Epoch 110/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1141 - accuracy: 0.8208 - val_loss: 0.5051 - val_accuracy: 0.6917\n",
            "Epoch 111/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1190 - accuracy: 0.8222 - val_loss: 0.5701 - val_accuracy: 0.6750\n",
            "Epoch 112/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1145 - accuracy: 0.8199 - val_loss: 0.5734 - val_accuracy: 0.6833\n",
            "Epoch 113/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1080 - accuracy: 0.8236 - val_loss: 0.5059 - val_accuracy: 0.7000\n",
            "Epoch 114/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1064 - accuracy: 0.8222 - val_loss: 0.6304 - val_accuracy: 0.6639\n",
            "Epoch 115/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1021 - accuracy: 0.8245 - val_loss: 0.5035 - val_accuracy: 0.7000\n",
            "Epoch 116/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1021 - accuracy: 0.8241 - val_loss: 0.6294 - val_accuracy: 0.6556\n",
            "Epoch 117/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1093 - accuracy: 0.8231 - val_loss: 0.7768 - val_accuracy: 0.6139\n",
            "Epoch 118/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1074 - accuracy: 0.8208 - val_loss: 0.4846 - val_accuracy: 0.7000\n",
            "Epoch 119/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1062 - accuracy: 0.8213 - val_loss: 0.5815 - val_accuracy: 0.6778\n",
            "Epoch 120/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1113 - accuracy: 0.8199 - val_loss: 0.5553 - val_accuracy: 0.6889\n",
            "Epoch 121/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0924 - accuracy: 0.8273 - val_loss: 0.4578 - val_accuracy: 0.7111\n",
            "Epoch 122/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1093 - accuracy: 0.8199 - val_loss: 0.5264 - val_accuracy: 0.7000\n",
            "Epoch 123/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1052 - accuracy: 0.8236 - val_loss: 0.5290 - val_accuracy: 0.6972\n",
            "Epoch 124/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0978 - accuracy: 0.8231 - val_loss: 0.5884 - val_accuracy: 0.6778\n",
            "Epoch 125/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.0956 - accuracy: 0.8227 - val_loss: 0.6006 - val_accuracy: 0.6750\n",
            "Epoch 126/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0853 - accuracy: 0.8255 - val_loss: 0.5608 - val_accuracy: 0.6861\n",
            "Epoch 127/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0988 - accuracy: 0.8236 - val_loss: 0.4940 - val_accuracy: 0.6917\n",
            "Epoch 128/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1004 - accuracy: 0.8218 - val_loss: 0.7029 - val_accuracy: 0.6472\n",
            "Epoch 129/200\n",
            "70/72 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.8190Restoring model weights from the end of the best epoch: 94.\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0966 - accuracy: 0.8218 - val_loss: 0.5982 - val_accuracy: 0.6861\n",
            "Epoch 129: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_gru_sub1.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history_class_mod = model_gru_sub1.fit(X_train[:,:,features], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_val[:,:,features], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting on validation set"
      ],
      "metadata": {
        "id": "ElFjnR8Y-ZKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1rhcpXqKfP4",
        "outputId": "57bcfbd8-f704-44e5-8136-1c3bfb2a5926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 2s 15ms/step\n",
            "12/12 [==============================] - 0s 11ms/step\n",
            "Accuracy validation: 0.7333333333333333\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_gru_sub1.predict(X_test[:,:,features]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_gru_sub1.predict(X_val[:,:,features]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ftpDGjKfP6"
      },
      "source": [
        "## Hybrid-LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZb0Nt1UKfP6"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR3wAMIKlgKE"
      },
      "outputs": [],
      "source": [
        "#Building the Hybrid-LSTM RNN model for \"Sub1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEiwTc32KfP6"
      },
      "outputs": [],
      "source": [
        "class fuse(Model):\n",
        "  def __init__(self):\n",
        "    super(fuse, self).__init__()\n",
        "    self.cnn = tf.keras.Sequential([\n",
        "      layers.Input(shape=(128, 48,1)),\n",
        "      layers.Conv2D(128, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,1)),\n",
        "      layers.Conv2D(64, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Conv2D(32, (3, 3), activation=None, padding='same', strides=1),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Flatten()])\n",
        "\n",
        "\n",
        "\n",
        "    self.rnn =  tf.keras.Sequential([\n",
        "            layers.LSTM(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=len(features), return_sequences=False,dropout=0.5),\n",
        "    ])\n",
        "\n",
        "    self.calssifier = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "\n",
        "    self.fc_cnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "    self.fc_rnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "\n",
        "  def call(self,x):\n",
        "    out0 = self.cnn(x[0])\n",
        "    out0 = self.fc_cnn(out0)\n",
        "    out1 = self.rnn(x[1])\n",
        "    out1 = self.fc_rnn(out1)\n",
        "\n",
        "    out = self.calssifier(layers.concatenate([out0,out1]))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3TUq5gNKfP6"
      },
      "outputs": [],
      "source": [
        "model_hy_lstm_sub1=fuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlAmrFaoKfP6",
        "outputId": "7fe60f10-2ad2-4fad-f42b-5217bc2d5e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_60\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 64, 24, 128)       1280      \n",
            "                                                                 \n",
            " activation_122 (Activation)  (None, 64, 24, 128)      0         \n",
            "                                                                 \n",
            " average_pooling2d_6 (Averag  (None, 32, 24, 128)      0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 16, 12, 64)        73792     \n",
            "                                                                 \n",
            " activation_123 (Activation)  (None, 16, 12, 64)       0         \n",
            "                                                                 \n",
            " average_pooling2d_7 (Averag  (None, 8, 6, 64)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 6, 32)          18464     \n",
            "                                                                 \n",
            " activation_124 (Activation)  (None, 8, 6, 32)         0         \n",
            "                                                                 \n",
            " average_pooling2d_8 (Averag  (None, 4, 3, 32)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 384)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,536\n",
            "Trainable params: 93,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_hy_lstm_sub1.cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OXsYK1_luSS"
      },
      "outputs": [],
      "source": [
        "#Training the Hybrid-LSTM RNN model on \"Sub1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sQUE1wkKfP6",
        "outputId": "270c4f66-10fd-46e2-a421-cb281bdf3b26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 11s 64ms/step - loss: 1.6418 - accuracy: 0.1963 - val_loss: 1.5448 - val_accuracy: 0.1667\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 1.4580 - accuracy: 0.2528 - val_loss: 1.5828 - val_accuracy: 0.1389\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.3169 - accuracy: 0.3556 - val_loss: 1.5431 - val_accuracy: 0.2000\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.2059 - accuracy: 0.4324 - val_loss: 1.5017 - val_accuracy: 0.2389\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.0916 - accuracy: 0.5185 - val_loss: 1.3269 - val_accuracy: 0.3722\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.0121 - accuracy: 0.5620 - val_loss: 1.2056 - val_accuracy: 0.4000\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.9411 - accuracy: 0.6032 - val_loss: 1.0292 - val_accuracy: 0.4889\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.8582 - accuracy: 0.6500 - val_loss: 0.9439 - val_accuracy: 0.5583\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.8090 - accuracy: 0.6569 - val_loss: 0.9034 - val_accuracy: 0.5694\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.7409 - accuracy: 0.6824 - val_loss: 0.8276 - val_accuracy: 0.6111\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.6959 - accuracy: 0.7065 - val_loss: 0.7448 - val_accuracy: 0.6444\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.6561 - accuracy: 0.7231 - val_loss: 0.8019 - val_accuracy: 0.6222\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.6114 - accuracy: 0.7338 - val_loss: 0.6914 - val_accuracy: 0.6556\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5752 - accuracy: 0.7509 - val_loss: 0.6516 - val_accuracy: 0.6583\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.5331 - accuracy: 0.7634 - val_loss: 0.6524 - val_accuracy: 0.6500\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.5015 - accuracy: 0.7745 - val_loss: 0.6513 - val_accuracy: 0.6472\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4808 - accuracy: 0.7741 - val_loss: 0.5374 - val_accuracy: 0.7028\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4558 - accuracy: 0.7815 - val_loss: 0.5299 - val_accuracy: 0.7111\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.4370 - accuracy: 0.7917 - val_loss: 0.6251 - val_accuracy: 0.6611\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4010 - accuracy: 0.8037 - val_loss: 0.5665 - val_accuracy: 0.6778\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3949 - accuracy: 0.8000 - val_loss: 0.5362 - val_accuracy: 0.6778\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3766 - accuracy: 0.8023 - val_loss: 0.5889 - val_accuracy: 0.6556\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.3659 - accuracy: 0.8028 - val_loss: 0.5273 - val_accuracy: 0.6833\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3402 - accuracy: 0.8102 - val_loss: 0.5627 - val_accuracy: 0.6722\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.3510 - accuracy: 0.8065 - val_loss: 0.5832 - val_accuracy: 0.6556\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3291 - accuracy: 0.8093 - val_loss: 0.4740 - val_accuracy: 0.7083\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3079 - accuracy: 0.8181 - val_loss: 0.5589 - val_accuracy: 0.6833\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3127 - accuracy: 0.8153 - val_loss: 0.5356 - val_accuracy: 0.6944\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3010 - accuracy: 0.8199 - val_loss: 0.5113 - val_accuracy: 0.6861\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2846 - accuracy: 0.8162 - val_loss: 0.5597 - val_accuracy: 0.6639\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2870 - accuracy: 0.8148 - val_loss: 0.4842 - val_accuracy: 0.7000\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2767 - accuracy: 0.8148 - val_loss: 0.5403 - val_accuracy: 0.6694\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2641 - accuracy: 0.8185 - val_loss: 0.5735 - val_accuracy: 0.6694\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2636 - accuracy: 0.8199 - val_loss: 0.5509 - val_accuracy: 0.6639\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2648 - accuracy: 0.8144 - val_loss: 0.5045 - val_accuracy: 0.6889\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2587 - accuracy: 0.8190 - val_loss: 0.5471 - val_accuracy: 0.6611\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2642 - accuracy: 0.8176 - val_loss: 0.4678 - val_accuracy: 0.7000\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2496 - accuracy: 0.8204 - val_loss: 0.4992 - val_accuracy: 0.6778\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2353 - accuracy: 0.8250 - val_loss: 0.4736 - val_accuracy: 0.7000\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2477 - accuracy: 0.8204 - val_loss: 0.5214 - val_accuracy: 0.6861\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2398 - accuracy: 0.8204 - val_loss: 0.5771 - val_accuracy: 0.6639\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2362 - accuracy: 0.8222 - val_loss: 0.5230 - val_accuracy: 0.6833\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2313 - accuracy: 0.8227 - val_loss: 0.5244 - val_accuracy: 0.6750\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2319 - accuracy: 0.8208 - val_loss: 0.4320 - val_accuracy: 0.7278\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2249 - accuracy: 0.8227 - val_loss: 0.4529 - val_accuracy: 0.7194\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2215 - accuracy: 0.8222 - val_loss: 0.5143 - val_accuracy: 0.6833\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2122 - accuracy: 0.8250 - val_loss: 0.5458 - val_accuracy: 0.6667\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2112 - accuracy: 0.8236 - val_loss: 0.4197 - val_accuracy: 0.7222\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2016 - accuracy: 0.8301 - val_loss: 0.4162 - val_accuracy: 0.7111\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1992 - accuracy: 0.8269 - val_loss: 0.4748 - val_accuracy: 0.7028\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1984 - accuracy: 0.8287 - val_loss: 0.4609 - val_accuracy: 0.7056\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2015 - accuracy: 0.8245 - val_loss: 0.4102 - val_accuracy: 0.7139\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1924 - accuracy: 0.8278 - val_loss: 0.5627 - val_accuracy: 0.6472\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2044 - accuracy: 0.8259 - val_loss: 0.5662 - val_accuracy: 0.6528\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1979 - accuracy: 0.8245 - val_loss: 0.4622 - val_accuracy: 0.6917\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1911 - accuracy: 0.8269 - val_loss: 0.4276 - val_accuracy: 0.7000\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1869 - accuracy: 0.8292 - val_loss: 0.4426 - val_accuracy: 0.7028\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1804 - accuracy: 0.8306 - val_loss: 0.4860 - val_accuracy: 0.6778\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1841 - accuracy: 0.8282 - val_loss: 0.4681 - val_accuracy: 0.6917\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.1832 - accuracy: 0.8264 - val_loss: 0.5021 - val_accuracy: 0.6639\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1833 - accuracy: 0.8255 - val_loss: 0.4346 - val_accuracy: 0.6972\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1982 - accuracy: 0.8204 - val_loss: 0.4649 - val_accuracy: 0.6806\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1855 - accuracy: 0.8269 - val_loss: 0.4858 - val_accuracy: 0.6639\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1755 - accuracy: 0.8278 - val_loss: 0.4610 - val_accuracy: 0.6917\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1749 - accuracy: 0.8269 - val_loss: 0.4746 - val_accuracy: 0.6944\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1772 - accuracy: 0.8278 - val_loss: 0.4635 - val_accuracy: 0.6917\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.1693 - accuracy: 0.8292 - val_loss: 0.4356 - val_accuracy: 0.7000\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1819 - accuracy: 0.8227 - val_loss: 0.4830 - val_accuracy: 0.6806\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1702 - accuracy: 0.8278 - val_loss: 0.4810 - val_accuracy: 0.6889\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1711 - accuracy: 0.8278 - val_loss: 0.5101 - val_accuracy: 0.6750\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1635 - accuracy: 0.8292 - val_loss: 0.4008 - val_accuracy: 0.7139\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1581 - accuracy: 0.8306 - val_loss: 0.4442 - val_accuracy: 0.7028\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1595 - accuracy: 0.8296 - val_loss: 0.5065 - val_accuracy: 0.6694\n",
            "Epoch 74/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1647 - accuracy: 0.8264 - val_loss: 0.5155 - val_accuracy: 0.6528\n",
            "Epoch 75/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1559 - accuracy: 0.8315 - val_loss: 0.4611 - val_accuracy: 0.6750\n",
            "Epoch 76/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1807 - accuracy: 0.8181 - val_loss: 0.6002 - val_accuracy: 0.6278\n",
            "Epoch 77/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1785 - accuracy: 0.8222 - val_loss: 0.4652 - val_accuracy: 0.6917\n",
            "Epoch 78/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1569 - accuracy: 0.8301 - val_loss: 0.5458 - val_accuracy: 0.6611\n",
            "Epoch 79/200\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.8306Restoring model weights from the end of the best epoch: 44.\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1517 - accuracy: 0.8306 - val_loss: 0.6056 - val_accuracy: 0.6361\n",
            "Epoch 79: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_hy_lstm_sub1.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history_class_mod = model_hy_lstm_sub1.fit([X_train_pres,X_train[:,:,features]], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=([X_val_pres,X_val[:,:,features]], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting on validation set"
      ],
      "metadata": {
        "id": "9RP9N46v-cQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn2N5OMoKfP6",
        "outputId": "e314204c-5731-499a-ffc0-59bdf741aff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 2s 17ms/step\n",
            "12/12 [==============================] - 0s 11ms/step\n",
            "Accuracy validation: 0.7277777777777777\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_hy_lstm_sub1.predict([X_test_pres,X_test[:,:,features]]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_hy_lstm_sub1.predict([X_val_pres,X_val[:,:,features]]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhH7k5aLNw5z"
      },
      "source": [
        "## Hybrid-GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j_FZ3asNw50"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAmgh0KVl97m"
      },
      "outputs": [],
      "source": [
        "#Building the Hybrid-GRU Model for \"Sub1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgD_ElEvNw51"
      },
      "outputs": [],
      "source": [
        "class fuse(Model):\n",
        "  def __init__(self):\n",
        "    super(fuse, self).__init__()\n",
        "    self.cnn = tf.keras.Sequential([\n",
        "      layers.Input(shape=(128, 48,1)),\n",
        "      layers.Conv2D(128, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,1)),\n",
        "      layers.Conv2D(64, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Conv2D(32, (3, 3), activation=None, padding='same', strides=1),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Flatten()])\n",
        "\n",
        "\n",
        "\n",
        "    self.rnn =  tf.keras.Sequential([\n",
        "            layers.GRU(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=len(features), return_sequences=False,dropout=0.5),\n",
        "    ])\n",
        "\n",
        "    self.calssifier = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "\n",
        "    self.fc_cnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "    self.fc_rnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "\n",
        "  def call(self,x):\n",
        "    out0 = self.cnn(x[0])\n",
        "    out0 = self.fc_cnn(out0)\n",
        "    out1 = self.rnn(x[1])\n",
        "    out1 = self.fc_rnn(out1)\n",
        "\n",
        "    out = self.calssifier(layers.concatenate([out0,out1]))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5l7Y-TfNw51"
      },
      "outputs": [],
      "source": [
        "model_hy_gru_sub1=fuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJy4B5llNw51",
        "outputId": "dce7d47b-63f4-4f4a-9ae4-f3fe3dfc6dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_65\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 64, 24, 128)       1280      \n",
            "                                                                 \n",
            " activation_133 (Activation)  (None, 64, 24, 128)      0         \n",
            "                                                                 \n",
            " average_pooling2d_9 (Averag  (None, 32, 24, 128)      0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 16, 12, 64)        73792     \n",
            "                                                                 \n",
            " activation_134 (Activation)  (None, 16, 12, 64)       0         \n",
            "                                                                 \n",
            " average_pooling2d_10 (Avera  (None, 8, 6, 64)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 8, 6, 32)          18464     \n",
            "                                                                 \n",
            " activation_135 (Activation)  (None, 8, 6, 32)         0         \n",
            "                                                                 \n",
            " average_pooling2d_11 (Avera  (None, 4, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 384)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,536\n",
            "Trainable params: 93,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_hy_gru_sub1.cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0Aq9e6HmHGA"
      },
      "outputs": [],
      "source": [
        "#Training the Hybrid-GRU Model on \"Sub1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2zMHzsxNw51",
        "outputId": "f71ae880-c820-462b-bd67-157c58bcd122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 10s 62ms/step - loss: 1.7087 - accuracy: 0.1787 - val_loss: 1.5289 - val_accuracy: 0.1167\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.5732 - accuracy: 0.2282 - val_loss: 1.5487 - val_accuracy: 0.1583\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.4628 - accuracy: 0.2560 - val_loss: 1.5406 - val_accuracy: 0.1833\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.3850 - accuracy: 0.2972 - val_loss: 1.4645 - val_accuracy: 0.2444\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.3072 - accuracy: 0.3653 - val_loss: 1.4284 - val_accuracy: 0.2611\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.2347 - accuracy: 0.4125 - val_loss: 1.3315 - val_accuracy: 0.3611\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.1564 - accuracy: 0.4611 - val_loss: 1.3077 - val_accuracy: 0.2972\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 1.0873 - accuracy: 0.5037 - val_loss: 1.2273 - val_accuracy: 0.2972\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.0397 - accuracy: 0.5204 - val_loss: 1.1289 - val_accuracy: 0.3944\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.9812 - accuracy: 0.5593 - val_loss: 1.0183 - val_accuracy: 0.4528\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.9206 - accuracy: 0.6037 - val_loss: 0.9534 - val_accuracy: 0.5222\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.8909 - accuracy: 0.6111 - val_loss: 0.8851 - val_accuracy: 0.5278\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.8332 - accuracy: 0.6352 - val_loss: 0.8811 - val_accuracy: 0.5278\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.7997 - accuracy: 0.6602 - val_loss: 0.8378 - val_accuracy: 0.5694\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.7663 - accuracy: 0.6606 - val_loss: 0.7974 - val_accuracy: 0.5611\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.7199 - accuracy: 0.6796 - val_loss: 0.7765 - val_accuracy: 0.5944\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6938 - accuracy: 0.6940 - val_loss: 0.7797 - val_accuracy: 0.5694\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.6728 - accuracy: 0.6944 - val_loss: 0.7839 - val_accuracy: 0.5556\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6428 - accuracy: 0.6958 - val_loss: 0.7507 - val_accuracy: 0.5722\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.6044 - accuracy: 0.7167 - val_loss: 0.7603 - val_accuracy: 0.5917\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5788 - accuracy: 0.7222 - val_loss: 0.7270 - val_accuracy: 0.5833\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.5689 - accuracy: 0.7176 - val_loss: 0.7356 - val_accuracy: 0.5778\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.5560 - accuracy: 0.7157 - val_loss: 0.7245 - val_accuracy: 0.5917\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5440 - accuracy: 0.7282 - val_loss: 0.7388 - val_accuracy: 0.6056\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5391 - accuracy: 0.7185 - val_loss: 0.7277 - val_accuracy: 0.5639\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5065 - accuracy: 0.7394 - val_loss: 0.6805 - val_accuracy: 0.5806\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.4931 - accuracy: 0.7347 - val_loss: 0.7182 - val_accuracy: 0.5917\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.4766 - accuracy: 0.7486 - val_loss: 0.7274 - val_accuracy: 0.5778\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4677 - accuracy: 0.7468 - val_loss: 0.6984 - val_accuracy: 0.5861\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4579 - accuracy: 0.7500 - val_loss: 0.7051 - val_accuracy: 0.5833\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.4572 - accuracy: 0.7426 - val_loss: 0.6958 - val_accuracy: 0.5778\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.4460 - accuracy: 0.7486 - val_loss: 0.6802 - val_accuracy: 0.5972\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.4188 - accuracy: 0.7676 - val_loss: 0.6432 - val_accuracy: 0.6194\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4206 - accuracy: 0.7606 - val_loss: 0.6781 - val_accuracy: 0.6000\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.4079 - accuracy: 0.7676 - val_loss: 0.6745 - val_accuracy: 0.5889\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.4106 - accuracy: 0.7588 - val_loss: 0.6451 - val_accuracy: 0.6028\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.4052 - accuracy: 0.7644 - val_loss: 0.5862 - val_accuracy: 0.6389\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3925 - accuracy: 0.7616 - val_loss: 0.6187 - val_accuracy: 0.6028\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3762 - accuracy: 0.7773 - val_loss: 0.6210 - val_accuracy: 0.6111\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3778 - accuracy: 0.7741 - val_loss: 0.6509 - val_accuracy: 0.6139\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3637 - accuracy: 0.7796 - val_loss: 0.6664 - val_accuracy: 0.5972\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3636 - accuracy: 0.7782 - val_loss: 0.6476 - val_accuracy: 0.5972\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3494 - accuracy: 0.7847 - val_loss: 0.6440 - val_accuracy: 0.6167\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3567 - accuracy: 0.7806 - val_loss: 0.6058 - val_accuracy: 0.6306\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3404 - accuracy: 0.7912 - val_loss: 0.6507 - val_accuracy: 0.6083\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3396 - accuracy: 0.7815 - val_loss: 0.5982 - val_accuracy: 0.6222\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3228 - accuracy: 0.7968 - val_loss: 0.6273 - val_accuracy: 0.6028\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3262 - accuracy: 0.7917 - val_loss: 0.6356 - val_accuracy: 0.6000\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3135 - accuracy: 0.8037 - val_loss: 0.6273 - val_accuracy: 0.6000\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3099 - accuracy: 0.7968 - val_loss: 0.5876 - val_accuracy: 0.6361\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3103 - accuracy: 0.7995 - val_loss: 0.6063 - val_accuracy: 0.6222\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2932 - accuracy: 0.8046 - val_loss: 0.6015 - val_accuracy: 0.6250\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2929 - accuracy: 0.8083 - val_loss: 0.6199 - val_accuracy: 0.6222\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2899 - accuracy: 0.8093 - val_loss: 0.6297 - val_accuracy: 0.6306\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2890 - accuracy: 0.8051 - val_loss: 0.5866 - val_accuracy: 0.6361\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2797 - accuracy: 0.8042 - val_loss: 0.6560 - val_accuracy: 0.6111\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2754 - accuracy: 0.8083 - val_loss: 0.6232 - val_accuracy: 0.6278\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2654 - accuracy: 0.8102 - val_loss: 0.6474 - val_accuracy: 0.6028\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2710 - accuracy: 0.8079 - val_loss: 0.6565 - val_accuracy: 0.6167\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2626 - accuracy: 0.8088 - val_loss: 0.5995 - val_accuracy: 0.6444\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2593 - accuracy: 0.8139 - val_loss: 0.6215 - val_accuracy: 0.6167\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2561 - accuracy: 0.8102 - val_loss: 0.6907 - val_accuracy: 0.5861\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2625 - accuracy: 0.8125 - val_loss: 0.5667 - val_accuracy: 0.6528\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2540 - accuracy: 0.8111 - val_loss: 0.5654 - val_accuracy: 0.6250\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2474 - accuracy: 0.8134 - val_loss: 0.6074 - val_accuracy: 0.6139\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2499 - accuracy: 0.8116 - val_loss: 0.5893 - val_accuracy: 0.6194\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2418 - accuracy: 0.8162 - val_loss: 0.6632 - val_accuracy: 0.6056\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2451 - accuracy: 0.8153 - val_loss: 0.6521 - val_accuracy: 0.6250\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2388 - accuracy: 0.8171 - val_loss: 0.7632 - val_accuracy: 0.5694\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2450 - accuracy: 0.8074 - val_loss: 0.6281 - val_accuracy: 0.6222\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2406 - accuracy: 0.8139 - val_loss: 0.6434 - val_accuracy: 0.6083\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2336 - accuracy: 0.8162 - val_loss: 0.6392 - val_accuracy: 0.6000\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2383 - accuracy: 0.8167 - val_loss: 0.6095 - val_accuracy: 0.6222\n",
            "Epoch 74/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2284 - accuracy: 0.8176 - val_loss: 0.7010 - val_accuracy: 0.5722\n",
            "Epoch 75/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2287 - accuracy: 0.8199 - val_loss: 0.6424 - val_accuracy: 0.6083\n",
            "Epoch 76/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2313 - accuracy: 0.8148 - val_loss: 0.6676 - val_accuracy: 0.6000\n",
            "Epoch 77/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2270 - accuracy: 0.8162 - val_loss: 0.6591 - val_accuracy: 0.6028\n",
            "Epoch 78/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2294 - accuracy: 0.8185 - val_loss: 0.5880 - val_accuracy: 0.6250\n",
            "Epoch 79/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2174 - accuracy: 0.8245 - val_loss: 0.7040 - val_accuracy: 0.5806\n",
            "Epoch 80/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2257 - accuracy: 0.8236 - val_loss: 0.6457 - val_accuracy: 0.6111\n",
            "Epoch 81/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2265 - accuracy: 0.8162 - val_loss: 0.6407 - val_accuracy: 0.6222\n",
            "Epoch 82/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2186 - accuracy: 0.8222 - val_loss: 0.6407 - val_accuracy: 0.6139\n",
            "Epoch 83/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2226 - accuracy: 0.8236 - val_loss: 0.6493 - val_accuracy: 0.5861\n",
            "Epoch 84/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2232 - accuracy: 0.8241 - val_loss: 0.6598 - val_accuracy: 0.6111\n",
            "Epoch 85/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2112 - accuracy: 0.8250 - val_loss: 0.6137 - val_accuracy: 0.6222\n",
            "Epoch 86/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2210 - accuracy: 0.8204 - val_loss: 0.5761 - val_accuracy: 0.6361\n",
            "Epoch 87/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2282 - accuracy: 0.8208 - val_loss: 0.6134 - val_accuracy: 0.6222\n",
            "Epoch 88/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2131 - accuracy: 0.8259 - val_loss: 0.6362 - val_accuracy: 0.6194\n",
            "Epoch 89/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2165 - accuracy: 0.8231 - val_loss: 0.6458 - val_accuracy: 0.6250\n",
            "Epoch 90/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2164 - accuracy: 0.8250 - val_loss: 0.6630 - val_accuracy: 0.5889\n",
            "Epoch 91/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2170 - accuracy: 0.8218 - val_loss: 0.6716 - val_accuracy: 0.6056\n",
            "Epoch 92/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2054 - accuracy: 0.8282 - val_loss: 0.6756 - val_accuracy: 0.5944\n",
            "Epoch 93/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2175 - accuracy: 0.8250 - val_loss: 0.6155 - val_accuracy: 0.6278\n",
            "Epoch 94/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2153 - accuracy: 0.8245 - val_loss: 0.6524 - val_accuracy: 0.6083\n",
            "Epoch 95/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2119 - accuracy: 0.8241 - val_loss: 0.5920 - val_accuracy: 0.6361\n",
            "Epoch 96/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2082 - accuracy: 0.8222 - val_loss: 0.6206 - val_accuracy: 0.6083\n",
            "Epoch 97/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2051 - accuracy: 0.8245 - val_loss: 0.6189 - val_accuracy: 0.6194\n",
            "Epoch 98/200\n",
            "70/72 [============================>.] - ETA: 0s - loss: 0.2174 - accuracy: 0.8262Restoring model weights from the end of the best epoch: 63.\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2162 - accuracy: 0.8259 - val_loss: 0.6170 - val_accuracy: 0.6167\n",
            "Epoch 98: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_hy_gru_sub1.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history_class_mod = model_hy_gru_sub1.fit([X_train_pres,X_train[:,:,features]], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=([X_val_pres,X_val[:,:,features]], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting on validation set"
      ],
      "metadata": {
        "id": "H5pEBgEY-efv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdmiXA69Nw51",
        "outputId": "493e53d2-f1ec-45e3-83b2-801b1fc624b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 1s 9ms/step\n",
            "12/12 [==============================] - 0s 11ms/step\n",
            "Accuracy validation: 0.6527777777777778\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_hy_gru_sub1.predict([X_test_pres,X_test[:,:,features]]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_hy_gru_sub1.predict([X_val_pres,X_val[:,:,features]]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mqp5SbqqKpuy"
      },
      "source": [
        "# Left & Right\n",
        "\n",
        "* LSTM\n",
        "* GRU\n",
        "* Hybrid pressuser+kinect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5inC9dPZTzx"
      },
      "outputs": [],
      "source": [
        "features = joints_left_right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ly0tMaeVj3S"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIX1MXm2Vj3T"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yb_zbHKr2fF"
      },
      "outputs": [],
      "source": [
        "#building the LSTM RNN for \"Left & Right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbteJgXFVj3T"
      },
      "outputs": [],
      "source": [
        "class classifier_lstm(Model):\n",
        "  def __init__(self):\n",
        "    super(classifier_lstm, self).__init__()\n",
        "    self.input_ = len(features)\n",
        "    self.rec =  tf.keras.Sequential([\n",
        "            layers.LSTM(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=self.input_, return_sequences=False,dropout=0.5),\n",
        "          ])\n",
        "\n",
        "    self.ffnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "  def call(self,x):\n",
        "    out = self.rec(x)\n",
        "    clas = self.ffnn(out)\n",
        "    return clas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihaw8665Vj3T"
      },
      "outputs": [],
      "source": [
        "model_lstm_ler = classifier_lstm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi4-B8QKVj3T",
        "outputId": "a6ab7ebc-787d-4430-fc98-85f64bdb2d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_70\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_80 (LSTM)              (None, None, 256)         343040    \n",
            "                                                                 \n",
            " lstm_81 (LSTM)              (None, None, 256)         525312    \n",
            "                                                                 \n",
            " lstm_82 (LSTM)              (None, None, 128)         197120    \n",
            "                                                                 \n",
            " lstm_83 (LSTM)              (None, 128)               131584    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,197,056\n",
            "Trainable params: 1,197,056\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_ler.rec.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVcCIja4Vj3T",
        "outputId": "ea368635-af70-4bfb-d413-f5349caf228c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_71\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_161 (Dense)           (2, 128)                  16384     \n",
            "                                                                 \n",
            " batch_normalization_132 (Ba  (2, 128)                 512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_144 (Activation)  (2, 128)                 0         \n",
            "                                                                 \n",
            " dense_162 (Dense)           (2, 64)                   8192      \n",
            "                                                                 \n",
            " batch_normalization_133 (Ba  (2, 64)                  256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_145 (Activation)  (2, 64)                  0         \n",
            "                                                                 \n",
            " dense_163 (Dense)           (2, 32)                   2048      \n",
            "                                                                 \n",
            " batch_normalization_134 (Ba  (2, 32)                  128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_146 (Activation)  (2, 32)                  0         \n",
            "                                                                 \n",
            " dense_164 (Dense)           (2, 16)                   512       \n",
            "                                                                 \n",
            " batch_normalization_135 (Ba  (2, 16)                  64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_147 (Activation)  (2, 16)                  0         \n",
            "                                                                 \n",
            " dense_165 (Dense)           (2, 6)                    102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,198\n",
            "Trainable params: 27,718\n",
            "Non-trainable params: 480\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_ler(X_train[1:3,:,features])\n",
        "model_lstm_ler.ffnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8vG40oFsN-q"
      },
      "outputs": [],
      "source": [
        "#Training the LSTM RNN on \"Left & Right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgNY9b9pVj3T",
        "outputId": "ee5961a8-08f3-4640-acf5-be3df44c731b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 11s 46ms/step - loss: 1.7283 - accuracy: 0.1866 - val_loss: 1.4838 - val_accuracy: 0.1667\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.4701 - accuracy: 0.2542 - val_loss: 1.4358 - val_accuracy: 0.1694\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 1.2794 - accuracy: 0.3398 - val_loss: 1.3281 - val_accuracy: 0.2333\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.1261 - accuracy: 0.4181 - val_loss: 1.0927 - val_accuracy: 0.4472\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.0184 - accuracy: 0.4819 - val_loss: 0.9393 - val_accuracy: 0.4917\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.9476 - accuracy: 0.5481 - val_loss: 0.7946 - val_accuracy: 0.5889\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.8620 - accuracy: 0.6389 - val_loss: 0.7494 - val_accuracy: 0.6111\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.7968 - accuracy: 0.6773 - val_loss: 0.7397 - val_accuracy: 0.6306\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.7342 - accuracy: 0.7278 - val_loss: 0.6463 - val_accuracy: 0.6528\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.6744 - accuracy: 0.7662 - val_loss: 0.6325 - val_accuracy: 0.7111\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.6206 - accuracy: 0.8171 - val_loss: 0.5592 - val_accuracy: 0.7833\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5760 - accuracy: 0.8403 - val_loss: 0.6806 - val_accuracy: 0.6944\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5321 - accuracy: 0.8644 - val_loss: 0.5829 - val_accuracy: 0.8306\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.4995 - accuracy: 0.8806 - val_loss: 0.5129 - val_accuracy: 0.8500\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.4546 - accuracy: 0.9028 - val_loss: 0.4806 - val_accuracy: 0.8833\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.4221 - accuracy: 0.9231 - val_loss: 0.5235 - val_accuracy: 0.8556\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.3855 - accuracy: 0.9315 - val_loss: 0.4709 - val_accuracy: 0.8611\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.3569 - accuracy: 0.9380 - val_loss: 0.4887 - val_accuracy: 0.8528\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3412 - accuracy: 0.9588 - val_loss: 0.4595 - val_accuracy: 0.8639\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3109 - accuracy: 0.9588 - val_loss: 0.4771 - val_accuracy: 0.8417\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2916 - accuracy: 0.9648 - val_loss: 0.4149 - val_accuracy: 0.8778\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2685 - accuracy: 0.9639 - val_loss: 0.3970 - val_accuracy: 0.8778\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2440 - accuracy: 0.9718 - val_loss: 0.3976 - val_accuracy: 0.8972\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2381 - accuracy: 0.9671 - val_loss: 0.5239 - val_accuracy: 0.8278\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2198 - accuracy: 0.9722 - val_loss: 0.3623 - val_accuracy: 0.9083\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2002 - accuracy: 0.9796 - val_loss: 0.4976 - val_accuracy: 0.8444\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1975 - accuracy: 0.9741 - val_loss: 0.4051 - val_accuracy: 0.8556\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1925 - accuracy: 0.9708 - val_loss: 0.5128 - val_accuracy: 0.8278\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2087 - accuracy: 0.9634 - val_loss: 0.5577 - val_accuracy: 0.8056\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1631 - accuracy: 0.9829 - val_loss: 0.4743 - val_accuracy: 0.8778\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1559 - accuracy: 0.9843 - val_loss: 0.4233 - val_accuracy: 0.8583\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1676 - accuracy: 0.9843 - val_loss: 0.5899 - val_accuracy: 0.8000\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1602 - accuracy: 0.9787 - val_loss: 0.3644 - val_accuracy: 0.9056\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1621 - accuracy: 0.9778 - val_loss: 0.6447 - val_accuracy: 0.7944\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1486 - accuracy: 0.9806 - val_loss: 0.4326 - val_accuracy: 0.8611\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1523 - accuracy: 0.9833 - val_loss: 0.3472 - val_accuracy: 0.8889\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1529 - accuracy: 0.9856 - val_loss: 0.4950 - val_accuracy: 0.8556\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1455 - accuracy: 0.9852 - val_loss: 0.3471 - val_accuracy: 0.9028\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1296 - accuracy: 0.9852 - val_loss: 0.6324 - val_accuracy: 0.7917\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1309 - accuracy: 0.9806 - val_loss: 0.5234 - val_accuracy: 0.8361\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1120 - accuracy: 0.9949 - val_loss: 0.4251 - val_accuracy: 0.8611\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1202 - accuracy: 0.9880 - val_loss: 0.6454 - val_accuracy: 0.8139\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1079 - accuracy: 0.9907 - val_loss: 0.4427 - val_accuracy: 0.8556\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1035 - accuracy: 0.9912 - val_loss: 0.4594 - val_accuracy: 0.8694\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0982 - accuracy: 0.9954 - val_loss: 0.6064 - val_accuracy: 0.8222\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0916 - accuracy: 0.9912 - val_loss: 0.7115 - val_accuracy: 0.8028\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0918 - accuracy: 0.9889 - val_loss: 0.5211 - val_accuracy: 0.8444\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0942 - accuracy: 0.9884 - val_loss: 0.4946 - val_accuracy: 0.8278\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1066 - accuracy: 0.9787 - val_loss: 0.4669 - val_accuracy: 0.8556\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.0878 - accuracy: 0.9671 - val_loss: 0.5855 - val_accuracy: 0.8306\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1021 - accuracy: 0.9569 - val_loss: 0.7136 - val_accuracy: 0.7889\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0842 - accuracy: 0.9718 - val_loss: 0.5502 - val_accuracy: 0.8556\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0769 - accuracy: 0.9579 - val_loss: 0.7574 - val_accuracy: 0.8083\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0980 - accuracy: 0.9593 - val_loss: 0.5635 - val_accuracy: 0.8278\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1071 - accuracy: 0.9380 - val_loss: 0.4981 - val_accuracy: 0.8500\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0888 - accuracy: 0.9444 - val_loss: 0.4901 - val_accuracy: 0.8472\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0743 - accuracy: 0.9458 - val_loss: 0.4498 - val_accuracy: 0.8611\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0669 - accuracy: 0.9486 - val_loss: 0.4399 - val_accuracy: 0.8722\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0652 - accuracy: 0.9394 - val_loss: 0.3980 - val_accuracy: 0.8889\n",
            "Epoch 60/200\n",
            "70/72 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9276Restoring model weights from the end of the best epoch: 25.\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0618 - accuracy: 0.9255 - val_loss: 0.7855 - val_accuracy: 0.7750\n",
            "Epoch 60: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_lstm_ler.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "history_class_mod = model_lstm_ler.fit(X_train[:,:,features], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_val[:,:,features], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KTUm9pOsSWa"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM8ItBhvVj3T",
        "outputId": "169b9bca-c848-4d45-83ca-86a677849c0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 1s 9ms/step\n",
            "12/12 [==============================] - 0s 9ms/step\n",
            "Accuracy validation: 0.9083333333333333\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_lstm_ler.predict(X_test[:,:,features]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_lstm_ler.predict(X_val[:,:,features]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting on test set"
      ],
      "metadata": {
        "id": "hkh-x6gF-i5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "L1N8AkS1Vj3T",
        "outputId": "962d6c18-4db6-4173-fdfb-76e4cc1889ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================== Test set accuracy ======================\n",
            "0.9333333333333333\n",
            "================== Test set Confusion Matrix ==================\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"====================== Test set accuracy ======================\")\n",
        "print(acc_test)\n",
        "print(\"================== Test set Confusion Matrix ==================\\n\")\n",
        "print\n",
        "cf_matrix = confusion_matrix(y_true, pred)\n",
        "categories = list(response_malattie.keys())\n",
        "\n",
        "sns.heatmap(cf_matrix, annot=True,\n",
        "            xticklabels = categories,\n",
        "            yticklabels = categories,\n",
        "            cmap=sns.light_palette(\"seagreen\", as_cmap=True))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs9qj8BQObnt"
      },
      "outputs": [],
      "source": [
        "model_lstm_ler.save(\"/content/drive/MyDrive/Progetto_HDA/models/model_lstm_ler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU_S6vqfVj3T"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts1L5VaKr_tX"
      },
      "outputs": [],
      "source": [
        "#building the GRU RNN for \"Left & Right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLDKqitOVj3U"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(3432)\n",
        "np.random.seed(38)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWupR8sEVj3U"
      },
      "outputs": [],
      "source": [
        "class classifier_gru(Model):\n",
        "  def __init__(self):\n",
        "    super(classifier_gru, self).__init__()\n",
        "    self.input_ = len(features)\n",
        "    self.rec =  tf.keras.Sequential([\n",
        "            layers.GRU(256,input_dim=self.input_, return_sequences=True,dropout=0.0),\n",
        "            layers.GRU(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=self.input_, return_sequences=False,dropout=0.5),\n",
        "\n",
        "            ])\n",
        "\n",
        "\n",
        "    self.ffnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "  def call(self,x):\n",
        "    out = self.rec(x)\n",
        "    clas = self.ffnn(out)\n",
        "    return clas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90audDi3Vj3U"
      },
      "outputs": [],
      "source": [
        "model_gru_ler = classifier_gru()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khQvtZsYVj3U",
        "outputId": "98503106-b555-4087-936b-de2259c92665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_28 (GRU)                (None, None, 256)         258048    \n",
            "                                                                 \n",
            " gru_29 (GRU)                (None, None, 256)         394752    \n",
            "                                                                 \n",
            " gru_30 (GRU)                (None, None, 128)         148224    \n",
            "                                                                 \n",
            " gru_31 (GRU)                (None, 128)               99072     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900,096\n",
            "Trainable params: 900,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_gru_ler.rec.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P--ojMf1Vj3U",
        "outputId": "02d1e9e6-34f8-4342-e63f-3de7a2619ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_35 (Dense)            (2, 128)                  16384     \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  (2, 128)                 512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_28 (Activation)  (2, 128)                  0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (2, 64)                   8192      \n",
            "                                                                 \n",
            " batch_normalization_29 (Bat  (2, 64)                  256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_29 (Activation)  (2, 64)                   0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (2, 32)                   2048      \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (2, 32)                  128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_30 (Activation)  (2, 32)                   0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (2, 16)                   512       \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (2, 16)                  64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_31 (Activation)  (2, 16)                   0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (2, 6)                    102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,198\n",
            "Trainable params: 27,718\n",
            "Non-trainable params: 480\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_gru_ler(X_train[1:3,:,features])\n",
        "model_gru_ler.ffnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP8yGZXEscfx"
      },
      "outputs": [],
      "source": [
        "#Training the GRU RNN on \"Left & Right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Po_BAd2Vj3U",
        "outputId": "10fc357d-a4c6-490a-a22e-6708a37c782c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 9s 52ms/step - loss: 1.8804 - accuracy: 0.1648 - val_loss: 1.5136 - val_accuracy: 0.1694\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.7224 - accuracy: 0.1954 - val_loss: 1.4892 - val_accuracy: 0.2528\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.6298 - accuracy: 0.2023 - val_loss: 1.4000 - val_accuracy: 0.2972\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.5144 - accuracy: 0.2500 - val_loss: 1.3194 - val_accuracy: 0.3333\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.4554 - accuracy: 0.2769 - val_loss: 1.2378 - val_accuracy: 0.3139\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.3973 - accuracy: 0.2963 - val_loss: 1.2184 - val_accuracy: 0.3611\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.3250 - accuracy: 0.3181 - val_loss: 1.1195 - val_accuracy: 0.4389\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.2874 - accuracy: 0.3384 - val_loss: 1.0675 - val_accuracy: 0.4861\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.2483 - accuracy: 0.3676 - val_loss: 1.0833 - val_accuracy: 0.4500\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.1844 - accuracy: 0.4167 - val_loss: 0.9893 - val_accuracy: 0.4889\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.1487 - accuracy: 0.4347 - val_loss: 0.9574 - val_accuracy: 0.5694\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.1055 - accuracy: 0.4787 - val_loss: 0.8616 - val_accuracy: 0.5972\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.0635 - accuracy: 0.5056 - val_loss: 0.8280 - val_accuracy: 0.6056\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.0048 - accuracy: 0.5435 - val_loss: 0.7608 - val_accuracy: 0.6972\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.9613 - accuracy: 0.5736 - val_loss: 0.7426 - val_accuracy: 0.7056\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.9036 - accuracy: 0.6037 - val_loss: 0.7011 - val_accuracy: 0.7139\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.8508 - accuracy: 0.6319 - val_loss: 0.6536 - val_accuracy: 0.7444\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.7953 - accuracy: 0.6644 - val_loss: 0.6899 - val_accuracy: 0.7556\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.7558 - accuracy: 0.6676 - val_loss: 0.6321 - val_accuracy: 0.7639\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.7003 - accuracy: 0.7037 - val_loss: 0.5880 - val_accuracy: 0.7861\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.6715 - accuracy: 0.7157 - val_loss: 0.5803 - val_accuracy: 0.7833\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.6367 - accuracy: 0.7412 - val_loss: 0.5419 - val_accuracy: 0.7667\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.5867 - accuracy: 0.7426 - val_loss: 0.4888 - val_accuracy: 0.7444\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.5476 - accuracy: 0.7806 - val_loss: 0.5223 - val_accuracy: 0.7972\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5145 - accuracy: 0.8083 - val_loss: 0.4205 - val_accuracy: 0.8139\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.4701 - accuracy: 0.8269 - val_loss: 0.4697 - val_accuracy: 0.8222\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.4433 - accuracy: 0.8384 - val_loss: 0.3711 - val_accuracy: 0.8333\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.4072 - accuracy: 0.8708 - val_loss: 0.3885 - val_accuracy: 0.7861\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.3790 - accuracy: 0.8745 - val_loss: 0.4197 - val_accuracy: 0.8333\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.3458 - accuracy: 0.8954 - val_loss: 0.4062 - val_accuracy: 0.8361\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.3317 - accuracy: 0.9116 - val_loss: 0.3637 - val_accuracy: 0.8361\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3098 - accuracy: 0.9167 - val_loss: 0.2924 - val_accuracy: 0.8500\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 3s 41ms/step - loss: 0.2882 - accuracy: 0.9204 - val_loss: 0.3208 - val_accuracy: 0.8556\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2778 - accuracy: 0.9319 - val_loss: 0.3188 - val_accuracy: 0.8361\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2498 - accuracy: 0.9394 - val_loss: 0.2931 - val_accuracy: 0.8583\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2311 - accuracy: 0.9361 - val_loss: 0.2704 - val_accuracy: 0.8861\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2320 - accuracy: 0.9343 - val_loss: 0.2248 - val_accuracy: 0.9000\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2069 - accuracy: 0.9315 - val_loss: 0.2462 - val_accuracy: 0.8333\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2102 - accuracy: 0.9162 - val_loss: 0.3112 - val_accuracy: 0.8333\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2027 - accuracy: 0.9134 - val_loss: 0.2262 - val_accuracy: 0.8500\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1772 - accuracy: 0.9273 - val_loss: 0.2292 - val_accuracy: 0.8806\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1763 - accuracy: 0.9171 - val_loss: 0.1972 - val_accuracy: 0.8750\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1763 - accuracy: 0.9231 - val_loss: 0.2116 - val_accuracy: 0.8611\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1683 - accuracy: 0.9375 - val_loss: 0.2162 - val_accuracy: 0.8722\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1556 - accuracy: 0.9333 - val_loss: 0.1926 - val_accuracy: 0.8500\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1471 - accuracy: 0.9481 - val_loss: 0.2507 - val_accuracy: 0.8389\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1484 - accuracy: 0.9412 - val_loss: 0.1809 - val_accuracy: 0.8806\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1432 - accuracy: 0.9384 - val_loss: 0.2042 - val_accuracy: 0.8583\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1389 - accuracy: 0.9412 - val_loss: 0.2084 - val_accuracy: 0.8250\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1411 - accuracy: 0.9449 - val_loss: 0.2661 - val_accuracy: 0.8111\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1265 - accuracy: 0.9449 - val_loss: 0.1662 - val_accuracy: 0.8750\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1192 - accuracy: 0.9500 - val_loss: 0.1929 - val_accuracy: 0.8694\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1253 - accuracy: 0.9444 - val_loss: 0.1489 - val_accuracy: 0.8944\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1134 - accuracy: 0.9301 - val_loss: 0.2838 - val_accuracy: 0.8083\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1149 - accuracy: 0.9181 - val_loss: 0.1768 - val_accuracy: 0.8417\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1108 - accuracy: 0.8981 - val_loss: 0.1502 - val_accuracy: 0.8556\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1153 - accuracy: 0.8963 - val_loss: 0.2812 - val_accuracy: 0.7806\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1110 - accuracy: 0.8912 - val_loss: 0.2300 - val_accuracy: 0.8111\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1103 - accuracy: 0.8778 - val_loss: 0.1961 - val_accuracy: 0.8278\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1072 - accuracy: 0.8796 - val_loss: 0.1768 - val_accuracy: 0.7972\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0967 - accuracy: 0.8676 - val_loss: 0.3300 - val_accuracy: 0.7444\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0901 - accuracy: 0.8773 - val_loss: 0.2249 - val_accuracy: 0.8000\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0950 - accuracy: 0.8630 - val_loss: 0.1814 - val_accuracy: 0.7917\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0859 - accuracy: 0.8644 - val_loss: 0.1143 - val_accuracy: 0.8194\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0867 - accuracy: 0.8593 - val_loss: 0.2895 - val_accuracy: 0.7528\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0819 - accuracy: 0.8495 - val_loss: 0.3269 - val_accuracy: 0.7333\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0830 - accuracy: 0.8495 - val_loss: 0.2449 - val_accuracy: 0.7667\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0786 - accuracy: 0.8495 - val_loss: 0.1465 - val_accuracy: 0.8056\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0796 - accuracy: 0.8486 - val_loss: 0.2930 - val_accuracy: 0.7667\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0870 - accuracy: 0.8449 - val_loss: 0.2214 - val_accuracy: 0.7889\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0725 - accuracy: 0.8394 - val_loss: 0.2510 - val_accuracy: 0.7861\n",
            "Epoch 72/200\n",
            "70/72 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.8381Restoring model weights from the end of the best epoch: 37.\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0756 - accuracy: 0.8398 - val_loss: 0.2234 - val_accuracy: 0.7778\n",
            "Epoch 72: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_gru_ler.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history_class_mod = model_gru_ler.fit(X_train[:,:,features], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_val[:,:,features], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICAPEh-Vsgye"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5osCYwwVj3U",
        "outputId": "e19f01cc-311c-4c00-d601-de1abf921e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 1s 15ms/step\n",
            "12/12 [==============================] - 0s 11ms/step\n",
            "Accuracy validation: 0.9\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_gru_ler.predict(X_test[:,:,features]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_gru_ler.predict(X_val[:,:,features]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Darp1HWZVj3U"
      },
      "source": [
        "## Hybrid-LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LBOQjVTsD4i"
      },
      "outputs": [],
      "source": [
        "#building the Hybrid-LSTM RNN CNN for \"Left & Right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XaZuUDGVj3U"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjJHOvrNVj3U"
      },
      "outputs": [],
      "source": [
        "class fuse(Model):\n",
        "  def __init__(self):\n",
        "    super(fuse, self).__init__()\n",
        "    self.cnn = tf.keras.Sequential([\n",
        "      layers.Input(shape=(128, 48,1)),\n",
        "      layers.Conv2D(128, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,1)),\n",
        "      layers.Conv2D(64, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Conv2D(32, (3, 3), activation=None, padding='same', strides=1),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Flatten()])\n",
        "\n",
        "\n",
        "\n",
        "    self.rnn =  tf.keras.Sequential([\n",
        "            layers.LSTM(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=len(features), return_sequences=False,dropout=0.5),\n",
        "\n",
        "    ])\n",
        "\n",
        "    self.calssifier = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "\n",
        "    self.fc_cnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "    self.fc_rnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "\n",
        "  def call(self,x):\n",
        "    out0 = self.cnn(x[0])\n",
        "    out0 = self.fc_cnn(out0)\n",
        "    out1 = self.rnn(x[1])\n",
        "    out1 = self.fc_rnn(out1)\n",
        "\n",
        "    out = self.calssifier(layers.concatenate([out0,out1]))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_bWhvdMVj3V"
      },
      "outputs": [],
      "source": [
        "model_hy_lstm_ler=fuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGeiMWMwVj3V",
        "outputId": "32aa914d-1619-490c-dd66-f77267de50a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_74\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 64, 24, 128)       1280      \n",
            "                                                                 \n",
            " activation_152 (Activation)  (None, 64, 24, 128)      0         \n",
            "                                                                 \n",
            " average_pooling2d_12 (Avera  (None, 32, 24, 128)      0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 16, 12, 64)        73792     \n",
            "                                                                 \n",
            " activation_153 (Activation)  (None, 16, 12, 64)       0         \n",
            "                                                                 \n",
            " average_pooling2d_13 (Avera  (None, 8, 6, 64)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 8, 6, 32)          18464     \n",
            "                                                                 \n",
            " activation_154 (Activation)  (None, 8, 6, 32)         0         \n",
            "                                                                 \n",
            " average_pooling2d_14 (Avera  (None, 4, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 384)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,536\n",
            "Trainable params: 93,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_hy_lstm_ler.cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaLZ6AmYsvFu"
      },
      "outputs": [],
      "source": [
        "#Training the Hybrid-LSTM RNN CNN on \"Left & Right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4kRzyceVj3V",
        "outputId": "7b3ec3cd-c994-4275-8403-ad39b0915ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 11s 54ms/step - loss: 1.6239 - accuracy: 0.2208 - val_loss: 1.5497 - val_accuracy: 0.0861\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.4304 - accuracy: 0.3157 - val_loss: 1.5800 - val_accuracy: 0.0417\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.2929 - accuracy: 0.3829 - val_loss: 1.5637 - val_accuracy: 0.0889\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1614 - accuracy: 0.4681 - val_loss: 1.4656 - val_accuracy: 0.2306\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 1.0733 - accuracy: 0.5171 - val_loss: 1.3092 - val_accuracy: 0.4278\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.9858 - accuracy: 0.5690 - val_loss: 1.2135 - val_accuracy: 0.3722\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.8986 - accuracy: 0.6144 - val_loss: 1.0422 - val_accuracy: 0.4667\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.8408 - accuracy: 0.6264 - val_loss: 0.9761 - val_accuracy: 0.5111\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.7808 - accuracy: 0.6662 - val_loss: 0.9155 - val_accuracy: 0.5389\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7107 - accuracy: 0.7014 - val_loss: 0.8311 - val_accuracy: 0.5556\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.6721 - accuracy: 0.7171 - val_loss: 0.8653 - val_accuracy: 0.5389\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.6380 - accuracy: 0.7227 - val_loss: 0.7365 - val_accuracy: 0.5917\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.5954 - accuracy: 0.7403 - val_loss: 0.6523 - val_accuracy: 0.6111\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.5591 - accuracy: 0.7569 - val_loss: 0.7418 - val_accuracy: 0.6056\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.5204 - accuracy: 0.7694 - val_loss: 0.6050 - val_accuracy: 0.6944\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4939 - accuracy: 0.7713 - val_loss: 0.6096 - val_accuracy: 0.6722\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4651 - accuracy: 0.7792 - val_loss: 0.5256 - val_accuracy: 0.7278\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4424 - accuracy: 0.7889 - val_loss: 0.6255 - val_accuracy: 0.6611\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4125 - accuracy: 0.7968 - val_loss: 0.4341 - val_accuracy: 0.7472\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3850 - accuracy: 0.8032 - val_loss: 0.5089 - val_accuracy: 0.7194\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3714 - accuracy: 0.8083 - val_loss: 0.4549 - val_accuracy: 0.7472\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3480 - accuracy: 0.8065 - val_loss: 0.6128 - val_accuracy: 0.6806\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3273 - accuracy: 0.8162 - val_loss: 0.4988 - val_accuracy: 0.7278\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3154 - accuracy: 0.8139 - val_loss: 0.4564 - val_accuracy: 0.7250\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3263 - accuracy: 0.8083 - val_loss: 0.4491 - val_accuracy: 0.7167\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3089 - accuracy: 0.8111 - val_loss: 0.5269 - val_accuracy: 0.6917\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2815 - accuracy: 0.8222 - val_loss: 0.5038 - val_accuracy: 0.7139\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2836 - accuracy: 0.8190 - val_loss: 0.5487 - val_accuracy: 0.6778\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2792 - accuracy: 0.8144 - val_loss: 0.3134 - val_accuracy: 0.7806\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2488 - accuracy: 0.8259 - val_loss: 0.7177 - val_accuracy: 0.6417\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2499 - accuracy: 0.8208 - val_loss: 0.5724 - val_accuracy: 0.6806\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2522 - accuracy: 0.8204 - val_loss: 0.6489 - val_accuracy: 0.6444\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2401 - accuracy: 0.8222 - val_loss: 0.6730 - val_accuracy: 0.6111\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2469 - accuracy: 0.8167 - val_loss: 0.5205 - val_accuracy: 0.7111\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2274 - accuracy: 0.8255 - val_loss: 0.5447 - val_accuracy: 0.6778\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2329 - accuracy: 0.8204 - val_loss: 0.4546 - val_accuracy: 0.7167\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2376 - accuracy: 0.8199 - val_loss: 0.5663 - val_accuracy: 0.6750\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2309 - accuracy: 0.8241 - val_loss: 0.5066 - val_accuracy: 0.6944\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2153 - accuracy: 0.8259 - val_loss: 0.5318 - val_accuracy: 0.6750\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2194 - accuracy: 0.8245 - val_loss: 0.4147 - val_accuracy: 0.7139\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2110 - accuracy: 0.8287 - val_loss: 0.5598 - val_accuracy: 0.6778\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2168 - accuracy: 0.8255 - val_loss: 0.7020 - val_accuracy: 0.6472\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2120 - accuracy: 0.8241 - val_loss: 0.6556 - val_accuracy: 0.6528\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2084 - accuracy: 0.8255 - val_loss: 0.6437 - val_accuracy: 0.6611\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2081 - accuracy: 0.8236 - val_loss: 0.4173 - val_accuracy: 0.7361\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2130 - accuracy: 0.8222 - val_loss: 0.6967 - val_accuracy: 0.6333\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2443 - accuracy: 0.8102 - val_loss: 0.5024 - val_accuracy: 0.6833\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.2013 - accuracy: 0.8245 - val_loss: 0.5261 - val_accuracy: 0.6778\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1869 - accuracy: 0.8306 - val_loss: 0.4461 - val_accuracy: 0.7139\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1858 - accuracy: 0.8282 - val_loss: 0.4028 - val_accuracy: 0.7333\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1820 - accuracy: 0.8310 - val_loss: 0.4557 - val_accuracy: 0.7111\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1819 - accuracy: 0.8296 - val_loss: 0.4664 - val_accuracy: 0.7028\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1759 - accuracy: 0.8315 - val_loss: 0.5107 - val_accuracy: 0.6944\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.1886 - accuracy: 0.8255 - val_loss: 0.4423 - val_accuracy: 0.7000\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1723 - accuracy: 0.8310 - val_loss: 0.4744 - val_accuracy: 0.7056\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1700 - accuracy: 0.8315 - val_loss: 0.4171 - val_accuracy: 0.7167\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1694 - accuracy: 0.8292 - val_loss: 0.3674 - val_accuracy: 0.7444\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1676 - accuracy: 0.8306 - val_loss: 0.3986 - val_accuracy: 0.7389\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1660 - accuracy: 0.8310 - val_loss: 0.3928 - val_accuracy: 0.7194\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1618 - accuracy: 0.8310 - val_loss: 0.3246 - val_accuracy: 0.7639\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.1762 - accuracy: 0.8264 - val_loss: 0.4636 - val_accuracy: 0.7167\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1997 - accuracy: 0.8167 - val_loss: 0.5189 - val_accuracy: 0.7139\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2023 - accuracy: 0.8176 - val_loss: 0.6120 - val_accuracy: 0.6250\n",
            "Epoch 64/200\n",
            "71/72 [============================>.] - ETA: 0s - loss: 0.1635 - accuracy: 0.8277Restoring model weights from the end of the best epoch: 29.\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1637 - accuracy: 0.8278 - val_loss: 0.5023 - val_accuracy: 0.6806\n",
            "Epoch 64: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_hy_lstm_ler.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history_class_mod = model_hy_lstm_ler.fit([X_train_pres,X_train[:,:,features]], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=([X_val_pres,X_val[:,:,features]], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKN6Jx48s2wP"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXXTTPWwVj3V",
        "outputId": "7af02010-4553-41e2-8348-9c4b6af36408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 2s 10ms/step\n",
            "12/12 [==============================] - 0s 11ms/step\n",
            "Accuracy validation: 0.7805555555555556\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_hy_lstm_ler.predict([X_test_pres,X_test[:,:,features]]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_hy_lstm_ler.predict([X_val_pres,X_val[:,:,features]]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYisg9Q2Vj3V"
      },
      "source": [
        "## Hybrid-GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rVqTKzHsI5H"
      },
      "outputs": [],
      "source": [
        "#building the Hybrid-GRU RNN CNN for \"Left & Right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adzi0tuDVj3V"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6Qww9b_Vj3V"
      },
      "outputs": [],
      "source": [
        "class fuse(Model):\n",
        "  def __init__(self):\n",
        "    super(fuse, self).__init__()\n",
        "    self.cnn = tf.keras.Sequential([\n",
        "      layers.Input(shape=(128, 48,1)),\n",
        "      layers.Conv2D(128, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,1)),\n",
        "      layers.Conv2D(64, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Conv2D(32, (3, 3), activation=None, padding='same', strides=1),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Flatten()])\n",
        "\n",
        "\n",
        "\n",
        "    self.rnn =  tf.keras.Sequential([\n",
        "            layers.GRU(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=len(features), return_sequences=False,dropout=0.5),\n",
        "            #layers.Dropout(0.4)\n",
        "    ])\n",
        "\n",
        "    self.calssifier = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "\n",
        "    self.fc_cnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "    self.fc_rnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "\n",
        "  def call(self,x):\n",
        "    out0 = self.cnn(x[0])\n",
        "    out0 = self.fc_cnn(out0)\n",
        "    out1 = self.rnn(x[1])\n",
        "    out1 = self.fc_rnn(out1)\n",
        "\n",
        "    out = self.calssifier(layers.concatenate([out0,out1]))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_BFyAgtVj3V"
      },
      "outputs": [],
      "source": [
        "model_hy_gru_ler=fuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXIhdQGaVj3W",
        "outputId": "30475fda-21d5-4b92-e622-ad59a10f8e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_79\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_15 (Conv2D)          (None, 64, 24, 128)       1280      \n",
            "                                                                 \n",
            " activation_163 (Activation)  (None, 64, 24, 128)      0         \n",
            "                                                                 \n",
            " average_pooling2d_15 (Avera  (None, 32, 24, 128)      0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 16, 12, 64)        73792     \n",
            "                                                                 \n",
            " activation_164 (Activation)  (None, 16, 12, 64)       0         \n",
            "                                                                 \n",
            " average_pooling2d_16 (Avera  (None, 8, 6, 64)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 8, 6, 32)          18464     \n",
            "                                                                 \n",
            " activation_165 (Activation)  (None, 8, 6, 32)         0         \n",
            "                                                                 \n",
            " average_pooling2d_17 (Avera  (None, 4, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 384)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,536\n",
            "Trainable params: 93,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_hy_gru_ler.cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCswHzTvt8r2"
      },
      "outputs": [],
      "source": [
        "#Training the Hybrid-GRU RNN CNN on Left & Right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMFx3VScVj3W",
        "outputId": "b117b2cd-aa17-4c69-9344-f4949b413099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 10s 51ms/step - loss: 1.6949 - accuracy: 0.1806 - val_loss: 1.5256 - val_accuracy: 0.1472\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 1.5548 - accuracy: 0.2440 - val_loss: 1.5106 - val_accuracy: 0.1389\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.4192 - accuracy: 0.2917 - val_loss: 1.5164 - val_accuracy: 0.1806\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.3518 - accuracy: 0.3319 - val_loss: 1.4741 - val_accuracy: 0.2361\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.2820 - accuracy: 0.3847 - val_loss: 1.5042 - val_accuracy: 0.2639\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.2124 - accuracy: 0.4194 - val_loss: 1.5082 - val_accuracy: 0.2917\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.1320 - accuracy: 0.4745 - val_loss: 1.3839 - val_accuracy: 0.3194\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.0566 - accuracy: 0.5079 - val_loss: 1.2266 - val_accuracy: 0.3889\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.9998 - accuracy: 0.5347 - val_loss: 1.1446 - val_accuracy: 0.4028\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.9471 - accuracy: 0.5606 - val_loss: 1.0042 - val_accuracy: 0.4750\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.8839 - accuracy: 0.5995 - val_loss: 0.9601 - val_accuracy: 0.5000\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.8490 - accuracy: 0.6292 - val_loss: 0.9387 - val_accuracy: 0.5306\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.7965 - accuracy: 0.6324 - val_loss: 0.8770 - val_accuracy: 0.5444\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.7563 - accuracy: 0.6500 - val_loss: 0.7774 - val_accuracy: 0.5972\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.7245 - accuracy: 0.6671 - val_loss: 0.8239 - val_accuracy: 0.5778\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.6810 - accuracy: 0.6870 - val_loss: 0.7553 - val_accuracy: 0.6139\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.6449 - accuracy: 0.6986 - val_loss: 0.7852 - val_accuracy: 0.5944\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6373 - accuracy: 0.6898 - val_loss: 0.7206 - val_accuracy: 0.6139\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6061 - accuracy: 0.7083 - val_loss: 0.7284 - val_accuracy: 0.6028\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5752 - accuracy: 0.7199 - val_loss: 0.7430 - val_accuracy: 0.5944\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5628 - accuracy: 0.7185 - val_loss: 0.6840 - val_accuracy: 0.6139\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5237 - accuracy: 0.7356 - val_loss: 0.7236 - val_accuracy: 0.5944\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.5091 - accuracy: 0.7435 - val_loss: 0.7258 - val_accuracy: 0.6028\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4987 - accuracy: 0.7398 - val_loss: 0.6603 - val_accuracy: 0.6417\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.4924 - accuracy: 0.7389 - val_loss: 0.6845 - val_accuracy: 0.6111\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4789 - accuracy: 0.7491 - val_loss: 0.6673 - val_accuracy: 0.6194\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4453 - accuracy: 0.7593 - val_loss: 0.6407 - val_accuracy: 0.6417\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4437 - accuracy: 0.7574 - val_loss: 0.7186 - val_accuracy: 0.6056\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4329 - accuracy: 0.7630 - val_loss: 0.6696 - val_accuracy: 0.6139\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4058 - accuracy: 0.7796 - val_loss: 0.6849 - val_accuracy: 0.6028\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3981 - accuracy: 0.7736 - val_loss: 0.6216 - val_accuracy: 0.6306\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4073 - accuracy: 0.7639 - val_loss: 0.6082 - val_accuracy: 0.6139\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3833 - accuracy: 0.7722 - val_loss: 0.6856 - val_accuracy: 0.5833\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3779 - accuracy: 0.7778 - val_loss: 0.6379 - val_accuracy: 0.6000\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3687 - accuracy: 0.7796 - val_loss: 0.6802 - val_accuracy: 0.5778\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3788 - accuracy: 0.7764 - val_loss: 0.6436 - val_accuracy: 0.6111\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3704 - accuracy: 0.7815 - val_loss: 0.7016 - val_accuracy: 0.5722\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3598 - accuracy: 0.7806 - val_loss: 0.6152 - val_accuracy: 0.6167\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3497 - accuracy: 0.7843 - val_loss: 0.6232 - val_accuracy: 0.6139\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3433 - accuracy: 0.7880 - val_loss: 0.6545 - val_accuracy: 0.6167\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3312 - accuracy: 0.7981 - val_loss: 0.6759 - val_accuracy: 0.5778\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3401 - accuracy: 0.7870 - val_loss: 0.6480 - val_accuracy: 0.6056\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3185 - accuracy: 0.7986 - val_loss: 0.6519 - val_accuracy: 0.6111\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3262 - accuracy: 0.7889 - val_loss: 0.6474 - val_accuracy: 0.6389\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3258 - accuracy: 0.7968 - val_loss: 0.6223 - val_accuracy: 0.6194\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3056 - accuracy: 0.8000 - val_loss: 0.5672 - val_accuracy: 0.6306\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2983 - accuracy: 0.8032 - val_loss: 0.5832 - val_accuracy: 0.6417\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3071 - accuracy: 0.7963 - val_loss: 0.5565 - val_accuracy: 0.6528\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2895 - accuracy: 0.8074 - val_loss: 0.5946 - val_accuracy: 0.6750\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2863 - accuracy: 0.8074 - val_loss: 0.5699 - val_accuracy: 0.6417\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2913 - accuracy: 0.8042 - val_loss: 0.6341 - val_accuracy: 0.6333\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2849 - accuracy: 0.8116 - val_loss: 0.6258 - val_accuracy: 0.6417\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2736 - accuracy: 0.8120 - val_loss: 0.6076 - val_accuracy: 0.6500\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2821 - accuracy: 0.8074 - val_loss: 0.5559 - val_accuracy: 0.6750\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2730 - accuracy: 0.8125 - val_loss: 0.6046 - val_accuracy: 0.6361\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2595 - accuracy: 0.8157 - val_loss: 0.6053 - val_accuracy: 0.6250\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2611 - accuracy: 0.8153 - val_loss: 0.5784 - val_accuracy: 0.6417\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2562 - accuracy: 0.8130 - val_loss: 0.5721 - val_accuracy: 0.6639\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2574 - accuracy: 0.8157 - val_loss: 0.5597 - val_accuracy: 0.6417\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2510 - accuracy: 0.8185 - val_loss: 0.5790 - val_accuracy: 0.6444\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2545 - accuracy: 0.8148 - val_loss: 0.5535 - val_accuracy: 0.6611\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2449 - accuracy: 0.8185 - val_loss: 0.5490 - val_accuracy: 0.6611\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2485 - accuracy: 0.8176 - val_loss: 0.5790 - val_accuracy: 0.6528\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2489 - accuracy: 0.8153 - val_loss: 0.5605 - val_accuracy: 0.6556\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2402 - accuracy: 0.8190 - val_loss: 0.5292 - val_accuracy: 0.6694\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2456 - accuracy: 0.8176 - val_loss: 0.5641 - val_accuracy: 0.6444\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2366 - accuracy: 0.8204 - val_loss: 0.5584 - val_accuracy: 0.6556\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2359 - accuracy: 0.8171 - val_loss: 0.5557 - val_accuracy: 0.6583\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2335 - accuracy: 0.8227 - val_loss: 0.6775 - val_accuracy: 0.5944\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2374 - accuracy: 0.8157 - val_loss: 0.5236 - val_accuracy: 0.6528\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2333 - accuracy: 0.8171 - val_loss: 0.5515 - val_accuracy: 0.6556\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2192 - accuracy: 0.8245 - val_loss: 0.5612 - val_accuracy: 0.6361\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2231 - accuracy: 0.8218 - val_loss: 0.5038 - val_accuracy: 0.6750\n",
            "Epoch 74/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2204 - accuracy: 0.8222 - val_loss: 0.5128 - val_accuracy: 0.6833\n",
            "Epoch 75/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2141 - accuracy: 0.8245 - val_loss: 0.5008 - val_accuracy: 0.6722\n",
            "Epoch 76/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2171 - accuracy: 0.8231 - val_loss: 0.5119 - val_accuracy: 0.6722\n",
            "Epoch 77/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2069 - accuracy: 0.8273 - val_loss: 0.4485 - val_accuracy: 0.7000\n",
            "Epoch 78/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2141 - accuracy: 0.8208 - val_loss: 0.5459 - val_accuracy: 0.6639\n",
            "Epoch 79/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2050 - accuracy: 0.8269 - val_loss: 0.4850 - val_accuracy: 0.6889\n",
            "Epoch 80/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2078 - accuracy: 0.8250 - val_loss: 0.4662 - val_accuracy: 0.6778\n",
            "Epoch 81/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2024 - accuracy: 0.8255 - val_loss: 0.4991 - val_accuracy: 0.6806\n",
            "Epoch 82/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1953 - accuracy: 0.8296 - val_loss: 0.4910 - val_accuracy: 0.6889\n",
            "Epoch 83/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2014 - accuracy: 0.8269 - val_loss: 0.4904 - val_accuracy: 0.6667\n",
            "Epoch 84/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1989 - accuracy: 0.8259 - val_loss: 0.4921 - val_accuracy: 0.6889\n",
            "Epoch 85/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1910 - accuracy: 0.8287 - val_loss: 0.5027 - val_accuracy: 0.6750\n",
            "Epoch 86/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1967 - accuracy: 0.8287 - val_loss: 0.5855 - val_accuracy: 0.6250\n",
            "Epoch 87/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2019 - accuracy: 0.8264 - val_loss: 0.5351 - val_accuracy: 0.6639\n",
            "Epoch 88/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1934 - accuracy: 0.8273 - val_loss: 0.4553 - val_accuracy: 0.6972\n",
            "Epoch 89/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1881 - accuracy: 0.8259 - val_loss: 0.4324 - val_accuracy: 0.7028\n",
            "Epoch 90/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1947 - accuracy: 0.8269 - val_loss: 0.4480 - val_accuracy: 0.7111\n",
            "Epoch 91/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1883 - accuracy: 0.8250 - val_loss: 0.4891 - val_accuracy: 0.6917\n",
            "Epoch 92/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1848 - accuracy: 0.8287 - val_loss: 0.5345 - val_accuracy: 0.6583\n",
            "Epoch 93/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1964 - accuracy: 0.8264 - val_loss: 0.4601 - val_accuracy: 0.7028\n",
            "Epoch 94/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1953 - accuracy: 0.8282 - val_loss: 0.3849 - val_accuracy: 0.7250\n",
            "Epoch 95/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1865 - accuracy: 0.8292 - val_loss: 0.4400 - val_accuracy: 0.7028\n",
            "Epoch 96/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1894 - accuracy: 0.8282 - val_loss: 0.4837 - val_accuracy: 0.6861\n",
            "Epoch 97/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1914 - accuracy: 0.8269 - val_loss: 0.4760 - val_accuracy: 0.6806\n",
            "Epoch 98/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2004 - accuracy: 0.8231 - val_loss: 0.4671 - val_accuracy: 0.6917\n",
            "Epoch 99/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1950 - accuracy: 0.8255 - val_loss: 0.4386 - val_accuracy: 0.7167\n",
            "Epoch 100/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1877 - accuracy: 0.8269 - val_loss: 0.4453 - val_accuracy: 0.6944\n",
            "Epoch 101/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1779 - accuracy: 0.8315 - val_loss: 0.4674 - val_accuracy: 0.6972\n",
            "Epoch 102/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1783 - accuracy: 0.8292 - val_loss: 0.4014 - val_accuracy: 0.7306\n",
            "Epoch 103/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1847 - accuracy: 0.8278 - val_loss: 0.3729 - val_accuracy: 0.7528\n",
            "Epoch 104/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1895 - accuracy: 0.8278 - val_loss: 0.6995 - val_accuracy: 0.5694\n",
            "Epoch 105/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1762 - accuracy: 0.8315 - val_loss: 0.4159 - val_accuracy: 0.7306\n",
            "Epoch 106/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1780 - accuracy: 0.8310 - val_loss: 0.5240 - val_accuracy: 0.6833\n",
            "Epoch 107/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1859 - accuracy: 0.8296 - val_loss: 0.4273 - val_accuracy: 0.7306\n",
            "Epoch 108/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1885 - accuracy: 0.8264 - val_loss: 0.4685 - val_accuracy: 0.6861\n",
            "Epoch 109/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1827 - accuracy: 0.8306 - val_loss: 0.3472 - val_accuracy: 0.7639\n",
            "Epoch 110/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1863 - accuracy: 0.8273 - val_loss: 0.4406 - val_accuracy: 0.7000\n",
            "Epoch 111/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1811 - accuracy: 0.8287 - val_loss: 0.3708 - val_accuracy: 0.7472\n",
            "Epoch 112/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1808 - accuracy: 0.8296 - val_loss: 0.3532 - val_accuracy: 0.7472\n",
            "Epoch 113/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1888 - accuracy: 0.8273 - val_loss: 0.5237 - val_accuracy: 0.6528\n",
            "Epoch 114/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.1801 - accuracy: 0.8306 - val_loss: 0.4421 - val_accuracy: 0.7056\n",
            "Epoch 115/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1795 - accuracy: 0.8296 - val_loss: 0.4164 - val_accuracy: 0.7278\n",
            "Epoch 116/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1796 - accuracy: 0.8319 - val_loss: 0.3774 - val_accuracy: 0.7472\n",
            "Epoch 117/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1880 - accuracy: 0.8282 - val_loss: 0.3922 - val_accuracy: 0.7389\n",
            "Epoch 118/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1850 - accuracy: 0.8306 - val_loss: 0.3336 - val_accuracy: 0.7583\n",
            "Epoch 119/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1872 - accuracy: 0.8315 - val_loss: 0.4049 - val_accuracy: 0.7222\n",
            "Epoch 120/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1850 - accuracy: 0.8315 - val_loss: 0.3647 - val_accuracy: 0.7500\n",
            "Epoch 121/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1858 - accuracy: 0.8296 - val_loss: 0.3254 - val_accuracy: 0.7667\n",
            "Epoch 122/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1923 - accuracy: 0.8296 - val_loss: 0.3309 - val_accuracy: 0.7667\n",
            "Epoch 123/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1833 - accuracy: 0.8310 - val_loss: 0.4018 - val_accuracy: 0.7250\n",
            "Epoch 124/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1854 - accuracy: 0.8315 - val_loss: 0.3311 - val_accuracy: 0.7639\n",
            "Epoch 125/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1819 - accuracy: 0.8292 - val_loss: 0.3195 - val_accuracy: 0.7806\n",
            "Epoch 126/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1868 - accuracy: 0.8278 - val_loss: 0.4650 - val_accuracy: 0.6972\n",
            "Epoch 127/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1910 - accuracy: 0.8301 - val_loss: 0.4140 - val_accuracy: 0.7167\n",
            "Epoch 128/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1863 - accuracy: 0.8306 - val_loss: 0.3942 - val_accuracy: 0.7417\n",
            "Epoch 129/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1819 - accuracy: 0.8296 - val_loss: 0.3513 - val_accuracy: 0.7583\n",
            "Epoch 130/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1797 - accuracy: 0.8319 - val_loss: 0.3554 - val_accuracy: 0.7528\n",
            "Epoch 131/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1811 - accuracy: 0.8319 - val_loss: 0.3329 - val_accuracy: 0.7694\n",
            "Epoch 132/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1812 - accuracy: 0.8301 - val_loss: 0.3545 - val_accuracy: 0.7639\n",
            "Epoch 133/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1876 - accuracy: 0.8301 - val_loss: 0.3894 - val_accuracy: 0.7333\n",
            "Epoch 134/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1842 - accuracy: 0.8292 - val_loss: 0.3999 - val_accuracy: 0.7361\n",
            "Epoch 135/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1787 - accuracy: 0.8310 - val_loss: 0.4821 - val_accuracy: 0.7194\n",
            "Epoch 136/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1709 - accuracy: 0.8324 - val_loss: 0.3767 - val_accuracy: 0.7444\n",
            "Epoch 137/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1833 - accuracy: 0.8306 - val_loss: 0.4759 - val_accuracy: 0.7056\n",
            "Epoch 138/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1827 - accuracy: 0.8315 - val_loss: 0.3915 - val_accuracy: 0.7500\n",
            "Epoch 139/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1833 - accuracy: 0.8306 - val_loss: 0.3688 - val_accuracy: 0.7472\n",
            "Epoch 140/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1836 - accuracy: 0.8310 - val_loss: 0.3852 - val_accuracy: 0.7417\n",
            "Epoch 141/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1816 - accuracy: 0.8324 - val_loss: 0.3408 - val_accuracy: 0.7583\n",
            "Epoch 142/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1755 - accuracy: 0.8315 - val_loss: 0.4288 - val_accuracy: 0.7222\n",
            "Epoch 143/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1809 - accuracy: 0.8310 - val_loss: 0.3063 - val_accuracy: 0.7806\n",
            "Epoch 144/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1816 - accuracy: 0.8310 - val_loss: 0.3485 - val_accuracy: 0.7583\n",
            "Epoch 145/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1791 - accuracy: 0.8315 - val_loss: 0.3823 - val_accuracy: 0.7417\n",
            "Epoch 146/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1792 - accuracy: 0.8301 - val_loss: 0.3568 - val_accuracy: 0.7583\n",
            "Epoch 147/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1751 - accuracy: 0.8310 - val_loss: 0.3076 - val_accuracy: 0.7778\n",
            "Epoch 148/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.1740 - accuracy: 0.8319 - val_loss: 0.3160 - val_accuracy: 0.7722\n",
            "Epoch 149/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1782 - accuracy: 0.8306 - val_loss: 0.3757 - val_accuracy: 0.7556\n",
            "Epoch 150/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1781 - accuracy: 0.8301 - val_loss: 0.3344 - val_accuracy: 0.7667\n",
            "Epoch 151/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1773 - accuracy: 0.8315 - val_loss: 0.3054 - val_accuracy: 0.7722\n",
            "Epoch 152/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1730 - accuracy: 0.8324 - val_loss: 0.3239 - val_accuracy: 0.7667\n",
            "Epoch 153/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1717 - accuracy: 0.8329 - val_loss: 0.3467 - val_accuracy: 0.7611\n",
            "Epoch 154/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1783 - accuracy: 0.8324 - val_loss: 0.3044 - val_accuracy: 0.7750\n",
            "Epoch 155/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1803 - accuracy: 0.8310 - val_loss: 0.3955 - val_accuracy: 0.7472\n",
            "Epoch 156/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1799 - accuracy: 0.8278 - val_loss: 0.4602 - val_accuracy: 0.7139\n",
            "Epoch 157/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1701 - accuracy: 0.8315 - val_loss: 0.3760 - val_accuracy: 0.7417\n",
            "Epoch 158/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1728 - accuracy: 0.8324 - val_loss: 0.4570 - val_accuracy: 0.7028\n",
            "Epoch 159/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1676 - accuracy: 0.8319 - val_loss: 0.3306 - val_accuracy: 0.7667\n",
            "Epoch 160/200\n",
            "71/72 [============================>.] - ETA: 0s - loss: 0.1709 - accuracy: 0.8329Restoring model weights from the end of the best epoch: 125.\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1707 - accuracy: 0.8333 - val_loss: 0.3423 - val_accuracy: 0.7639\n",
            "Epoch 160: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_hy_gru_ler.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history_class_mod = model_hy_gru_ler.fit([X_train_pres,X_train[:,:,features]], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=([X_val_pres,X_val[:,:,features]], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhhpQvIZugdt"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx463_K1Vj3W",
        "outputId": "2e772a10-8641-4bf0-9f3d-7ed352c0db69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 2s 18ms/step\n",
            "12/12 [==============================] - 0s 14ms/step\n",
            "Accuracy validation: 0.7805555555555556\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_hy_gru_ler.predict([X_test_pres,X_test[:,:,features]]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_hy_gru_ler.predict([X_val_pres,X_val[:,:,features]]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3GEPDV0Kq7Z"
      },
      "source": [
        "# Legs\n",
        "\n",
        "* LSTM\n",
        "* GRU\n",
        "* Hybrid pressuser+kinect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vhwO8_JZpvi"
      },
      "outputs": [],
      "source": [
        "features = joints_leg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmgBR3orVwev"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu9MPdYyvAbl"
      },
      "outputs": [],
      "source": [
        "#Building the LSTM RNN model for \"Legs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGYUU1g8Vwew"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bftLop_sVwew"
      },
      "outputs": [],
      "source": [
        "class classifier_lstm(Model):\n",
        "  def __init__(self):\n",
        "    super(classifier_lstm, self).__init__()\n",
        "    self.input_ = len(features)\n",
        "    self.rec =  tf.keras.Sequential([\n",
        "            layers.LSTM(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=self.input_, return_sequences=False,dropout=0.5),\n",
        "          ])\n",
        "\n",
        "    self.ffnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "  def call(self,x):\n",
        "    out = self.rec(x)\n",
        "    clas = self.ffnn(out)\n",
        "    return clas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hx8I6qpVwex"
      },
      "outputs": [],
      "source": [
        "model_lstm_legs = classifier_lstm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLOuuLlgVwex",
        "outputId": "3106e863-f276-49b9-f230-9b9482fa0098"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_84\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_88 (LSTM)              (None, None, 256)         287744    \n",
            "                                                                 \n",
            " lstm_89 (LSTM)              (None, None, 256)         525312    \n",
            "                                                                 \n",
            " lstm_90 (LSTM)              (None, None, 128)         197120    \n",
            "                                                                 \n",
            " lstm_91 (LSTM)              (None, 128)               131584    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,141,760\n",
            "Trainable params: 1,141,760\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_legs.rec.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgwOtNcMVwex",
        "outputId": "0b3c0230-96d6-47eb-c6c4-bd8a82c8f471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_85\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_189 (Dense)           (2, 128)                  16384     \n",
            "                                                                 \n",
            " batch_normalization_156 (Ba  (2, 128)                 512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_174 (Activation)  (2, 128)                 0         \n",
            "                                                                 \n",
            " dense_190 (Dense)           (2, 64)                   8192      \n",
            "                                                                 \n",
            " batch_normalization_157 (Ba  (2, 64)                  256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_175 (Activation)  (2, 64)                  0         \n",
            "                                                                 \n",
            " dense_191 (Dense)           (2, 32)                   2048      \n",
            "                                                                 \n",
            " batch_normalization_158 (Ba  (2, 32)                  128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_176 (Activation)  (2, 32)                  0         \n",
            "                                                                 \n",
            " dense_192 (Dense)           (2, 16)                   512       \n",
            "                                                                 \n",
            " batch_normalization_159 (Ba  (2, 16)                  64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_177 (Activation)  (2, 16)                  0         \n",
            "                                                                 \n",
            " dense_193 (Dense)           (2, 6)                    102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,198\n",
            "Trainable params: 27,718\n",
            "Non-trainable params: 480\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_legs(X_train[1:3,:,features])\n",
        "model_lstm_legs.ffnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t836qvyVwex",
        "outputId": "fe3a009e-0684-45f4-afe8-50dd910aa106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 9s 47ms/step - loss: 1.7055 - accuracy: 0.1782 - val_loss: 1.4964 - val_accuracy: 0.1667\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 1.4075 - accuracy: 0.2620 - val_loss: 1.4455 - val_accuracy: 0.1222\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 1.2444 - accuracy: 0.3481 - val_loss: 1.3350 - val_accuracy: 0.2889\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.1270 - accuracy: 0.4014 - val_loss: 1.1852 - val_accuracy: 0.4972\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.0425 - accuracy: 0.4574 - val_loss: 1.0028 - val_accuracy: 0.5806\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.9522 - accuracy: 0.5449 - val_loss: 0.9354 - val_accuracy: 0.6222\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.8748 - accuracy: 0.6074 - val_loss: 0.9009 - val_accuracy: 0.5944\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.8048 - accuracy: 0.6662 - val_loss: 0.8250 - val_accuracy: 0.6667\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.7431 - accuracy: 0.7255 - val_loss: 0.8336 - val_accuracy: 0.6222\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6913 - accuracy: 0.7477 - val_loss: 0.7279 - val_accuracy: 0.6750\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6371 - accuracy: 0.7829 - val_loss: 0.7407 - val_accuracy: 0.6556\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.5977 - accuracy: 0.8088 - val_loss: 0.8248 - val_accuracy: 0.6167\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.5545 - accuracy: 0.8222 - val_loss: 0.7346 - val_accuracy: 0.6694\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.5357 - accuracy: 0.8477 - val_loss: 0.7072 - val_accuracy: 0.6861\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.4866 - accuracy: 0.8704 - val_loss: 0.6740 - val_accuracy: 0.6778\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.4627 - accuracy: 0.8745 - val_loss: 0.8209 - val_accuracy: 0.6028\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.4420 - accuracy: 0.8889 - val_loss: 0.7481 - val_accuracy: 0.7056\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.4064 - accuracy: 0.9037 - val_loss: 0.7243 - val_accuracy: 0.7056\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.3819 - accuracy: 0.9157 - val_loss: 0.8032 - val_accuracy: 0.6389\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.3524 - accuracy: 0.9278 - val_loss: 0.6859 - val_accuracy: 0.7250\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.3225 - accuracy: 0.9352 - val_loss: 0.7494 - val_accuracy: 0.7139\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2942 - accuracy: 0.9472 - val_loss: 0.6602 - val_accuracy: 0.7278\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2744 - accuracy: 0.9556 - val_loss: 0.6508 - val_accuracy: 0.7528\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2656 - accuracy: 0.9523 - val_loss: 0.7671 - val_accuracy: 0.7306\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2563 - accuracy: 0.9551 - val_loss: 0.6142 - val_accuracy: 0.7417\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2429 - accuracy: 0.9560 - val_loss: 0.7116 - val_accuracy: 0.7306\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2222 - accuracy: 0.9662 - val_loss: 0.7028 - val_accuracy: 0.7500\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2038 - accuracy: 0.9708 - val_loss: 0.6577 - val_accuracy: 0.7500\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2104 - accuracy: 0.9634 - val_loss: 0.7151 - val_accuracy: 0.7167\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1894 - accuracy: 0.9708 - val_loss: 0.7803 - val_accuracy: 0.7194\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1852 - accuracy: 0.9685 - val_loss: 0.6273 - val_accuracy: 0.7083\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1844 - accuracy: 0.9602 - val_loss: 0.6137 - val_accuracy: 0.7639\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1921 - accuracy: 0.9394 - val_loss: 0.7042 - val_accuracy: 0.7083\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1729 - accuracy: 0.9435 - val_loss: 0.7728 - val_accuracy: 0.6972\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.1575 - accuracy: 0.9579 - val_loss: 0.6283 - val_accuracy: 0.7444\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1610 - accuracy: 0.9546 - val_loss: 0.7356 - val_accuracy: 0.6889\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1649 - accuracy: 0.9495 - val_loss: 0.6245 - val_accuracy: 0.7528\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1702 - accuracy: 0.9440 - val_loss: 0.8207 - val_accuracy: 0.6944\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1517 - accuracy: 0.9370 - val_loss: 0.5975 - val_accuracy: 0.7722\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1396 - accuracy: 0.9500 - val_loss: 0.7926 - val_accuracy: 0.7056\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1341 - accuracy: 0.9546 - val_loss: 0.6674 - val_accuracy: 0.7639\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1407 - accuracy: 0.9417 - val_loss: 0.7544 - val_accuracy: 0.7417\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1457 - accuracy: 0.9366 - val_loss: 0.8274 - val_accuracy: 0.6694\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1331 - accuracy: 0.9190 - val_loss: 0.7445 - val_accuracy: 0.7194\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1245 - accuracy: 0.9301 - val_loss: 0.7535 - val_accuracy: 0.7222\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1161 - accuracy: 0.9384 - val_loss: 0.6178 - val_accuracy: 0.7667\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1031 - accuracy: 0.9255 - val_loss: 0.5318 - val_accuracy: 0.8000\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1171 - accuracy: 0.9273 - val_loss: 0.4773 - val_accuracy: 0.8139\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0994 - accuracy: 0.9208 - val_loss: 0.6368 - val_accuracy: 0.7861\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1012 - accuracy: 0.9060 - val_loss: 0.7590 - val_accuracy: 0.7361\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1003 - accuracy: 0.8935 - val_loss: 0.6188 - val_accuracy: 0.7472\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1063 - accuracy: 0.8769 - val_loss: 0.7409 - val_accuracy: 0.6472\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1342 - accuracy: 0.8472 - val_loss: 0.7058 - val_accuracy: 0.6472\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1154 - accuracy: 0.8611 - val_loss: 0.5269 - val_accuracy: 0.6750\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0974 - accuracy: 0.8630 - val_loss: 0.6051 - val_accuracy: 0.6528\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1046 - accuracy: 0.8616 - val_loss: 0.6170 - val_accuracy: 0.7028\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0940 - accuracy: 0.9083 - val_loss: 0.6980 - val_accuracy: 0.6889\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.0912 - accuracy: 0.9231 - val_loss: 0.5910 - val_accuracy: 0.7889\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.1034 - accuracy: 0.9315 - val_loss: 0.7661 - val_accuracy: 0.7250\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0946 - accuracy: 0.9412 - val_loss: 0.6635 - val_accuracy: 0.7694\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0923 - accuracy: 0.9505 - val_loss: 0.5905 - val_accuracy: 0.7806\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0930 - accuracy: 0.9630 - val_loss: 0.5552 - val_accuracy: 0.7889\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0900 - accuracy: 0.9532 - val_loss: 0.6669 - val_accuracy: 0.7500\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0966 - accuracy: 0.9440 - val_loss: 0.6153 - val_accuracy: 0.7583\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0850 - accuracy: 0.9560 - val_loss: 0.5972 - val_accuracy: 0.7833\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.0811 - accuracy: 0.9449 - val_loss: 0.6372 - val_accuracy: 0.7556\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0820 - accuracy: 0.9597 - val_loss: 0.7400 - val_accuracy: 0.7444\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0796 - accuracy: 0.9505 - val_loss: 0.5927 - val_accuracy: 0.7667\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0767 - accuracy: 0.9519 - val_loss: 0.5741 - val_accuracy: 0.7944\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0762 - accuracy: 0.9370 - val_loss: 0.6408 - val_accuracy: 0.7778\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1017 - accuracy: 0.9319 - val_loss: 0.7360 - val_accuracy: 0.7611\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0775 - accuracy: 0.9375 - val_loss: 0.9006 - val_accuracy: 0.7417\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0677 - accuracy: 0.9394 - val_loss: 0.6748 - val_accuracy: 0.7750\n",
            "Epoch 74/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.0861 - accuracy: 0.9199 - val_loss: 0.5408 - val_accuracy: 0.7944\n",
            "Epoch 75/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0792 - accuracy: 0.9505 - val_loss: 0.5560 - val_accuracy: 0.7972\n",
            "Epoch 76/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0749 - accuracy: 0.9338 - val_loss: 0.5063 - val_accuracy: 0.8139\n",
            "Epoch 77/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0764 - accuracy: 0.9255 - val_loss: 0.5314 - val_accuracy: 0.8056\n",
            "Epoch 78/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0615 - accuracy: 0.9301 - val_loss: 0.6956 - val_accuracy: 0.7778\n",
            "Epoch 79/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0839 - accuracy: 0.9185 - val_loss: 0.6641 - val_accuracy: 0.6750\n",
            "Epoch 80/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0799 - accuracy: 0.9125 - val_loss: 0.9304 - val_accuracy: 0.7111\n",
            "Epoch 81/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0673 - accuracy: 0.9102 - val_loss: 0.6805 - val_accuracy: 0.7583\n",
            "Epoch 82/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.0700 - accuracy: 0.9148 - val_loss: 0.9638 - val_accuracy: 0.7139\n",
            "Epoch 83/200\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9185Restoring model weights from the end of the best epoch: 48.\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0666 - accuracy: 0.9185 - val_loss: 0.9118 - val_accuracy: 0.7222\n",
            "Epoch 83: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_lstm_legs.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "history_class_mod = model_lstm_legs.fit(X_train[:,:,features], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_val[:,:,features], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "370r1h6RvKUA"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDBoC4CuVwex",
        "outputId": "8c727bb3-f0ec-4c71-e272-e7f56a1d9ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 2s 16ms/step\n",
            "12/12 [==============================] - 0s 12ms/step\n",
            "Accuracy validation: 0.8138888888888889\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_lstm_legs.predict(X_test[:,:,features]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_lstm_legs.predict(X_val[:,:,features]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uia438imVwey"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXOGB9yCVwey"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O8dUmeSw1Ag"
      },
      "outputs": [],
      "source": [
        "#Building the GRU RNN model for \"Legs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGoOPLt8Vwey"
      },
      "outputs": [],
      "source": [
        "class classifier_gru(Model):\n",
        "  def __init__(self):\n",
        "    super(classifier_gru, self).__init__()\n",
        "    self.input_ = len(features)\n",
        "    self.rec =  tf.keras.Sequential([\n",
        "            layers.GRU(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(256,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=self.input_, return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=self.input_, return_sequences=False,dropout=0.5),\n",
        "            ])\n",
        "\n",
        "\n",
        "    self.ffnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "  def call(self,x):\n",
        "    out = self.rec(x)\n",
        "    clas = self.ffnn(out)\n",
        "    return clas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IOEOxoXVwey"
      },
      "outputs": [],
      "source": [
        "model_gru_legs = classifier_gru()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjytHD-aVwey",
        "outputId": "19074388-7a80-42d7-b5ee-efff6d2b1bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_86\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_44 (GRU)                (None, None, 256)         216576    \n",
            "                                                                 \n",
            " gru_45 (GRU)                (None, None, 256)         394752    \n",
            "                                                                 \n",
            " gru_46 (GRU)                (None, None, 128)         148224    \n",
            "                                                                 \n",
            " gru_47 (GRU)                (None, 128)               99072     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 858,624\n",
            "Trainable params: 858,624\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_gru_legs.rec.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqJB7Zz6Vwey",
        "outputId": "7fe1c3cf-84e5-4195-a13d-91cd30af2eda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_87\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_194 (Dense)           (2, 128)                  16384     \n",
            "                                                                 \n",
            " batch_normalization_160 (Ba  (2, 128)                 512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_178 (Activation)  (2, 128)                 0         \n",
            "                                                                 \n",
            " dense_195 (Dense)           (2, 64)                   8192      \n",
            "                                                                 \n",
            " batch_normalization_161 (Ba  (2, 64)                  256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_179 (Activation)  (2, 64)                  0         \n",
            "                                                                 \n",
            " dense_196 (Dense)           (2, 32)                   2048      \n",
            "                                                                 \n",
            " batch_normalization_162 (Ba  (2, 32)                  128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_180 (Activation)  (2, 32)                  0         \n",
            "                                                                 \n",
            " dense_197 (Dense)           (2, 16)                   512       \n",
            "                                                                 \n",
            " batch_normalization_163 (Ba  (2, 16)                  64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_181 (Activation)  (2, 16)                  0         \n",
            "                                                                 \n",
            " dense_198 (Dense)           (2, 6)                    102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,198\n",
            "Trainable params: 27,718\n",
            "Non-trainable params: 480\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_gru_legs(X_train[1:3,:,features])\n",
        "model_gru_legs.ffnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_Pud1QvxvIU"
      },
      "outputs": [],
      "source": [
        "#Training the GRU RNN model on \"Legs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozuGoHpgVwez",
        "outputId": "27a697af-9eb0-471e-b054-75a7802e5919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 8s 42ms/step - loss: 1.8856 - accuracy: 0.1639 - val_loss: 1.4871 - val_accuracy: 0.1722\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.7698 - accuracy: 0.1713 - val_loss: 1.4923 - val_accuracy: 0.2083\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.6725 - accuracy: 0.1995 - val_loss: 1.3965 - val_accuracy: 0.2083\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.5761 - accuracy: 0.2171 - val_loss: 1.3178 - val_accuracy: 0.2639\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 1.4943 - accuracy: 0.2417 - val_loss: 1.2404 - val_accuracy: 0.3639\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.4398 - accuracy: 0.2745 - val_loss: 1.2215 - val_accuracy: 0.4028\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.4069 - accuracy: 0.2907 - val_loss: 1.0829 - val_accuracy: 0.4472\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 1.3675 - accuracy: 0.3093 - val_loss: 1.0344 - val_accuracy: 0.4528\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 1.3171 - accuracy: 0.3301 - val_loss: 1.0364 - val_accuracy: 0.4306\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 1.2761 - accuracy: 0.3597 - val_loss: 0.9774 - val_accuracy: 0.5083\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.2283 - accuracy: 0.4046 - val_loss: 1.0048 - val_accuracy: 0.5194\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 1.2163 - accuracy: 0.3894 - val_loss: 0.9570 - val_accuracy: 0.5417\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 1.1829 - accuracy: 0.4176 - val_loss: 0.8867 - val_accuracy: 0.6167\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.1540 - accuracy: 0.4384 - val_loss: 0.8215 - val_accuracy: 0.6167\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.1323 - accuracy: 0.4403 - val_loss: 0.8168 - val_accuracy: 0.5806\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.0809 - accuracy: 0.4773 - val_loss: 0.7808 - val_accuracy: 0.6167\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.0291 - accuracy: 0.5093 - val_loss: 0.7060 - val_accuracy: 0.6306\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.9638 - accuracy: 0.5157 - val_loss: 0.6341 - val_accuracy: 0.6417\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.9276 - accuracy: 0.5468 - val_loss: 0.6535 - val_accuracy: 0.6306\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.8744 - accuracy: 0.5583 - val_loss: 0.6363 - val_accuracy: 0.6417\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.8343 - accuracy: 0.5889 - val_loss: 0.6144 - val_accuracy: 0.6278\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.8038 - accuracy: 0.6046 - val_loss: 0.5842 - val_accuracy: 0.6306\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.7738 - accuracy: 0.6162 - val_loss: 0.5958 - val_accuracy: 0.6306\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.7424 - accuracy: 0.6338 - val_loss: 0.6265 - val_accuracy: 0.6306\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.7036 - accuracy: 0.6500 - val_loss: 0.5690 - val_accuracy: 0.6361\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.6766 - accuracy: 0.6681 - val_loss: 0.6461 - val_accuracy: 0.6306\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.6281 - accuracy: 0.6949 - val_loss: 0.5835 - val_accuracy: 0.6528\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.6010 - accuracy: 0.6981 - val_loss: 0.5849 - val_accuracy: 0.6528\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.5779 - accuracy: 0.7153 - val_loss: 0.5563 - val_accuracy: 0.6500\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5288 - accuracy: 0.7255 - val_loss: 0.5575 - val_accuracy: 0.6361\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.5060 - accuracy: 0.7296 - val_loss: 0.5992 - val_accuracy: 0.6417\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.4879 - accuracy: 0.7389 - val_loss: 0.6274 - val_accuracy: 0.6222\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.4531 - accuracy: 0.7569 - val_loss: 0.5905 - val_accuracy: 0.6556\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.4441 - accuracy: 0.7593 - val_loss: 0.6278 - val_accuracy: 0.6528\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.4085 - accuracy: 0.7815 - val_loss: 0.5000 - val_accuracy: 0.6833\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.4077 - accuracy: 0.7861 - val_loss: 0.5671 - val_accuracy: 0.6444\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.3920 - accuracy: 0.7880 - val_loss: 0.5655 - val_accuracy: 0.6556\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.3752 - accuracy: 0.8116 - val_loss: 0.5853 - val_accuracy: 0.6500\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.3515 - accuracy: 0.8227 - val_loss: 0.6153 - val_accuracy: 0.6444\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.3345 - accuracy: 0.8319 - val_loss: 0.5604 - val_accuracy: 0.6667\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.3249 - accuracy: 0.8375 - val_loss: 0.5346 - val_accuracy: 0.6667\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.3207 - accuracy: 0.8407 - val_loss: 0.5261 - val_accuracy: 0.6694\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.3105 - accuracy: 0.8366 - val_loss: 0.5286 - val_accuracy: 0.6722\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.3061 - accuracy: 0.8463 - val_loss: 0.5307 - val_accuracy: 0.6694\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.2929 - accuracy: 0.8505 - val_loss: 0.5286 - val_accuracy: 0.6778\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2715 - accuracy: 0.8574 - val_loss: 0.5731 - val_accuracy: 0.6556\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2595 - accuracy: 0.8481 - val_loss: 0.5206 - val_accuracy: 0.6639\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2699 - accuracy: 0.8472 - val_loss: 0.4736 - val_accuracy: 0.6722\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.2382 - accuracy: 0.8569 - val_loss: 0.6875 - val_accuracy: 0.6139\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.2302 - accuracy: 0.8514 - val_loss: 0.5016 - val_accuracy: 0.6750\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2346 - accuracy: 0.8481 - val_loss: 0.5640 - val_accuracy: 0.6667\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.2231 - accuracy: 0.8514 - val_loss: 0.5152 - val_accuracy: 0.6833\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.2101 - accuracy: 0.8458 - val_loss: 0.5687 - val_accuracy: 0.6611\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.2304 - accuracy: 0.8366 - val_loss: 0.5701 - val_accuracy: 0.6556\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.2103 - accuracy: 0.8440 - val_loss: 0.5499 - val_accuracy: 0.6750\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1953 - accuracy: 0.8560 - val_loss: 0.5809 - val_accuracy: 0.6556\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.2026 - accuracy: 0.8412 - val_loss: 0.5258 - val_accuracy: 0.6806\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1897 - accuracy: 0.8481 - val_loss: 0.5576 - val_accuracy: 0.6611\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.2029 - accuracy: 0.8352 - val_loss: 0.4961 - val_accuracy: 0.6917\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1851 - accuracy: 0.8458 - val_loss: 0.5480 - val_accuracy: 0.6778\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1797 - accuracy: 0.8403 - val_loss: 0.5503 - val_accuracy: 0.6750\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1740 - accuracy: 0.8389 - val_loss: 0.5505 - val_accuracy: 0.6639\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1726 - accuracy: 0.8394 - val_loss: 0.6090 - val_accuracy: 0.6500\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1683 - accuracy: 0.8407 - val_loss: 0.5028 - val_accuracy: 0.6917\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1659 - accuracy: 0.8319 - val_loss: 0.5574 - val_accuracy: 0.6778\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1670 - accuracy: 0.8241 - val_loss: 0.4834 - val_accuracy: 0.6917\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1566 - accuracy: 0.8352 - val_loss: 0.4634 - val_accuracy: 0.6806\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1570 - accuracy: 0.8301 - val_loss: 0.5695 - val_accuracy: 0.6667\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1483 - accuracy: 0.8324 - val_loss: 0.5254 - val_accuracy: 0.6861\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1487 - accuracy: 0.8269 - val_loss: 0.5460 - val_accuracy: 0.6806\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1462 - accuracy: 0.8236 - val_loss: 0.5535 - val_accuracy: 0.6694\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1330 - accuracy: 0.8319 - val_loss: 0.5975 - val_accuracy: 0.6722\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1380 - accuracy: 0.8250 - val_loss: 0.5532 - val_accuracy: 0.6861\n",
            "Epoch 74/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1386 - accuracy: 0.8213 - val_loss: 0.4826 - val_accuracy: 0.6944\n",
            "Epoch 75/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1381 - accuracy: 0.8213 - val_loss: 0.4744 - val_accuracy: 0.7000\n",
            "Epoch 76/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1268 - accuracy: 0.8227 - val_loss: 0.5792 - val_accuracy: 0.6778\n",
            "Epoch 77/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1216 - accuracy: 0.8250 - val_loss: 0.5428 - val_accuracy: 0.6806\n",
            "Epoch 78/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1200 - accuracy: 0.8231 - val_loss: 0.4764 - val_accuracy: 0.6889\n",
            "Epoch 79/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1069 - accuracy: 0.8282 - val_loss: 0.5752 - val_accuracy: 0.6750\n",
            "Epoch 80/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1179 - accuracy: 0.8231 - val_loss: 0.5817 - val_accuracy: 0.6722\n",
            "Epoch 81/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.1147 - accuracy: 0.8255 - val_loss: 0.5849 - val_accuracy: 0.6778\n",
            "Epoch 82/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1099 - accuracy: 0.8264 - val_loss: 0.6133 - val_accuracy: 0.6556\n",
            "Epoch 83/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1073 - accuracy: 0.8431 - val_loss: 0.5847 - val_accuracy: 0.6861\n",
            "Epoch 84/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1168 - accuracy: 0.8653 - val_loss: 0.5220 - val_accuracy: 0.6917\n",
            "Epoch 85/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1105 - accuracy: 0.8782 - val_loss: 0.5323 - val_accuracy: 0.6889\n",
            "Epoch 86/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1159 - accuracy: 0.8889 - val_loss: 0.5125 - val_accuracy: 0.6917\n",
            "Epoch 87/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1165 - accuracy: 0.8852 - val_loss: 0.6269 - val_accuracy: 0.6694\n",
            "Epoch 88/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1192 - accuracy: 0.8921 - val_loss: 0.5281 - val_accuracy: 0.6833\n",
            "Epoch 89/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1139 - accuracy: 0.9014 - val_loss: 0.4495 - val_accuracy: 0.7111\n",
            "Epoch 90/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1184 - accuracy: 0.8986 - val_loss: 0.4920 - val_accuracy: 0.7167\n",
            "Epoch 91/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1146 - accuracy: 0.8935 - val_loss: 0.5924 - val_accuracy: 0.6833\n",
            "Epoch 92/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1144 - accuracy: 0.9046 - val_loss: 0.5168 - val_accuracy: 0.6944\n",
            "Epoch 93/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1142 - accuracy: 0.9069 - val_loss: 0.5569 - val_accuracy: 0.6861\n",
            "Epoch 94/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1268 - accuracy: 0.8991 - val_loss: 0.4430 - val_accuracy: 0.7167\n",
            "Epoch 95/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1087 - accuracy: 0.9000 - val_loss: 0.6001 - val_accuracy: 0.6722\n",
            "Epoch 96/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1117 - accuracy: 0.8829 - val_loss: 0.5947 - val_accuracy: 0.6722\n",
            "Epoch 97/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1199 - accuracy: 0.8801 - val_loss: 0.6021 - val_accuracy: 0.6750\n",
            "Epoch 98/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1245 - accuracy: 0.8764 - val_loss: 0.4942 - val_accuracy: 0.7000\n",
            "Epoch 99/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1106 - accuracy: 0.8847 - val_loss: 0.5289 - val_accuracy: 0.7000\n",
            "Epoch 100/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1126 - accuracy: 0.8727 - val_loss: 0.5181 - val_accuracy: 0.6972\n",
            "Epoch 101/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1034 - accuracy: 0.8699 - val_loss: 0.4774 - val_accuracy: 0.7139\n",
            "Epoch 102/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1186 - accuracy: 0.8583 - val_loss: 0.5234 - val_accuracy: 0.6889\n",
            "Epoch 103/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1136 - accuracy: 0.8560 - val_loss: 0.4308 - val_accuracy: 0.7250\n",
            "Epoch 104/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1114 - accuracy: 0.8551 - val_loss: 0.4423 - val_accuracy: 0.7306\n",
            "Epoch 105/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1013 - accuracy: 0.8560 - val_loss: 0.3924 - val_accuracy: 0.7389\n",
            "Epoch 106/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0975 - accuracy: 0.8546 - val_loss: 0.5265 - val_accuracy: 0.6806\n",
            "Epoch 107/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.1050 - accuracy: 0.8444 - val_loss: 0.4756 - val_accuracy: 0.7083\n",
            "Epoch 108/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.1017 - accuracy: 0.8343 - val_loss: 0.4667 - val_accuracy: 0.7250\n",
            "Epoch 109/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1095 - accuracy: 0.8282 - val_loss: 0.4667 - val_accuracy: 0.7139\n",
            "Epoch 110/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1074 - accuracy: 0.8287 - val_loss: 0.3637 - val_accuracy: 0.7472\n",
            "Epoch 111/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1058 - accuracy: 0.8245 - val_loss: 0.4223 - val_accuracy: 0.7278\n",
            "Epoch 112/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1057 - accuracy: 0.8236 - val_loss: 0.4363 - val_accuracy: 0.7139\n",
            "Epoch 113/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.0988 - accuracy: 0.8278 - val_loss: 0.4889 - val_accuracy: 0.7250\n",
            "Epoch 114/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1033 - accuracy: 0.8227 - val_loss: 0.5068 - val_accuracy: 0.7111\n",
            "Epoch 115/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0939 - accuracy: 0.8269 - val_loss: 0.5459 - val_accuracy: 0.7083\n",
            "Epoch 116/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0941 - accuracy: 0.8255 - val_loss: 0.4277 - val_accuracy: 0.7306\n",
            "Epoch 117/200\n",
            "72/72 [==============================] - 2s 23ms/step - loss: 0.0931 - accuracy: 0.8250 - val_loss: 0.5327 - val_accuracy: 0.7111\n",
            "Epoch 118/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0926 - accuracy: 0.8269 - val_loss: 0.5547 - val_accuracy: 0.6972\n",
            "Epoch 119/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0989 - accuracy: 0.8213 - val_loss: 0.4572 - val_accuracy: 0.7278\n",
            "Epoch 120/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0950 - accuracy: 0.8236 - val_loss: 0.4746 - val_accuracy: 0.7222\n",
            "Epoch 121/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0941 - accuracy: 0.8269 - val_loss: 0.5018 - val_accuracy: 0.7250\n",
            "Epoch 122/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0926 - accuracy: 0.8259 - val_loss: 0.4706 - val_accuracy: 0.7167\n",
            "Epoch 123/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0891 - accuracy: 0.8278 - val_loss: 0.4956 - val_accuracy: 0.7222\n",
            "Epoch 124/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.0965 - accuracy: 0.8227 - val_loss: 0.4550 - val_accuracy: 0.7306\n",
            "Epoch 125/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0959 - accuracy: 0.8264 - val_loss: 0.4995 - val_accuracy: 0.7250\n",
            "Epoch 126/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0931 - accuracy: 0.8259 - val_loss: 0.4743 - val_accuracy: 0.7250\n",
            "Epoch 127/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.1065 - accuracy: 0.8227 - val_loss: 0.5213 - val_accuracy: 0.7167\n",
            "Epoch 128/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0999 - accuracy: 0.8245 - val_loss: 0.4572 - val_accuracy: 0.7250\n",
            "Epoch 129/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.1020 - accuracy: 0.8241 - val_loss: 0.5250 - val_accuracy: 0.7222\n",
            "Epoch 130/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.0937 - accuracy: 0.8264 - val_loss: 0.4919 - val_accuracy: 0.7167\n",
            "Epoch 131/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.0968 - accuracy: 0.8255 - val_loss: 0.5519 - val_accuracy: 0.7000\n",
            "Epoch 132/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0988 - accuracy: 0.8245 - val_loss: 0.4835 - val_accuracy: 0.7083\n",
            "Epoch 133/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0937 - accuracy: 0.8255 - val_loss: 0.5293 - val_accuracy: 0.7194\n",
            "Epoch 134/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0946 - accuracy: 0.8264 - val_loss: 0.5423 - val_accuracy: 0.7167\n",
            "Epoch 135/200\n",
            "72/72 [==============================] - 1s 21ms/step - loss: 0.0910 - accuracy: 0.8282 - val_loss: 0.4949 - val_accuracy: 0.6944\n",
            "Epoch 136/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0924 - accuracy: 0.8259 - val_loss: 0.5849 - val_accuracy: 0.7000\n",
            "Epoch 137/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0957 - accuracy: 0.8259 - val_loss: 0.5342 - val_accuracy: 0.7139\n",
            "Epoch 138/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0957 - accuracy: 0.8255 - val_loss: 0.5562 - val_accuracy: 0.7194\n",
            "Epoch 139/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.1009 - accuracy: 0.8245 - val_loss: 0.5908 - val_accuracy: 0.6889\n",
            "Epoch 140/200\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0965 - accuracy: 0.8259 - val_loss: 0.5758 - val_accuracy: 0.7111\n",
            "Epoch 141/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0898 - accuracy: 0.8282 - val_loss: 0.4846 - val_accuracy: 0.7278\n",
            "Epoch 142/200\n",
            "72/72 [==============================] - 2s 24ms/step - loss: 0.0922 - accuracy: 0.8278 - val_loss: 0.5207 - val_accuracy: 0.7167\n",
            "Epoch 143/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0909 - accuracy: 0.8296 - val_loss: 0.5413 - val_accuracy: 0.7139\n",
            "Epoch 144/200\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 0.0969 - accuracy: 0.8282 - val_loss: 0.6606 - val_accuracy: 0.6861\n",
            "Epoch 145/200\n",
            "70/72 [============================>.] - ETA: 0s - loss: 0.0953 - accuracy: 0.8267Restoring model weights from the end of the best epoch: 110.\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 0.0947 - accuracy: 0.8278 - val_loss: 0.5629 - val_accuracy: 0.7083\n",
            "Epoch 145: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_gru_legs.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history_class_mod = model_gru_legs.fit(X_train[:,:,features], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_val[:,:,features], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9v2Q1xXx0Bo"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCjOJMJ9Vwez",
        "outputId": "965ff8f8-cacf-48f8-bf84-e4f157cb90ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 2s 16ms/step\n",
            "12/12 [==============================] - 0s 13ms/step\n",
            "Accuracy validation: 0.7472222222222222\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_gru_legs.predict(X_test[:,:,features]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_gru_legs.predict(X_val[:,:,features]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44u6sMzgVwe0"
      },
      "source": [
        "## Hybrid-LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c63ms0fCVwe0"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "538yqqjlx-Ze"
      },
      "outputs": [],
      "source": [
        "#Building the Hybrid-LSTM RNN CNN model for \"Legs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOpxOAZLVwe0"
      },
      "outputs": [],
      "source": [
        "class fuse(Model):\n",
        "  def __init__(self):\n",
        "    super(fuse, self).__init__()\n",
        "    self.cnn = tf.keras.Sequential([\n",
        "      layers.Input(shape=(128, 48,1)),\n",
        "      layers.Conv2D(128, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,1)),\n",
        "      layers.Conv2D(64, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Conv2D(32, (3, 3), activation=None, padding='same', strides=1),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Flatten()])\n",
        "\n",
        "\n",
        "\n",
        "    self.rnn =  tf.keras.Sequential([\n",
        "            layers.LSTM(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.LSTM(128,input_dim=len(features), return_sequences=False,dropout=0.5),\n",
        "    ])\n",
        "\n",
        "    self.calssifier = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "\n",
        "    self.fc_cnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "    self.fc_rnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "\n",
        "  def call(self,x):\n",
        "    out0 = self.cnn(x[0])\n",
        "    out0 = self.fc_cnn(out0)\n",
        "    out1 = self.rnn(x[1])\n",
        "    out1 = self.fc_rnn(out1)\n",
        "\n",
        "    out = self.calssifier(layers.concatenate([out0,out1]))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GR3jJbGVwe0"
      },
      "outputs": [],
      "source": [
        "model_hy_lstm_legs=fuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if3SC-_gVwe0",
        "outputId": "617b2a8e-710f-4521-fced-fbe16e91e10e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_88\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_18 (Conv2D)          (None, 64, 24, 128)       1280      \n",
            "                                                                 \n",
            " activation_182 (Activation)  (None, 64, 24, 128)      0         \n",
            "                                                                 \n",
            " average_pooling2d_18 (Avera  (None, 32, 24, 128)      0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 16, 12, 64)        73792     \n",
            "                                                                 \n",
            " activation_183 (Activation)  (None, 16, 12, 64)       0         \n",
            "                                                                 \n",
            " average_pooling2d_19 (Avera  (None, 8, 6, 64)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 8, 6, 32)          18464     \n",
            "                                                                 \n",
            " activation_184 (Activation)  (None, 8, 6, 32)         0         \n",
            "                                                                 \n",
            " average_pooling2d_20 (Avera  (None, 4, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 384)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,536\n",
            "Trainable params: 93,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_hy_lstm_legs.cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbEMhEJwyGyn"
      },
      "outputs": [],
      "source": [
        "#Training the Hybrid-LSTM RNN CNN model for \"Legs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOnJT1oDVwe0",
        "outputId": "ca8b1d78-57f7-488c-af36-f2abaffcfc0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 12s 66ms/step - loss: 1.6451 - accuracy: 0.1889 - val_loss: 1.5316 - val_accuracy: 0.1667\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 1.4335 - accuracy: 0.2917 - val_loss: 1.5482 - val_accuracy: 0.1667\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.2987 - accuracy: 0.3685 - val_loss: 1.5613 - val_accuracy: 0.1500\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.1778 - accuracy: 0.4403 - val_loss: 1.5681 - val_accuracy: 0.1667\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.0968 - accuracy: 0.4972 - val_loss: 1.4094 - val_accuracy: 0.2861\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.0103 - accuracy: 0.5694 - val_loss: 1.2408 - val_accuracy: 0.4056\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9353 - accuracy: 0.5986 - val_loss: 1.0742 - val_accuracy: 0.4278\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.8488 - accuracy: 0.6449 - val_loss: 0.9572 - val_accuracy: 0.5500\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.7914 - accuracy: 0.6681 - val_loss: 0.8938 - val_accuracy: 0.5389\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.7225 - accuracy: 0.6968 - val_loss: 0.7704 - val_accuracy: 0.6056\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.6678 - accuracy: 0.7204 - val_loss: 0.7451 - val_accuracy: 0.6361\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.6293 - accuracy: 0.7296 - val_loss: 0.7157 - val_accuracy: 0.6861\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.5723 - accuracy: 0.7537 - val_loss: 0.6610 - val_accuracy: 0.6833\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.5482 - accuracy: 0.7634 - val_loss: 0.6488 - val_accuracy: 0.6750\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4951 - accuracy: 0.7727 - val_loss: 0.5956 - val_accuracy: 0.7167\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4686 - accuracy: 0.7884 - val_loss: 0.6393 - val_accuracy: 0.6500\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4353 - accuracy: 0.7931 - val_loss: 0.5649 - val_accuracy: 0.6972\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4151 - accuracy: 0.8046 - val_loss: 0.5954 - val_accuracy: 0.6806\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3990 - accuracy: 0.7954 - val_loss: 0.5562 - val_accuracy: 0.7000\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3740 - accuracy: 0.8032 - val_loss: 0.5780 - val_accuracy: 0.6667\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3575 - accuracy: 0.8074 - val_loss: 0.5632 - val_accuracy: 0.6861\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3375 - accuracy: 0.8106 - val_loss: 0.4425 - val_accuracy: 0.7306\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3416 - accuracy: 0.8079 - val_loss: 0.4492 - val_accuracy: 0.7194\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3252 - accuracy: 0.8032 - val_loss: 0.4245 - val_accuracy: 0.7333\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3347 - accuracy: 0.8042 - val_loss: 0.5714 - val_accuracy: 0.6500\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3050 - accuracy: 0.8167 - val_loss: 0.4364 - val_accuracy: 0.7278\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2960 - accuracy: 0.8167 - val_loss: 0.4813 - val_accuracy: 0.7111\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2866 - accuracy: 0.8171 - val_loss: 0.4345 - val_accuracy: 0.7278\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2836 - accuracy: 0.8167 - val_loss: 0.5268 - val_accuracy: 0.6917\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2704 - accuracy: 0.8190 - val_loss: 0.5448 - val_accuracy: 0.7028\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2599 - accuracy: 0.8190 - val_loss: 0.4755 - val_accuracy: 0.7056\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2665 - accuracy: 0.8130 - val_loss: 0.4575 - val_accuracy: 0.7167\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2492 - accuracy: 0.8208 - val_loss: 0.4928 - val_accuracy: 0.6889\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2446 - accuracy: 0.8222 - val_loss: 0.4067 - val_accuracy: 0.7278\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2450 - accuracy: 0.8171 - val_loss: 0.5108 - val_accuracy: 0.6861\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2405 - accuracy: 0.8227 - val_loss: 0.5428 - val_accuracy: 0.6750\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2422 - accuracy: 0.8171 - val_loss: 0.5007 - val_accuracy: 0.6806\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2576 - accuracy: 0.8093 - val_loss: 0.6501 - val_accuracy: 0.6333\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.2222 - accuracy: 0.8245 - val_loss: 0.4043 - val_accuracy: 0.7444\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2178 - accuracy: 0.8273 - val_loss: 0.4560 - val_accuracy: 0.6972\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2156 - accuracy: 0.8264 - val_loss: 0.4655 - val_accuracy: 0.7000\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2178 - accuracy: 0.8241 - val_loss: 0.6307 - val_accuracy: 0.6361\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2194 - accuracy: 0.8236 - val_loss: 0.4158 - val_accuracy: 0.7333\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2174 - accuracy: 0.8241 - val_loss: 0.4171 - val_accuracy: 0.7250\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2054 - accuracy: 0.8269 - val_loss: 0.3835 - val_accuracy: 0.7361\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2015 - accuracy: 0.8282 - val_loss: 0.4980 - val_accuracy: 0.6944\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2056 - accuracy: 0.8250 - val_loss: 0.3932 - val_accuracy: 0.7333\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2093 - accuracy: 0.8222 - val_loss: 0.3587 - val_accuracy: 0.7361\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2074 - accuracy: 0.8259 - val_loss: 0.5448 - val_accuracy: 0.6528\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1926 - accuracy: 0.8287 - val_loss: 0.3960 - val_accuracy: 0.7056\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2251 - accuracy: 0.8157 - val_loss: 0.5174 - val_accuracy: 0.6694\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.1984 - accuracy: 0.8273 - val_loss: 0.4134 - val_accuracy: 0.7083\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1912 - accuracy: 0.8282 - val_loss: 0.5730 - val_accuracy: 0.6417\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1928 - accuracy: 0.8287 - val_loss: 0.4959 - val_accuracy: 0.6806\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1942 - accuracy: 0.8241 - val_loss: 0.5588 - val_accuracy: 0.6667\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1888 - accuracy: 0.8259 - val_loss: 0.5723 - val_accuracy: 0.6889\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.1782 - accuracy: 0.8301 - val_loss: 0.4030 - val_accuracy: 0.7028\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1847 - accuracy: 0.8259 - val_loss: 0.5140 - val_accuracy: 0.6556\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2017 - accuracy: 0.8213 - val_loss: 0.5120 - val_accuracy: 0.6972\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1783 - accuracy: 0.8296 - val_loss: 0.5614 - val_accuracy: 0.6556\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1763 - accuracy: 0.8273 - val_loss: 0.4804 - val_accuracy: 0.6972\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1740 - accuracy: 0.8296 - val_loss: 0.5300 - val_accuracy: 0.6750\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.1749 - accuracy: 0.8296 - val_loss: 0.4551 - val_accuracy: 0.7028\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1656 - accuracy: 0.8301 - val_loss: 0.5229 - val_accuracy: 0.6944\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1680 - accuracy: 0.8296 - val_loss: 0.4168 - val_accuracy: 0.7333\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1737 - accuracy: 0.8250 - val_loss: 0.6814 - val_accuracy: 0.6222\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1685 - accuracy: 0.8282 - val_loss: 0.6111 - val_accuracy: 0.6472\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1602 - accuracy: 0.8306 - val_loss: 0.5846 - val_accuracy: 0.6694\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1586 - accuracy: 0.8310 - val_loss: 0.5456 - val_accuracy: 0.6583\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.1640 - accuracy: 0.8296 - val_loss: 0.6471 - val_accuracy: 0.6444\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1790 - accuracy: 0.8231 - val_loss: 0.4478 - val_accuracy: 0.6944\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1803 - accuracy: 0.8218 - val_loss: 0.5151 - val_accuracy: 0.6667\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.1624 - accuracy: 0.8273 - val_loss: 0.7279 - val_accuracy: 0.6444\n",
            "Epoch 74/200\n",
            "71/72 [============================>.] - ETA: 0s - loss: 0.1613 - accuracy: 0.8272Restoring model weights from the end of the best epoch: 39.\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.1618 - accuracy: 0.8264 - val_loss: 0.3434 - val_accuracy: 0.7444\n",
            "Epoch 74: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_hy_lstm_legs.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history_class_mod = model_hy_lstm_legs.fit([X_train_pres,X_train[:,:,features]], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=([X_val_pres,X_val[:,:,features]], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs9LyxIKyKxY"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRb1dLT2Vwe0",
        "outputId": "cc803b74-a6d9-4a20-8b22-d4698d3399de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 2s 10ms/step\n",
            "12/12 [==============================] - 0s 9ms/step\n",
            "Accuracy validation: 0.7444444444444445\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_hy_lstm_legs.predict([X_test_pres,X_test[:,:,features]]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_hy_lstm_legs.predict([X_val_pres,X_val[:,:,features]]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifpRyWUUVwe1"
      },
      "source": [
        "## Hybrid-GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyoQUjPOVwe1"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(23431)\n",
        "np.random.seed(23)\n",
        "random.seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsvkyFazyXf0"
      },
      "outputs": [],
      "source": [
        "#Building the Hybrid_GRU RNN CNN model for \"Legs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMIwZF0BVwe1"
      },
      "outputs": [],
      "source": [
        "class fuse(Model):\n",
        "  def __init__(self):\n",
        "    super(fuse, self).__init__()\n",
        "    self.cnn = tf.keras.Sequential([\n",
        "      layers.Input(shape=(128, 48,1)),\n",
        "      layers.Conv2D(128, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,1)),\n",
        "      layers.Conv2D(64, (3, 3), activation=None, padding='same', strides=2),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Conv2D(32, (3, 3), activation=None, padding='same', strides=1),\n",
        "      layers.Activation('relu'),\n",
        "      layers.AvgPool2D((2,2)),\n",
        "      layers.Flatten()])\n",
        "\n",
        "\n",
        "\n",
        "    self.rnn =  tf.keras.Sequential([\n",
        "            layers.GRU(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(256,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=len(features), return_sequences=True,dropout=0.3),\n",
        "            layers.GRU(128,input_dim=len(features), return_sequences=False,dropout=0.5),\n",
        "            #layers.Dropout(0.4)\n",
        "    ])\n",
        "\n",
        "    self.calssifier = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(32,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(16,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(6,activation =\"softmax\")])\n",
        "\n",
        "    self.fc_cnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "    self.fc_rnn = tf.keras.Sequential([layers.Dense(128,activation =None,use_bias=False),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),\n",
        "                                     layers.Dense(64,activation =None),\n",
        "                                     layers.BatchNormalization(axis=-1),\n",
        "                                     tf.keras.layers.Activation('relu'),])\n",
        "\n",
        "\n",
        "  def call(self,x):\n",
        "    out0 = self.cnn(x[0])\n",
        "    out0 = self.fc_cnn(out0)\n",
        "    out1 = self.rnn(x[1])\n",
        "    out1 = self.fc_rnn(out1)\n",
        "\n",
        "    out = self.calssifier(layers.concatenate([out0,out1]))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGYBMph2Vwe1"
      },
      "outputs": [],
      "source": [
        "model_hy_gru_legs=fuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfl4vA3dVwe2",
        "outputId": "2e26a825-26f6-4c51-f671-0022d8c72fb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_93\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_21 (Conv2D)          (None, 64, 24, 128)       1280      \n",
            "                                                                 \n",
            " activation_193 (Activation)  (None, 64, 24, 128)      0         \n",
            "                                                                 \n",
            " average_pooling2d_21 (Avera  (None, 32, 24, 128)      0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 16, 12, 64)        73792     \n",
            "                                                                 \n",
            " activation_194 (Activation)  (None, 16, 12, 64)       0         \n",
            "                                                                 \n",
            " average_pooling2d_22 (Avera  (None, 8, 6, 64)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 8, 6, 32)          18464     \n",
            "                                                                 \n",
            " activation_195 (Activation)  (None, 8, 6, 32)         0         \n",
            "                                                                 \n",
            " average_pooling2d_23 (Avera  (None, 4, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 384)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,536\n",
            "Trainable params: 93,536\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_hy_gru_legs.cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbSPtSbnygGs"
      },
      "outputs": [],
      "source": [
        "#Training the Hybrid_GRU RNN CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6h9aQUBVwe2",
        "outputId": "d60de09f-7a24-42fd-8662-2732edd24231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 10s 50ms/step - loss: 1.6734 - accuracy: 0.1981 - val_loss: 1.5354 - val_accuracy: 0.1611\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.5449 - accuracy: 0.2338 - val_loss: 1.5781 - val_accuracy: 0.1861\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 1.4223 - accuracy: 0.2972 - val_loss: 1.5887 - val_accuracy: 0.1722\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 1.3400 - accuracy: 0.3352 - val_loss: 1.5069 - val_accuracy: 0.2556\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.2670 - accuracy: 0.3852 - val_loss: 1.4151 - val_accuracy: 0.2944\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.2065 - accuracy: 0.4227 - val_loss: 1.3683 - val_accuracy: 0.3083\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 1.1536 - accuracy: 0.4731 - val_loss: 1.2967 - val_accuracy: 0.3417\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.0774 - accuracy: 0.5046 - val_loss: 1.2368 - val_accuracy: 0.3500\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.0294 - accuracy: 0.5431 - val_loss: 1.1692 - val_accuracy: 0.3833\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.9762 - accuracy: 0.5616 - val_loss: 1.1066 - val_accuracy: 0.4194\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.9189 - accuracy: 0.5972 - val_loss: 1.0335 - val_accuracy: 0.4750\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.8846 - accuracy: 0.6037 - val_loss: 0.9585 - val_accuracy: 0.5306\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.8396 - accuracy: 0.6157 - val_loss: 0.9237 - val_accuracy: 0.5889\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.8033 - accuracy: 0.6380 - val_loss: 0.8711 - val_accuracy: 0.5306\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.7635 - accuracy: 0.6523 - val_loss: 0.8675 - val_accuracy: 0.5556\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.7320 - accuracy: 0.6681 - val_loss: 0.8075 - val_accuracy: 0.5806\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.6904 - accuracy: 0.6852 - val_loss: 0.8059 - val_accuracy: 0.5889\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.6831 - accuracy: 0.6773 - val_loss: 0.7942 - val_accuracy: 0.6250\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6486 - accuracy: 0.6981 - val_loss: 0.7541 - val_accuracy: 0.6167\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.6156 - accuracy: 0.7000 - val_loss: 0.7436 - val_accuracy: 0.6111\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5922 - accuracy: 0.7120 - val_loss: 0.7436 - val_accuracy: 0.5917\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5703 - accuracy: 0.7083 - val_loss: 0.7086 - val_accuracy: 0.6111\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5639 - accuracy: 0.7162 - val_loss: 0.6848 - val_accuracy: 0.6389\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.5373 - accuracy: 0.7218 - val_loss: 0.6295 - val_accuracy: 0.6528\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.5334 - accuracy: 0.7241 - val_loss: 0.6977 - val_accuracy: 0.6083\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.5036 - accuracy: 0.7269 - val_loss: 0.6928 - val_accuracy: 0.6222\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4824 - accuracy: 0.7398 - val_loss: 0.6786 - val_accuracy: 0.6111\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.4753 - accuracy: 0.7421 - val_loss: 0.6115 - val_accuracy: 0.6278\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.4633 - accuracy: 0.7463 - val_loss: 0.6440 - val_accuracy: 0.6278\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4347 - accuracy: 0.7662 - val_loss: 0.6184 - val_accuracy: 0.6528\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.4282 - accuracy: 0.7602 - val_loss: 0.6822 - val_accuracy: 0.5889\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4309 - accuracy: 0.7532 - val_loss: 0.6464 - val_accuracy: 0.6028\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4121 - accuracy: 0.7616 - val_loss: 0.6132 - val_accuracy: 0.6361\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4161 - accuracy: 0.7620 - val_loss: 0.5304 - val_accuracy: 0.6556\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4034 - accuracy: 0.7616 - val_loss: 0.5592 - val_accuracy: 0.6556\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3992 - accuracy: 0.7722 - val_loss: 0.5761 - val_accuracy: 0.6472\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3977 - accuracy: 0.7704 - val_loss: 0.5286 - val_accuracy: 0.6361\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3786 - accuracy: 0.7769 - val_loss: 0.6500 - val_accuracy: 0.6000\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3775 - accuracy: 0.7718 - val_loss: 0.6116 - val_accuracy: 0.6194\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3697 - accuracy: 0.7778 - val_loss: 0.5429 - val_accuracy: 0.6528\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3658 - accuracy: 0.7782 - val_loss: 0.5269 - val_accuracy: 0.6444\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3629 - accuracy: 0.7750 - val_loss: 0.6339 - val_accuracy: 0.6083\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3459 - accuracy: 0.7894 - val_loss: 0.5436 - val_accuracy: 0.6472\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3603 - accuracy: 0.7824 - val_loss: 0.5408 - val_accuracy: 0.6278\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3484 - accuracy: 0.7815 - val_loss: 0.5601 - val_accuracy: 0.6361\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3312 - accuracy: 0.7917 - val_loss: 0.6297 - val_accuracy: 0.6028\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3256 - accuracy: 0.7949 - val_loss: 0.5494 - val_accuracy: 0.6444\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3308 - accuracy: 0.7935 - val_loss: 0.5230 - val_accuracy: 0.6583\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3146 - accuracy: 0.8014 - val_loss: 0.5545 - val_accuracy: 0.6361\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3068 - accuracy: 0.7995 - val_loss: 0.5684 - val_accuracy: 0.6278\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.3063 - accuracy: 0.7986 - val_loss: 0.5452 - val_accuracy: 0.6528\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3073 - accuracy: 0.7949 - val_loss: 0.6236 - val_accuracy: 0.5972\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2906 - accuracy: 0.7995 - val_loss: 0.4976 - val_accuracy: 0.6556\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2941 - accuracy: 0.7995 - val_loss: 0.5581 - val_accuracy: 0.6389\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2868 - accuracy: 0.8032 - val_loss: 0.5388 - val_accuracy: 0.6389\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2816 - accuracy: 0.8023 - val_loss: 0.5488 - val_accuracy: 0.6278\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2825 - accuracy: 0.8051 - val_loss: 0.4934 - val_accuracy: 0.6528\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2675 - accuracy: 0.8074 - val_loss: 0.5171 - val_accuracy: 0.6611\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2688 - accuracy: 0.8120 - val_loss: 0.5138 - val_accuracy: 0.6694\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2670 - accuracy: 0.8069 - val_loss: 0.5462 - val_accuracy: 0.6361\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2639 - accuracy: 0.8083 - val_loss: 0.5842 - val_accuracy: 0.6389\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2580 - accuracy: 0.8130 - val_loss: 0.4946 - val_accuracy: 0.6583\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2652 - accuracy: 0.8065 - val_loss: 0.5465 - val_accuracy: 0.6417\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2516 - accuracy: 0.8120 - val_loss: 0.4789 - val_accuracy: 0.6889\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2478 - accuracy: 0.8093 - val_loss: 0.5860 - val_accuracy: 0.6306\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2512 - accuracy: 0.8111 - val_loss: 0.5128 - val_accuracy: 0.6556\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2422 - accuracy: 0.8162 - val_loss: 0.5760 - val_accuracy: 0.6417\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.2487 - accuracy: 0.8153 - val_loss: 0.5512 - val_accuracy: 0.6333\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2467 - accuracy: 0.8097 - val_loss: 0.5666 - val_accuracy: 0.6444\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2441 - accuracy: 0.8144 - val_loss: 0.5183 - val_accuracy: 0.6417\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2350 - accuracy: 0.8167 - val_loss: 0.5444 - val_accuracy: 0.6333\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2274 - accuracy: 0.8199 - val_loss: 0.5267 - val_accuracy: 0.6444\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2325 - accuracy: 0.8167 - val_loss: 0.5195 - val_accuracy: 0.6583\n",
            "Epoch 74/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2261 - accuracy: 0.8185 - val_loss: 0.5356 - val_accuracy: 0.6667\n",
            "Epoch 75/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2238 - accuracy: 0.8176 - val_loss: 0.5862 - val_accuracy: 0.6389\n",
            "Epoch 76/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2339 - accuracy: 0.8139 - val_loss: 0.5065 - val_accuracy: 0.6667\n",
            "Epoch 77/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2152 - accuracy: 0.8236 - val_loss: 0.5243 - val_accuracy: 0.6389\n",
            "Epoch 78/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2252 - accuracy: 0.8157 - val_loss: 0.5318 - val_accuracy: 0.6500\n",
            "Epoch 79/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2211 - accuracy: 0.8190 - val_loss: 0.5026 - val_accuracy: 0.6694\n",
            "Epoch 80/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2237 - accuracy: 0.8167 - val_loss: 0.5208 - val_accuracy: 0.6472\n",
            "Epoch 81/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2170 - accuracy: 0.8190 - val_loss: 0.5384 - val_accuracy: 0.6444\n",
            "Epoch 82/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2161 - accuracy: 0.8213 - val_loss: 0.5417 - val_accuracy: 0.6333\n",
            "Epoch 83/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2178 - accuracy: 0.8227 - val_loss: 0.5721 - val_accuracy: 0.6361\n",
            "Epoch 84/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2160 - accuracy: 0.8245 - val_loss: 0.5512 - val_accuracy: 0.6472\n",
            "Epoch 85/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2079 - accuracy: 0.8264 - val_loss: 0.6209 - val_accuracy: 0.6250\n",
            "Epoch 86/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2178 - accuracy: 0.8236 - val_loss: 0.5276 - val_accuracy: 0.6472\n",
            "Epoch 87/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2194 - accuracy: 0.8218 - val_loss: 0.5575 - val_accuracy: 0.6444\n",
            "Epoch 88/200\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2094 - accuracy: 0.8282 - val_loss: 0.5318 - val_accuracy: 0.6417\n",
            "Epoch 89/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2079 - accuracy: 0.8227 - val_loss: 0.5452 - val_accuracy: 0.6361\n",
            "Epoch 90/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2220 - accuracy: 0.8185 - val_loss: 0.6434 - val_accuracy: 0.6111\n",
            "Epoch 91/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2157 - accuracy: 0.8227 - val_loss: 0.5904 - val_accuracy: 0.6222\n",
            "Epoch 92/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2081 - accuracy: 0.8278 - val_loss: 0.5248 - val_accuracy: 0.6472\n",
            "Epoch 93/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2175 - accuracy: 0.8255 - val_loss: 0.5446 - val_accuracy: 0.6556\n",
            "Epoch 94/200\n",
            "72/72 [==============================] - 2s 25ms/step - loss: 0.2203 - accuracy: 0.8245 - val_loss: 0.6339 - val_accuracy: 0.6278\n",
            "Epoch 95/200\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.2157 - accuracy: 0.8231 - val_loss: 0.5300 - val_accuracy: 0.6500\n",
            "Epoch 96/200\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.2222 - accuracy: 0.8241 - val_loss: 0.5416 - val_accuracy: 0.6444\n",
            "Epoch 97/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2177 - accuracy: 0.8278 - val_loss: 0.5143 - val_accuracy: 0.6639\n",
            "Epoch 98/200\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2266 - accuracy: 0.8236 - val_loss: 0.5501 - val_accuracy: 0.6667\n",
            "Epoch 99/200\n",
            "70/72 [============================>.] - ETA: 0s - loss: 0.2244 - accuracy: 0.8224Restoring model weights from the end of the best epoch: 64.\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.2243 - accuracy: 0.8227 - val_loss: 0.5522 - val_accuracy: 0.6639\n",
            "Epoch 99: early stopping\n"
          ]
        }
      ],
      "source": [
        "model_hy_gru_legs.compile(loss='CategoricalCrossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-4,decay=1e-4),metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,restore_best_weights=True, patience=35)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history_class_mod = model_hy_gru_legs.fit([X_train_pres,X_train[:,:,features]], Y_train,\n",
        "                batch_size=30,\n",
        "                epochs=200,\n",
        "                shuffle=True,\n",
        "                validation_data=([X_val_pres,X_val[:,:,features]], Y_val),callbacks=[es])\n",
        "\n",
        "end = datetime.timedelta(seconds=(time.time()-time_start))\n",
        "train_time.append(end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shxhX0P-ykUd"
      },
      "outputs": [],
      "source": [
        "#predicting on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdPzlNeke65J",
        "outputId": "87833112-6d78-482a-b13a-f27ed7110c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 1s 10ms/step\n",
            "12/12 [==============================] - 0s 10ms/step\n",
            "Accuracy validation: 0.6888888888888889\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model_hy_gru_legs.predict([X_test_pres,X_test[:,:,features]]),axis=1)\n",
        "y_true = np.argmax(Y_test,axis=1)\n",
        "acc_test = sum(pred==y_true)/len(Y_test)\n",
        "acc_hist_test.append(acc_test)\n",
        "\n",
        "pred_ = np.argmax(model_hy_gru_legs.predict([X_val_pres,X_val[:,:,features]]),axis=1)\n",
        "y_true_ = np.argmax(Y_val,axis=1)\n",
        "acc_val = sum(pred_==y_true_)/len(Y_val)\n",
        "acc_hist_val.append(acc_val)\n",
        "print(f\"Accuracy validation: {acc_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL8s3C42U7w4"
      },
      "source": [
        "# Time plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "MYvGboYMU_4C",
        "outputId": "3b69e2cb-fa07-407e-db71-0e0f1feede3c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 3800x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "label = [\"LSTM all fetures\",\"GRU all fetures\",\"HYBRID LSTM all fetures\",\"HYBRID GRU all fetures\",\n",
        "         \"LSTM subset1\",\"GRU subset1\",\"HYBRID LSTM  subset1\",\"HYBRID GRU  subset1\",\n",
        "         \"LSTM left&right\",\"GRU left&right\",\"HYBRID LSTM  left&right\",\"HYBRID GRU  left&right\",\n",
        "         \"LSTM legs\",\"GRU legs\",\"HYBRID LSTM  legs\",\"HYBRID GRU  legs\"]\n",
        "\n",
        "\n",
        "values = [float(\".\".join([str(j).split(\".\")[0] for j in [float(i) for i in str(l).split(\":\")[1:3]]])) for l in train_time]\n",
        "\n",
        "fig = plt.subplots(figsize=(38,10))\n",
        "\n",
        "\n",
        "plt.bar(label,values)\n",
        "plt.title(\"Training Time\")\n",
        "plt.ylabel(\"Minutes\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylim(bottom=.5,top=5.8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "U9kHEQBquP7_",
        "outputId": "d7589164-c647-44ff-bed7-9bdaddc4370e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e3ae1e7f-1dbd-4390-9087-2e0e0a7ae730\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LSTM all fetures</th>\n",
              "      <td>2.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GRU all fetures</th>\n",
              "      <td>2.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HYBRID LSTM all fetures</th>\n",
              "      <td>2.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HYBRID GRU all fetures</th>\n",
              "      <td>5.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LSTM subset1</th>\n",
              "      <td>2.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GRU subset1</th>\n",
              "      <td>3.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HYBRID LSTM  subset1</th>\n",
              "      <td>2.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HYBRID GRU  subset1</th>\n",
              "      <td>3.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LSTM left&amp;right</th>\n",
              "      <td>1.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GRU left&amp;right</th>\n",
              "      <td>2.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HYBRID LSTM  left&amp;right</th>\n",
              "      <td>2.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HYBRID GRU  left&amp;right</th>\n",
              "      <td>5.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LSTM legs</th>\n",
              "      <td>2.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GRU legs</th>\n",
              "      <td>3.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HYBRID LSTM  legs</th>\n",
              "      <td>2.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HYBRID GRU  legs</th>\n",
              "      <td>3.18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3ae1e7f-1dbd-4390-9087-2e0e0a7ae730')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3ae1e7f-1dbd-4390-9087-2e0e0a7ae730 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3ae1e7f-1dbd-4390-9087-2e0e0a7ae730');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                         Train time\n",
              "LSTM all fetures               2.10\n",
              "GRU all fetures                2.25\n",
              "HYBRID LSTM all fetures        2.90\n",
              "HYBRID GRU all fetures         5.50\n",
              "LSTM subset1                   2.13\n",
              "GRU subset1                    3.31\n",
              "HYBRID LSTM  subset1           2.52\n",
              "HYBRID GRU  subset1            3.13\n",
              "LSTM left&right                1.55\n",
              "GRU left&right                 2.40\n",
              "HYBRID LSTM  left&right        2.27\n",
              "HYBRID GRU  left&right         5.24\n",
              "LSTM legs                      2.30\n",
              "GRU legs                       3.56\n",
              "HYBRID LSTM  legs              2.48\n",
              "HYBRID GRU  legs               3.18"
            ]
          },
          "execution_count": 398,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp={\"Train time\":values}\n",
        "time_pd = pd.DataFrame(temp,index=label)\n",
        "time_pd"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lBes_Cto2S13",
        "2mkNMBBWiquZ",
        "WIJqHSxe4UwD",
        "4pGngtdQh-Lq",
        "1JK-WKlC0-Hh",
        "TX7NHlmd_FwG",
        "pM67Mznil7va",
        "oqghDYpol7va",
        "fA7r3Z60r1JV",
        "qoFD4BH2l7vb",
        "R4FHiYVaH5sO",
        "QVrNHMNzKfPp",
        "PAh4PQTUKfP2",
        "L6psAvIYKfP4",
        "B5ftpDGjKfP6",
        "OhH7k5aLNw5z",
        "Mqp5SbqqKpuy",
        "3Ly0tMaeVj3S",
        "cU_S6vqfVj3T",
        "Darp1HWZVj3U",
        "xYisg9Q2Vj3V",
        "R3GEPDV0Kq7Z",
        "QmgBR3orVwev",
        "Uia438imVwey",
        "44u6sMzgVwe0",
        "ifpRyWUUVwe1",
        "WL8s3C42U7w4"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}