# Deep_Learning_Based_Gait_Classification

Gait patterns, their irregularities, their recognition and classification may represent an important source of information on several other health conditions, and may provide useful hints for the future well-being of a person. Abnormal gait is possibly caused by various reasons on a physical, psychological or even neural level and the early and prompt recognition of the same may largely impact the future life condition of the person.

This work addresses the classification of six different classes of complicated gait conditions using skeleton tracking and foot pressure data. The dataset includes skeleton tracking data obtained from the ”kinect” dataset and feet pressure data from the ”feet-pressure” dataset. Advanced machine learning and deep learning algorithms, such as CNN, LSTM, and GRU, are employed to classify the data. One notable aspect of our methodology was the utilization of the Subject hold out technique. We evaluate various combinations of features and models to identify the most effective approach for accurate classification. Through experimentation, we demonstrate that the LSTM model trained on the left and right feature set achieves the highest accuracy of 93.33%. These findings highlight the significance of accurate feature selection and model architecture in the field of skeleton-based pathological gait classification. Furthermore, our research showcases the potential for
early recognition and classification of abnormal gait patterns, offering promising avenues for personalized healthcare interventions and advancements in gait analysis. The outcomes of this study can benefit future research endeavors and find applications in the development of commercial products focused on improving gait analysis and personalized healthcare interventions.

We were largely inspidred by the work:

K. Jun, S. Lee, D. -W. Lee and M. S. Kim, "Deep Learning-Based Multimodal Abnormal Gait Classification Using a 3D Skeleton and Plantar Foot Pressure," in IEEE Access, vol. 9, pp. 161576-161589, 2021, doi: 10.1109/ACCESS.2021.3131613.
In fact many image are taken from that work.
